<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <meta name="generator" content="pandoc">
  <meta name="dcterms.date" content="2021-03-02">
  <title>Representation and Transfer Learning</title>
  <meta name="apple-mobile-web-app-capable" content="yes">
  <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
  <meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=no, minimal-ui">
  <link rel="stylesheet" href="https://unpkg.com/reveal.js@3.9.2/css/reveal.css">
  <style type="text/css">
      code{white-space: pre-wrap;}
      span.smallcaps{font-variant: small-caps;}
      span.underline{text-decoration: underline;}
      div.column{display: inline-block; vertical-align: top; width: 50%;}
  </style>
  <link rel="stylesheet" href="https://unpkg.com/reveal.js@3.9.2/css/theme/black.css" id="theme">
  <link rel="stylesheet" href="../assets/css/talks.css"/>
  <!-- Printing and PDF exports -->
  <script>
    var link = document.createElement( 'link' );
    link.rel = 'stylesheet';
    link.type = 'text/css';
    link.href = window.location.search.match( /print-pdf/gi ) ? 'https://unpkg.com/reveal.js@3.9.2/css/print/pdf.css' : 'https://unpkg.com/reveal.js@3.9.2/css/print/paper.css';
    document.getElementsByTagName( 'head' )[0].appendChild( link );
  </script>
  <!--[if lt IE 9]>
  <script src="https://unpkg.com/reveal.js@3.9.2/lib/js/html5shiv.js"></script>
  <![endif]-->
  <script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS-MML_SVG" type="text/javascript"></script>
  <script type="text/x-mathjax-config">
    MathJax.Hub.Config({
    TeX: {
         extensions: ["color.js"]
      }
    });
  </script>
  <script src="../assets/js/figure-animate.js"></script>
</head>
<body>
  <div class="reveal">
    <div class="slides">

<section id="title-slide">
  <h1 class="title">Representation and Transfer Learning</h1>
  <p class="author" style="text-align:center"><a href="https://www.inference.vc/about/">Ferenc Husz√°r</a></p>
  <p class="date" style="text-align:center"><time>2021-03-02</time></p>
  <p class="venue" style="text-align:center">Zoom</p>
</section>

<section class="slide level2">

<!-- Do not edit this file locally. -->
<!---->
<!-- Do not edit this file locally. -->
<!-- The last names to be defined. Should be defined entirely in terms of macros from above-->
</section>
<section id="unsupervised-learning" class="slide level2">
<h2>Unsupervised learning</h2>
<ul>
<li>observations <span class="math inline">\(x_1, x_2, \ldots\)</span></li>
<li>drawn i.i.d. from some <span class="math inline">\(p_\mathcal{D}\)</span></li>
<li>can we learn something from this?</li>
</ul>
</section>
<section id="unsupervised-learning-goals" class="slide level2">
<h2>Unsupervised learning goals</h2>
<ul>
<li>can we learn something from this?
<ul>
<li>a model of data distribution <span class="math inline">\(p_\theta(x) \approx p_{\mathcal{D}}(x)\)</span>
<ul>
<li>compression</li>
<li>data reconstruction</li>
<li>sampling/generation</li>
</ul></li>
<li>a representation <span class="math inline">\(z=g_\theta(x)\)</span> or <span class="math inline">\(q_{\theta}(z\vert x)\)</span>
<ul>
<li>downstream classification task</li>
<li>data visualisation</li>
</ul></li>
</ul></li>
</ul>
</section>
<section id="ul-as-distribution-modeling" class="slide level2">
<h2>UL as distribution modeling</h2>
<ul>
<li>defines goal as modeling <span class="math inline">\(p_\theta(x)\approx p_\mathcal{D}(x)\)</span></li>
<li><span class="math inline">\(\theta\)</span>: parameters</li>
<li>maximum likelihood estimation: <span class="math display">\[
  \theta^{ML} = \operatorname{argmax}_\theta \sum_{x_i \in \mathcal{D}} \log p_\theta(x_i)
\]</span></li>
</ul>
</section>
<section id="deep-learning-for-modelling-distributions" class="slide level2">
<h2>Deep learning for modelling distributions</h2>
<ul>
<li>auto-regressive models (e.g.¬†RNNs)
<ul>
<li><span class="math inline">\(p_{\theta}(x_{1:T}) = \prod_{t=1}^T p_\theta(x_t\vert x_{1:t-1})\)</span></li>
</ul></li>
<li>implicit distributions (e.g.¬†GANs)
<ul>
<li>x = <span class="math inline">\(g_\theta(z), z\sim \mathcal{N}(0, I)\)</span></li>
</ul></li>
<li>flow models (e.g.¬†RealNVP)
<ul>
<li>like above but <span class="math inline">\(g_\theta(z)\)</span> invertible</li>
</ul></li>
<li>latent variable models (LVMs, e.g.¬†VAE)
<ul>
<li><span class="math inline">\(p_\theta(x) = \int p_\theta(x, z) dz\)</span></li>
</ul></li>
</ul>
</section>
<section id="latent-variable-models" class="slide level2">
<h2>Latent variable models</h2>
<p><span class="math display">\[
p_\theta(x) = \int p_\theta(x, z) dz
\]</span></p>
</section>
<section id="latent-variable-models-1" class="slide level2">
<h2>Latent variable models</h2>
<p><span class="math display">\[
p_\theta(x) = \int p_\theta(x\vert z) p_\theta(z) dz
\]</span></p>
</section>
<section id="motivation-1" class="slide level2">
<h2>Motivation 1</h2>
<p><strong>‚Äúit makes sense‚Äù</strong></p>
<ul>
<li>describes data in terms of a generative process</li>
<li>e.g.¬†object properties, locations</li>
<li>learnt <span class="math inline">\(z\)</span> often interpretable</li>
<li>causal reasoning often needs latent variables</li>
</ul>
</section>
<section id="motivation-2" class="slide level2">
<h2>Motivation 2</h2>
<p><strong>manifold assumption</strong></p>
<ul>
<li>high-dimensional data</li>
<li>doesn‚Äôt occupy all the space</li>
<li>concentrated along low-dimensional manifold</li>
<li><span class="math inline">\(z \approx\)</span> intrinsic coordinates within the manifold</li>
</ul>
</section>
<section id="motivation-3" class="slide level2">
<h2>Motivation 3</h2>
<p><strong>from simple to complicated</strong></p>
<p><span class="math display">\[
p_\theta(x) = \int p_\theta(x\vert z) p_\theta(z) dz
\]</span></p>
</section>
<section id="motivation-3-1" class="slide level2">
<h2>Motivation 3</h2>
<p><strong>from simple to complicated</strong></p>
<p><span class="math display">\[
\underbrace{p_\theta(x)}_\text{complicated} = \int \underbrace{p_\theta(x\vert z) }_\text{simple}\underbrace{p_\theta(z)}_\text{simple} dz
\]</span></p>
</section>
<section id="motivation-3-2" class="slide level2">
<h2>Motivation 3</h2>
<p><strong>from simple to complicated</strong></p>
<p><span class="math display">\[
\underbrace{p_\theta(x)}_\text{complicated} = \int \underbrace{\mathcal{N}\left(x; \mu_\theta(z), \operatorname{diag}(\sigma_\theta(z)) \right)}_\text{simple}\underbrace{\mathcal{N}(z; 0, I)}_\text{simple} dz
\]</span></p>
</section>
<section id="motivation-4" class="slide level2">
<h2>Motivation 4</h2>
<p><strong>variational learning</strong></p>
<ul>
<li>evaluating <span class="math inline">\(p_\theta(x)\)</span> is hard
<ul>
<li>learning is hard</li>
</ul></li>
<li>evaluating <span class="math inline">\(p_\theta(z\vert x)\)</span> is hard
<ul>
<li>inference is hard</li>
</ul></li>
<li>variational framework:
<ul>
<li>approximate learning</li>
<li>approximate inference</li>
</ul></li>
</ul>
</section>
<section id="section" class="slide level2">
<h2></h2>
<center>
<div class="centered" style="">
<img class="" src="https://i.imgur.com/rmgzOHJ.png" width="600px" height="auto" align="center" style="background:none; border:none; box-shadow:none; display:block; margin-left:auto; margin-right:auto;vertical-align:middle">
</div>
</center>
<p><a href="https://arxiv.org/abs/1906.02691">(Kingma and Welling, 2019)</a> Variational Autoencoder</p>
</section>
<section id="variational-autoencoder" class="slide level2">
<h2>Variational autoencoder</h2>
<ul>
<li>Decoder: <span class="math inline">\(p_\theta(x\vert z) = \mathcal{N}(\mu_\theta(z), \sigma_n I)\)</span></li>
<li>Encoder: <span class="math inline">\(q_\psi(z\vert x) = \mathcal{N}(\mu_\psi(z), \sigma_\psi(z))\)</span></li>
<li>Prior: <span class="math inline">\(p_\theta(z)=\mathcal{N}(0, I)\)</span></li>
</ul>
</section>
<section id="variational-encoder-interpretable-z" class="slide level2">
<h2>Variational encoder: interpretable <span class="math inline">\(z\)</span></h2>
<center>
<div class="centered" style="">
<img class="" src="https://i.imgur.com/kDgc74S.png" width="" height="auto" align="center" style="background:none; border:none; box-shadow:none; display:block; margin-left:auto; margin-right:auto;vertical-align:middle">
</div>
</center>
<!-- SECTION Self-supervised learning -->
</section>
<section id="self-supervised-learning" class="slide level2">
<h2>Self-supervised learning</h2>
</section>
<section id="basic-idea" class="slide level2">
<h2>basic idea</h2>
<ul>
<li>turn unsupervised problem into supervised one</li>
<li>turn datapoints <span class="math inline">\(x_i\)</span> into input-output pairs</li>
<li>called auxiliary or pretext task</li>
<li>learn to solve auxiliary task</li>
<li>transfer representation leaned to the downstream task</li>
</ul>
</section>
<section id="example-jigsaw-puzzles" class="slide level2">
<h2>example: jigsaw puzzles</h2>
<center>
<div class="centered centered" style="">
<img class="" src="https://i.imgur.com/VtCWtrq.jpg" width="" height="auto" align="center" style="background:none; border:none; box-shadow:none; display:block; margin-left:auto; margin-right:auto;vertical-align:middle">
</div>
</center>
<p><a href="https://arxiv.org/abs/1603.09246">(Noroozi and Favaro, 2016)</a></p>
</section>
<section id="data-efficiency-in-downstream-task" class="slide level2">
<h2>Data-efficiency in downstream task</h2>
<center>
<div class="centered" style="">
<img class="" src="https://i.imgur.com/bX3BzNx.png" width="570px" height="auto" align="center" style="background:none; border:none; box-shadow:none; display:block; margin-left:auto; margin-right:auto;vertical-align:middle">
</div>
</center>
<p><a href="https://proceedings.icml.cc/static/paper_files/icml/2020/3694-Paper.pdf">(H√®naff et al, 2020)</a></p>
</section>
<section id="linearity-in-downstream-task" class="slide level2">
<h2>Linearity in downstream task</h2>
<center>
<div class="centered" style="">
<img class="" src="https://i.imgur.com/4YiDM38.png" width="570px" height="auto" align="center" style="background:none; border:none; box-shadow:none; display:block; margin-left:auto; margin-right:auto;vertical-align:middle">
</div>
</center>
<p><a href="https://arxiv.org/abs/2002.05709">(Chen et al, 2020)</a></p>
</section>
<section id="several-self-supervised-methods" class="slide level2">
<h2>Several self-supervised methods</h2>
<ul>
<li>auto-encoding</li>
<li>denoising auto-encoding</li>
<li>pseudo-likelihood</li>
<li>instance classification</li>
<li>contrastive learning</li>
<li>masked language models</li>
</ul>
</section>
<section id="example-instance-classification" class="slide level2">
<h2>Example: instance classification</h2>
<ul>
<li>pick random data index <span class="math inline">\(i\)</span></li>
<li>randomly transform image <span class="math inline">\(x_i\)</span>: <span class="math inline">\(T(x_i)\)</span></li>
<li>auxilliary task: guess data index <span class="math inline">\(i\)</span> from transformed input <span class="math inline">\(T(x_i)\)</span></li>
<li>difficulty: N-way classification</li>
</ul>
</section>
<section id="example-contrastive-learning" class="slide level2">
<h2>Example: contrastive learning</h2>
<ul>
<li>pick random <span class="math inline">\(y\)</span></li>
<li>if <span class="math inline">\(y=1\)</span> pick two random images <span class="math inline">\(x_1\)</span>, <span class="math inline">\(x_2\)</span></li>
<li>if <span class="math inline">\(y=0\)</span> use same image twice <span class="math inline">\(x_1=x_2\)</span></li>
<li>aux task: predict <span class="math inline">\(y\)</span> from <span class="math inline">\(f_\theta(T_1(x_1)), f_\theta(T_2(x_2))\)</span></li>
</ul>
</section>
<section id="example-masked-language-models" class="slide level2">
<h2>Example: Masked Language Models</h2>
<center>
<div class="centered" style="">
<img class="" src="https://i.imgur.com/HeRgOXp.png" width="" height="auto" align="center" style="background:none; border:none; box-shadow:none; display:block; margin-left:auto; margin-right:auto;vertical-align:middle">
</div>
</center>
<p><small>image credit: (<a href="https://arxiv.org/pdf/1901.07291.pdf">Lample and Conneau, 2019</a>)</small></p>
</section>
<section id="bert" class="slide level2">
<h2>BERT</h2>
<center>
<div class="centered centered" style="">
<img class="" src="https://i.imgur.com/zeA1Mix.jpg" width="" height="auto" align="center" style="background:none; border:none; box-shadow:none; display:block; margin-left:auto; margin-right:auto;vertical-align:middle">
</div>
</center>
<!-- SECTION Why should any of this work? -->
</section>
<section id="why-should-any-of-this-work" class="slide level2">
<h2>Why should any of this work?</h2>
<p><strong>Predicting What you Already Know Helps: Provable Self-Supervised Learning</strong></p>
<p><a href="https://arxiv.org/abs/2008.01064">(Lee et al, 2020)</a></p>
</section>
<section id="provable-self-supervised-learning" class="slide level2">
<h2>Provable Self-Supervised Learning</h2>
<p>Assumptions:</p>
<ul>
<li>observable <span class="math inline">\(X\)</span> decomposes into <span class="math inline">\(X_1, X_2\)</span></li>
<li>pretext: only given <span class="math inline">\((X_1, X_2)\)</span> pairs</li>
<li>downstream: we will want to predict <span class="math inline">\(Y\)</span></li>
<li><span class="math inline">\(X_1 \perp \!\!\! \perp X_2 \vert Y, Z\)</span></li>
<li>(+1 additional strong assumption)</li>
</ul>
</section>
<section id="provable-self-supervised-learning-1" class="slide level2">
<h2>Provable Self-Supervised Learning</h2>
<center>
<div class="centered" style="">
<img class="" src="https://i.imgur.com/8SomFq9.png" width="" height="auto" align="center" style="background:none; border:none; box-shadow:none; display:block; margin-left:auto; margin-right:auto;vertical-align:middle">
</div>
</center>
<p><span class="math inline">\(X_1 \perp \!\!\! \perp X_2 \vert Y, Z\)</span></p>
</section>
<section id="provable-self-supervised-learning-2" class="slide level2">
<h2>Provable Self-Supervised Learning</h2>
<center>
<div class="centered" style="">
<img class="" src="https://i.imgur.com/K754eB2.png" width="" height="auto" align="center" style="background:none; border:none; box-shadow:none; display:block; margin-left:auto; margin-right:auto;vertical-align:middle">
</div>
</center>
</section>
<section id="provable-self-supervised-learning-3" class="slide level2">
<h2>Provable Self-Supervised Learning</h2>
<center>
<div class="centered" style="">
<img class="" src="https://i.imgur.com/SuCKgJq.png" width="" height="auto" align="center" style="background:none; border:none; box-shadow:none; display:block; margin-left:auto; margin-right:auto;vertical-align:middle">
</div>
</center>
<p><span class="math display">\[
X_1 \perp \!\!\! \perp  X_2 \vert Y, Z
\]</span></p>
</section>
<section id="provable-self-supervised-learning-4" class="slide level2">
<h2>Provable Self-Supervised Learning</h2>
<center>
<div class="centered" style="">
<img class="" src="https://i.imgur.com/SuCKgJq.png" width="" height="auto" align="center" style="background:none; border:none; box-shadow:none; display:block; margin-left:auto; margin-right:auto;vertical-align:middle">
</div>
</center>
<p><span class="math display">\[
üëÄ \perp \!\!\! \perp  üëÑ \vert \text{age}, \text{gender}, \text{ethnicity}
\]</span></p>
</section>
<section id="provable-self-supervised-learning-5" class="slide level2">
<h2>Provable Self-Supervised Learning</h2>
<p>If <span class="math inline">\(X_1 \perp \!\!\! \perp X_2 \vert Y\)</span>, then</p>
<p><span class="math display">\[
\mathbb{E}[X_2 \vert X_1] = \sum_k \mathbb{E}[X_2\vert Y=k] \mathbb{P}[Y=k\vert X_1 = x_1]
\]</span></p>
</section>
<section id="provable-self-supervised-learning-6" class="slide level2">
<h2>Provable Self-Supervised Learning</h2>
<p><span class="math display">\[\begin{align}
&amp;\mathbb{E}[X_2 \vert X_1=x_1] = \\
&amp;\left[\begin{matrix} \mathbb{E}[X_2\vert Y=1], \ldots, \mathbb{E}[X_2\vert Y=k]\end{matrix}\right] \left[\begin{matrix} \mathbb{P}[Y=1\vert X_1=x_1]\\ \vdots \\ \mathbb{P}[Y=k\vert X_1=x_1]\end{matrix}\right]
\end{align}\]</span></p>
</section>
<section id="provable-self-supervised-learning-7" class="slide level2">
<h2>Provable Self-Supervised Learning</h2>
<p><span class="math display">\[\begin{align}
&amp;\mathbb{E}[X_2 \vert X_1=x_1] = \\
&amp;\underbrace{\left[\begin{matrix} \mathbb{E}[X_2\vert Y=1], \ldots, \mathbb{E}[X_2\vert Y=k]\end{matrix}\right]}_\mathbf{A}\left[\begin{matrix} \mathbb{P}[Y=1\vert X_1=x_1]\\ \vdots \\ \mathbb{P}[Y=k\vert X_1=x_1]\end{matrix}\right]
\end{align}\]</span></p>
</section>
<section id="provable-self-supervised-learning-8" class="slide level2">
<h2>Provable Self-Supervised Learning</h2>
<p><span class="math display">\[
\mathbb{E}[X_2 \vert X_1=x_1] = \mathbf{A}\left[\begin{matrix} \mathbb{P}[Y=1\vert X_1=x_1]\\ \vdots \\ \mathbb{P}[Y=k\vert X_1=x_1]\end{matrix}\right]
\]</span></p>
</section>
<section id="provable-self-supervised-learning-9" class="slide level2">
<h2>Provable Self-Supervised Learning</h2>
<p><span class="math display">\[
\mathbf{A}^\dagger \mathbb{E}[X_2 \vert X_1=x_1] = \left[\begin{matrix} \mathbb{P}[Y=1\vert X_1=x_1]\\ \vdots \\ \mathbb{P}[Y=k\vert X_1=x_1]\end{matrix}\right]
\]</span></p>
</section>
<section id="provable-self-supervised-learning-10" class="slide level2">
<h2>Provable Self-Supervised Learning</h2>
<p><span class="math display">\[
\mathbf{A}^\dagger \underbrace{\mathbb{E}[X_2 \vert X_1=x_1]}_\text{pretext task} = \underbrace{\left[\begin{matrix} \mathbb{P}[Y=1\vert X_1=x_1]\\ \vdots \\ \mathbb{P}[Y=k\vert X_1=x_1]\end{matrix}\right]}_\text{downstream task}
\]</span></p>
</section>
<section id="provable-self-supervised-learning-summary" class="slide level2">
<h2>Provable self-supervised learning summary</h2>
<ul>
<li>under assumptions of conditional independence</li>
<li>(and that matrix <span class="math inline">\(\mathbf{A}\)</span> is full rank)</li>
<li><span class="math inline">\(\mathbb{P}[Y|x_1]\)</span> is in linear span of <span class="math inline">\(\mathbb{E}[X_2\vert x_1]\)</span></li>
<li>All we need is linear model on top of <span class="math inline">\(\mathbb{E}[X_2\vert x_1]\)</span></li>
<li>note: <span class="math inline">\(\mathbb{P}[Y|x_1, x_2]\)</span> would be really optimal</li>
</ul>
<!-- SECTION Recap -->
</section>
<section id="recap" class="slide level2">
<h2>Recap</h2>
</section>
<section id="variational-learning" class="slide level2">
<h2>Variational learning</h2>
<p><span class="math display">\[
\theta^\text{ML} = \operatorname{argmax}_\theta \sum_{x_i \in \mathcal{D}} \log p_\theta(x_i)
\]</span></p>
</section>
<section id="variational-learning-1" class="slide level2">
<h2>Variational learning</h2>
<p><span class="math display">\[
\mathcal{L}(\theta, \psi) = \sum_{x_i \in \mathcal{D}} \log  p_\theta(x_i) - \operatorname{KL}[q_\psi(z\vert x_i) \| p_\theta(z\vert x_i)]
\]</span></p>
</section>
<section id="variational-learning-2" class="slide level2">
<h2>Variational learning</h2>
<p><span class="math display">\[
\mathcal{L}(\theta, \psi) = \sum_{x_i \in \mathcal{D}} \log p_\theta(x_i) + \mathbb{E}_{z\sim q_\psi} \log \frac{p_\theta(z\vert x_i)}{q_\psi(z\vert x_i)}
\]</span></p>
</section>
<section id="variational-learning-3" class="slide level2">
<h2>Variational learning</h2>
<p><span class="math display">\[
\mathcal{L}(\theta, \psi) = \sum_{x_i \in \mathcal{D}} \mathbb{E}_{z\sim q_\psi} \log \frac{p_\theta(z\vert x_i) p_\theta(x_i)}{q_\psi(z\vert x_i)}
\]</span></p>
</section>
<section id="variational-learning-4" class="slide level2">
<h2>Variational learning</h2>
<p><span class="math display">\[
\mathcal{L}(\theta, \psi) = \sum_{x_i \in \mathcal{D}} \mathbb{E}_{z\sim q_\psi} \log \frac{p_\theta(z,  x_i)}{q_\psi(z\vert x_i)}
\]</span></p>
</section>
<section id="variational-learning-5" class="slide level2">
<h2>Variational learning</h2>
<p><span class="math display">\[
\mathcal{L}(\theta, \psi) = \sum_{x_i \in \mathcal{D}} \mathbb{E}_{z\sim q_\psi(z\vert x_i)} \log p(x_i\vert z) - \operatorname{KL}[q_\psi(z\vert x_i)\vert p_\theta(z)]
\]</span></p>
</section>
<section id="variational-learning-6" class="slide level2">
<h2>Variational learning</h2>
<p><span class="math display">\[
\mathcal{L}(\theta, \psi) = \sum_{x_i \in \mathcal{D}} \underbrace{\mathbb{E}_{z\sim q_\psi(z\vert x_i)} \log p(x_i\vert z)}_\text{reconstruction} - \operatorname{KL}[q_\psi(z\vert x_i)\vert p_\theta(z)]
\]</span></p>
</section>
<section id="discussion-of-max-likelihood" class="slide level2">
<h2>Discussion of max likelihood</h2>
<ul>
<li>trained so that <span class="math inline">\(p_\theta(x)\)</span> matches data</li>
<li>evaluated by how useful <span class="math inline">\(p_\theta(z\vert x)\)</span> is</li>
<li>there is a mismatch</li>
</ul>
</section>
<section id="representation-learning-vs-max-likelihood" class="slide level2">
<h2>Representation learning vs max likelihood</h2>
<center>
<div class="centered" style="">
<img class="" src="https://i.imgur.com/SPx9AoA.png" width="" height="auto" align="center" style="background:none; border:none; box-shadow:none; display:block; margin-left:auto; margin-right:auto;vertical-align:middle">
</div>
</center>
</section>
<section id="representation-learning-vs-max-likelihood-1" class="slide level2">
<h2>Representation learning vs max likelihood</h2>
<center>
<div class="centered" style="">
<img class="" src="https://i.imgur.com/EqHhQVh.png" width="" height="auto" align="center" style="background:none; border:none; box-shadow:none; display:block; margin-left:auto; margin-right:auto;vertical-align:middle">
</div>
</center>
</section>
<section id="representation-learning-vs-max-likelihood-2" class="slide level2">
<h2>Representation learning vs max likelihood</h2>
<center>
<div class="centered" style="">
<img class="" src="https://i.imgur.com/L0n5kSI.png" width="" height="auto" align="center" style="background:none; border:none; box-shadow:none; display:block; margin-left:auto; margin-right:auto;vertical-align:middle">
</div>
</center>
</section>
<section id="representation-learning-vs-max-likelihood-3" class="slide level2">
<h2>Representation learning vs max likelihood</h2>
<center>
<div class="centered" style="">
<img class="" src="https://i.imgur.com/wuAdSbB.png" width="" height="auto" align="center" style="background:none; border:none; box-shadow:none; display:block; margin-left:auto; margin-right:auto;vertical-align:middle">
</div>
</center>
</section>
<section id="representation-learning-vs-max-likelihood-4" class="slide level2">
<h2>Representation learning vs max likelihood</h2>
<center>
<div class="centered" style="">
<img class="" src="https://i.imgur.com/DwGlp8k.png" width="" height="auto" align="center" style="background:none; border:none; box-shadow:none; display:block; margin-left:auto; margin-right:auto;vertical-align:middle">
</div>
</center>
</section>
<section id="representation-learning-vs-max-likelihood-5" class="slide level2">
<h2>Representation learning vs max likelihood</h2>
<center>
<div class="centered" style="">
<img class="" src="https://i.imgur.com/yuoEcbt.png" width="" height="auto" align="center" style="background:none; border:none; box-shadow:none; display:block; margin-left:auto; margin-right:auto;vertical-align:middle">
</div>
</center>
</section>
<section id="discussion-of-max-likelihood-1" class="slide level2">
<h2>Discussion of max likelihood</h2>
<ul>
<li>max likelihood may not produce good representations</li>
<li>Why do variational methods find good representations?</li>
<li>Are there alternative principles?</li>
</ul>
</section>
    </div>
  </div>

  <script src="https://unpkg.com/reveal.js@3.9.2/lib/js/head.min.js"></script>
  <script src="https://unpkg.com/reveal.js@3.9.2/js/reveal.js"></script>

  <script>

      // Full list of configuration options available at:
      // https://github.com/hakimel/reveal.js#configuration
      Reveal.initialize({
        // Push each slide change to the browser history
        history: true,
        // Transition style
        transition: 'None', // none/fade/slide/convex/concave/zoom
        math: {
          mathjax: 'https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js',
          config: 'TeX-AMS_HTML-full',
          tex2jax: {
            inlineMath: [['\\(','\\)']],
            displayMath: [['\\[','\\]']],
            balanceBraces: true,
            processEscapes: false,
            processRefs: true,
            processEnvironments: true,
            preview: 'TeX',
            skipTags: ['script','noscript','style','textarea','pre','code'],
            ignoreClass: 'tex2jax_ignore',
            processClass: 'tex2jax_process'
          },
        },

        // Optional reveal.js plugins
        dependencies: [
          { src: 'https://unpkg.com/reveal.js@3.9.2/lib/js/classList.js', condition: function() { return !document.body.classList; } },
          { src: 'https://unpkg.com/reveal.js@3.9.2/plugin/zoom-js/zoom.js', async: true },
          { src: 'https://unpkg.com/reveal.js@3.9.2/plugin/math/math.js', async: true },
          { src: 'https://unpkg.com/reveal.js@3.9.2/plugin/notes/notes.js', async: true }
        ]
      });
    </script>
    </body>
</html>
