<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <meta name="generator" content="pandoc">
  <meta name="dcterms.date" content="2022-02-01">
  <title>Stochastic Optimization</title>
  <meta name="apple-mobile-web-app-capable" content="yes">
  <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
  <meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=no, minimal-ui">
  <link rel="stylesheet" href="https://unpkg.com/reveal.js@3.9.2/css/reveal.css">
  <style type="text/css">
      code{white-space: pre-wrap;}
      span.smallcaps{font-variant: small-caps;}
      span.underline{text-decoration: underline;}
      div.column{display: inline-block; vertical-align: top; width: 50%;}
  </style>
  <link rel="stylesheet" href="https://unpkg.com/reveal.js@3.9.2/css/theme/black.css" id="theme">
  <link rel="stylesheet" href="https://inverseprobability.com/assets/css/talks.css"/>
  <!-- Printing and PDF exports -->
  <script>
    var link = document.createElement( 'link' );
    link.rel = 'stylesheet';
    link.type = 'text/css';
    link.href = window.location.search.match( /print-pdf/gi ) ? 'https://unpkg.com/reveal.js@3.9.2/css/print/pdf.css' : 'https://unpkg.com/reveal.js@3.9.2/css/print/paper.css';
    document.getElementsByTagName( 'head' )[0].appendChild( link );
  </script>
  <!--[if lt IE 9]>
  <script src="https://unpkg.com/reveal.js@3.9.2/lib/js/html5shiv.js"></script>
  <![endif]-->
  <script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS-MML_SVG" type="text/javascript"></script>
  <script type="text/x-mathjax-config">
    MathJax.Hub.Config({
    TeX: {
         extensions: ["color.js"]
      }
    });
  </script>
  <script src="../assets/js/figure-animate.js"></script>
</head>
<body>
  <div class="reveal">
    <div class="slides">

<section id="title-slide">
  <h1 class="title">Stochastic Optimization</h1>
  <p class="author" style="text-align:center"><a href="https://www.inference.vc/about/">Ferenc Huszár</a></p>
  <p class="date" style="text-align:center"><time>2022-02-01</time></p>
  <p class="venue" style="text-align:center">LT1, William Gates
Building</p>
</section>

<section class="slide level2">

<!-- Do not edit this file locally. -->
<!---->
<!-- Do not edit this file locally. -->
<!-- The last names to be defined. Should be defined entirely in terms of macros from above-->
</section>
<section id="empirical-risk-minimization-via-gradient-descent"
class="slide level2">
<h2>Empirical Risk Minimization via gradient descent</h2>
<p><span class="math display">\[
\mathbf{w}_{t+1} = \mathbf{w}_t - \eta \nabla_\mathbf{w}
\hat{L}(\mathbf{w_t}, \mathcal{D})
\]</span></p>
<p>Calculating the gradient: * takes time to cycle through whole dataset
* limited memory on GPU * is wasteful: <span
class="math inline">\(\hat{L}\)</span> is a sum, CLT applies</p>
</section>
<section id="stochastic-gradient-descent" class="slide level2">
<h2>Stochastic gradient descent</h2>
<p><span class="math display">\[
\mathbf{w}_{t+1} = \mathbf{w}_t - \eta \nabla_\mathbf{w}
\hat{L}(\mathbf{w_t}, \mathcal{D}_t)
\]</span></p>
<p>where <span class="math inline">\(\mathcal{D}_t\)</span> is a random
subset (minibatch) of <span
class="math inline">\(\mathcal{D}\)</span>.</p>
<p>Also known as minibatch-SGD.</p>
</section>
<section id="does-it-converge" class="slide level2">
<h2>Does it converge?</h2>
<p>Unbiased gradient estimator:</p>
<p><span class="math display">\[
\mathbb{E}[\hat{L}(\mathbf{w}, \mathcal{D}_t)] = \hat{L}(\mathbf{w},
\mathcal{D})
\]</span></p>
<ul>
<li>empirical risk does not increase in expectation</li>
<li><span class="math inline">\(\hat{L}(\mathbf{w}_t)\)</span> is a
supermartingale</li>
<li>Doob’s martingale convergence theorem: a.s. convergence.</li>
</ul>
</section>
<section id="does-it-behave-the-same-way" class="slide level2">
<h2>Does it behave the same way?</h2>
<p><img data-src="https://i.imgur.com/xRYHk0m.png%20=1200x" /></p>
</section>
<section id="improving-sgd-two-key-ideas" class="slide level2">
<h2>Improving SGD: Two key ideas</h2>
<ul>
<li>idea 1: momentum
<ul>
<li><strong>problem:</strong>
<ul>
<li>high variance of gradients due to stochasticity</li>
<li>oscillation in narrow valley situation</li>
</ul></li>
<li><strong>solution</strong>: maintain running average of
gradients</li>
</ul></li>
</ul>
<p>https://distill.pub/2017/momentum/</p>
</section>
<section id="improving-sgd-two-key-ideas-1" class="slide level2">
<h2>Improving SGD: two key ideas</h2>
<ul>
<li>idea 2: adaptive stepsizes
<ul>
<li><strong>problem</strong>:
<ul>
<li>parameters have different magnitude gradients</li>
<li>some parameters tolerate high learning rates, others don’t</li>
</ul></li>
<li><strong>solution</strong>: normalize by running average of gradient
magnitudes</li>
</ul></li>
</ul>
</section>
<section id="adam-combines-the-two-ideas" class="slide level2">
<h2>Adam: combines the two ideas</h2>
<p><img data-src="https://i.imgur.com/MpiCllk.png" /></p>
</section>
<section id="how-good-is-adam" class="slide level2">
<h2>How good is Adam?</h2>
<p>optimization vs. generalisation</p>
</section>
<section id="how-good-is-adam-1" class="slide level2">
<h2>How good is Adam?</h2>
<p><img data-src="https://i.imgur.com/0yelxmm.png" /></p>
</section>
<section id="how-good-is-adam-2" class="slide level2">
<h2>How good is Adam?</h2>
<p><img data-src="https://i.imgur.com/5rAyMYc.png" /></p>
</section>
<section id="revisiting-the-cartoon-example" class="slide level2">
<h2>Revisiting the cartoon example</h2>
<p><img data-src="https://i.imgur.com/xRYHk0m.png%20=1200x" /></p>
</section>
<section id="can-we-describe-sgds-behaviour" class="slide level2">
<h2>Can we describe SGD’s behaviour?</h2>
<p><img data-src="https://i.imgur.com/AyrSiFZ.png" /></p>
</section>
<section id="analysis-of-mean-iterate" class="slide level2">
<h2>Analysis of mean iterate</h2>
<p><img data-src="https://i.imgur.com/9j85UIv.png" /></p>
<p>(<a href="https://arxiv.org/abs/2101.12176">Smith et al, 2021</a>)
“On the Origin of Implicit Regularization in Stochastic Gradient
Descent”</p>
</section>
<section id="implicit-regularization-in-sgd" class="slide level2">
<h2>Implicit regularization in SGD</h2>
<p><span class="math display">\[
\mathbf{w}_{t+1} = \mathbf{w}_t - \eta \nabla_\mathbf{w}
\hat{L}(\mathbf{w_t}, \mathcal{D}_t)
\]</span></p>
<p>mean iterate in SGD:</p>
<p><span class="math display">\[
\mu_t = \mathbb{E}[\mathbf{w}_t]
\]</span></p>
</section>
<section id="implicit-regularization-in-sgd-1" class="slide level2">
<h2>Implicit regularization in SGD</h2>
<p>(<a href="https://arxiv.org/abs/2101.12176">Smith et al, 2021</a>):
mean iterate approximated as continuous gradient flow:</p>
<p><span class="math display">\[
\small
\dot{\mu}(t) = -\eta \nabla_\mathbf{w}\tilde{L}_{SGD}(\mu(t),
\mathcal{D})
\]</span></p>
<p>where</p>
<p><span class="math display">\[
\small
\tilde{L}_{SGD}(\mathbf{w}, \mathcal{D}) = \tilde{L}_{GD}(\mathbf{w},
\mathcal{D}) +
\frac{\eta}{4}\mathbb{E}\|\nabla_\mathbf{w}\hat{L}(\mathbf{w},
\mathcal{D_t}) - \nabla_\mathbf{w}\hat{L}(\mathbf{w}, \mathcal{D})\|^2
\]</span></p>
</section>
<section id="implicit-regularization-in-sgd-2" class="slide level2">
<h2>Implicit regularization in SGD</h2>
<p>(<a href="https://arxiv.org/abs/2101.12176">Smith et al, 2021</a>):
mean iterate approximated as continuous gradient flow:</p>
<p><span class="math display">\[
\small
\dot{\mu}(t) = -\eta \nabla_\mathbf{w}\tilde{L}_{SGD}(\mu(t),
\mathcal{D})
\]</span></p>
<p>where</p>
<p><span class="math display">\[
\small
\tilde{L}_{SGD}(\mathbf{w}, \mathcal{D}) = \tilde{L}_{GD}(\mathbf{w},
\mathcal{D}) +
\frac{\eta}{4}\underbrace{\mathbb{E}\|\nabla_\mathbf{w}\hat{L}(\mathbf{w},
\mathcal{D_t}) - \nabla_\mathbf{w}\hat{L}(\mathbf{w},
\mathcal{D})\|^2}_{\text{variance of gradients}}
\]</span></p>
</section>
<section id="revisiting-cartoon-example" class="slide level2">
<h2>Revisiting cartoon example</h2>
<p><img data-src="https://i.imgur.com/xRYHk0m.png%20=1200x" /></p>
</section>
<section id="is-stochastic-training-necessary" class="slide level2">
<h2>Is Stochastic Training Necessary?</h2>
<p><img data-src="https://i.imgur.com/1WThiku.png" /></p>
</section>
<section id="is-stochastic-training-necessary-1" class="slide level2">
<h2>Is Stochastic Training Necessary?</h2>
<p><img data-src="https://i.imgur.com/dHJKsKS.png" /></p>
<ul>
<li>reg <span class="math inline">\(\approx\)</span> flatness of
minimum</li>
<li>bs32 <span class="math inline">\(\approx\)</span> variance of
gradients size 32 batches</li>
</ul>
</section>
<section id="sgd-summary" class="slide level2">
<h2>SGD summary</h2>
<ul>
<li>gradient noise is a feature not bug</li>
<li>SGD avoids regions with high gradient noise</li>
<li>this may help with generalization</li>
<li>improved SGD, like Adam, may not always help</li>
<li>an optimization algorithm can be “too good”</li>
</ul>
</section>
    </div>
  </div>

  <script src="https://unpkg.com/reveal.js@3.9.2/lib/js/head.min.js"></script>
  <script src="https://unpkg.com/reveal.js@3.9.2/js/reveal.js"></script>

  <script>

      // Full list of configuration options available at:
      // https://github.com/hakimel/reveal.js#configuration
      Reveal.initialize({
        // Display controls in the bottom right corner
        controls: true,
        // Display a presentation progress bar
        progress: true,
        // Push each slide change to the browser history
        history: true,
        // Enable keyboard shortcuts for navigation
        keyboard: true,
        // Enable the slide overview mode
        overview: true,
        // Vertical centering of slides
        center: true,
        // Enables touch navigation on devices with touch input
        touch: true,
        // Turns fragments on and off globally
        fragments: true,
        // Flags if we should show a help overlay when the questionmark
        // key is pressed
        help: true,
        // Number of milliseconds between automatically proceeding to the
        // next slide, disabled when set to 0, this value can be overwritten
        // by using a data-autoslide attribute on your slides
        autoSlide: 0,
        // Stop auto-sliding after user input
        autoSlideStoppable: true,
        // Transition style
        transition: 'slide', // none/fade/slide/convex/concave/zoom
        // Transition speed
        transitionSpeed: 'default', // default/fast/slow
        // Transition style for full page slide backgrounds
        backgroundTransition: 'fade', // none/fade/slide/convex/concave/zoom
        // Number of slides away from the current that are visible
        viewDistance: 3,
        math: {
          mathjax: 'https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js',
          config: 'TeX-AMS_HTML-full',
          tex2jax: {
            inlineMath: [['\\(','\\)']],
            displayMath: [['\\[','\\]']],
            balanceBraces: true,
            processEscapes: false,
            processRefs: true,
            processEnvironments: true,
            preview: 'TeX',
            skipTags: ['script','noscript','style','textarea','pre','code'],
            ignoreClass: 'tex2jax_ignore',
            processClass: 'tex2jax_process'
          },
        },

        // Optional reveal.js plugins
        dependencies: [
          { src: 'https://unpkg.com/reveal.js@3.9.2/lib/js/classList.js', condition: function() { return !document.body.classList; } },
          { src: 'https://unpkg.com/reveal.js@3.9.2/plugin/zoom-js/zoom.js', async: true },
          { src: 'https://unpkg.com/reveal.js@3.9.2/plugin/math/math.js', async: true },
          { src: 'https://unpkg.com/reveal.js@3.9.2/plugin/notes/notes.js', async: true }
        ]
      });
    </script>
    </body>
</html>
