---
title: "Introduction"
venue: "Computer Laboratory, William Gates Building, 15 J. J. Thomson Avenue"
abstract: "<p>This lecture will give the background to what this course is about, and how it fits in to other material you can find on deep neural network models. It explains deep neural networks are, how they fit into the wider context of the field and why they are succesful.</p>"
author:
- given: Neil D.
  family: Lawrence
  url: http://inverseprobability.com
  institute: 
  twitter: 
  gscholar: 
  orcid: 
date: 2021-01-21
published: 2021-01-21
time: "14:00"
week: 1
session: 1
reveal: 01-01-introduction.slides.html
ipynb: 01-01-introduction.ipynb
layout: lecture
categories:
- notes
---



<!-- Do not edit this file locally. -->
<!---->
<!-- Do not edit this file locally. -->
<!-- Do not edit this file locally. -->
<!-- The last names to be defined. Should be defined entirely in terms of macros from above-->
<!--

-->
<h2 id="course-overview">Course Overview</h2>
<p><span style="text-align:right"><span class="editsection-bracket" style="">[</span><span class="editsection" style=""><a href="https://github.com/lawrennd/talks/edit/gh-pages/_deepnn/includes/overview-2020.md" target="_blank" onclick="ga('send', 'event', 'Edit Page', 'Edit', 'https://github.com/lawrennd/talks/edit/gh-pages/_deepnn/includes/overview-2020.md', 13);">edit</a></span><span class="editsection-bracket" style="">]</span></span></p>
<p>Deep Neural Networks is an eight we course that introduces you to the fundamental concepts behind deep learning with neural networks. Over the last decade, deep neural network models have been behind some of the most impressive feats in machine learning. From the <a href="https://en.wikipedia.org/wiki/AlexNet">convolutional neural networks</a> that made breakthrough progess on the <a href="https://en.wikipedia.org/wiki/ImageNet">ImageNet challenge</a>, object detection and the image net challenge, and most recently with the <a href="https://predictioncenter.org/casp14/doc/CASP14_press_release.html">CASP14 Protein Folding result from DeepMind</a>.</p>
<p>Welcome to this course, which is designed to introduce you to the principles and ideas behind deep neural network models.</p>
<p>Due to widespread international interest in neural networks there is now a great deal of material to help you train and deploy these models. It is not our aim to substitute this material but to augment it with deeper understanding of the reasons why deep learning is successful and the context in which that success has occurred.</p>
<p>You are strongly encouraged to explore that material. For example, you can find <a href="https://atcold.github.io/pytorch-Deep-Learning/">Yann LeCun’s neural networks course from NYU here</a>. For example, <a href="https://atcold.github.io/pytorch-Deep-Learning/">here is their introductory session</a> with Yann giving the first lecture available here.</p>
<div class="figure">
<div id="nyu-deep-learning-figure" class="figure-frame">
<iframe width="800" height="600" src="https://www.youtube.com/embed/0bMe_vCZo30?start=" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen>
</iframe>
</div>
<div id="nyu-deep-learning-magnify" class="magnify" onclick="magnifyFigure(&#39;nyu-deep-learning&#39;)">
<img class="img-button" src="{{ '/assets/images/Magnify_Large.svg' | relative_url }}" style="width:1.5ex">
</div>
<div id="nyu-deep-learning-caption" class="caption-frame">
<p>Figure: Lecture from the NYU course on Deep Learning given by Yann LeCun, Alfredo Canziani and Mark Goldstein</p>
</div>
</div>
<p>The full playlist is <a href="https://www.youtube.com/playlist?list=PLLHTzKZzVU9eaEyErdV26ikyolxOsz6mq">available here</a>.</p>
<div class="centered" style="">
<svg viewBox="0 0 200 200" style="width:15%">
<defs> <clipPath id="clip0">
<style>
circle {
  fill: black;
}
</style>
<circle cx="100" cy="100" r="100"/> </clipPath> </defs>
<title>
Ferenc Huszar
</title>
<image preserveAspectRatio="xMinYMin slice" width="100%" xlink:href="../slides/diagrams/people/ferenc-huszar.jpg" clip-path="url(#clip0)"/>
</svg>
<svg viewBox="0 0 200 200" style="width:15%">
<defs> <clipPath id="clip1">
<style>
circle {
  fill: black;
}
</style>
<circle cx="100" cy="100" r="100"/> </clipPath> </defs>
<title>
Nic Lane
</title>
<image preserveAspectRatio="xMinYMin slice" width="100%" xlink:href="../slides/diagrams/people/nic-lane.jpg" clip-path="url(#clip1)"/>
</svg>
<svg viewBox="0 0 200 200" style="width:15%">
<defs> <clipPath id="clip2">
<style>
circle {
  fill: black;
}
</style>
<circle cx="100" cy="100" r="100"/> </clipPath> </defs>
<title>
Neil Lawrence
</title>
<image preserveAspectRatio="xMinYMin slice" width="100%" xlink:href="../slides/diagrams/people/neil-lawrence.png" clip-path="url(#clip2)"/>
</svg>
</div>
<p>Our course is taught by Ferenc Huszár, Nic Lane and Neil Lawrence.</p>
<p>Alongside such teaching material there are frameworks such as PyTorch, that help you in constructing these models and deploying the training on GPUs.</p>
<p>Neural network models are not new, and this is not the first wave of interest in them, it is the third wave. There are reasons why they have become more successful in this wave than in their first two incarnations. Those reasons include the context in which they were reintroduced. In particular, the wide availability of <em>data</em> and <em>fast compute</em> have been critical in their success.</p>
<p>Our aim is to give you the understanding of that context, and therefore deepen your understanding of neural networks, how and <em>when</em> they should be used. The neural network is not a panacea, it does not solve all the challenges of machine learning (at least not in its current incarnation). But it is a powerful framework for flexible modelling of data. Just like any powerful framework, it is important to understand its strengths as well as its limitations.</p>
<h2 id="schedule">Schedule</h2>
<ul>
<li>Week 1:
<ol type="1">
<li>Introduction and Data. Lecturer: <a href="http://inverseprobability.com/">Neil D. Lawrence</a></li>
<li>Generalisation and Neural Networks. Lectuer: <a href="http://inverseprobability.com/">Neil D. Lawrence</a></li>
</ol></li>
<li>Week 2:
<ol type="1">
<li>Automatic Differentiation. Lecturer: <a href="https://www.inference.vc/about/">Ferenc Huszár</a></li>
<li>Optimization and Stochastic Gradient Descent. Lecturer: <a href="https://www.inference.vc/about/">Ferenc Huszár</a></li>
</ol></li>
<li>Week 3:
<ol type="1">
<li>Hardware. Lecturer: <a href="http://niclane.org/">Nic Lane</a></li>
<li>Summary and Gap Filling: <a href="https://www.inference.vc/about/">Ferenc Huszár</a>, <a href="http://inverseprobability.com/">Neil D. Lawrence</a>, <a href="http://niclane.org/">Nic Lane</a></li>
</ol></li>
</ul>
<p><strong>Set Assignment 1 (30%)</strong></p>
<ul>
<li>Week 4:
<ol type="1">
<li>Convolutional Neural Networks: <a href="http://niclane.org/">Nic Lane</a>, <a href="https://www.inference.vc/about/">Ferenc Huszár</a></li>
<li>Recurrent Neural Networks: <a href="http://niclane.org/">Nic Lane</a>, <a href="https://www.inference.vc/about/">Ferenc Huszár</a></li>
</ol></li>
</ul>
<p><strong>Assignment 1 Submitted</strong></p>
<ul>
<li>Week 5:
<ol type="1">
<li>Sequence to Sequence and Attention: <a href="https://www.inference.vc/about/">Ferenc Huszár</a></li>
<li>Transformers: <a href="https://www.inference.vc/about/">Ferenc Huszár</a></li>
</ol></li>
</ul>
<p><strong>Set Assignment 2 (70%)</strong></p>
<h2 id="special-topics">Special Topics</h2>
<p>Weeks 6-8 will involve a series of guest lectures and discussion sessions to relate the fundamental material you’ve learnt about to how people are deploying these models in the real world and how they are innovating with these models to extend their capabilities.</p>
<p>The two assignments will make up the entire mark of the module. They should be submitted via Moodle.</p>
<h1 id="what-is-machine-learning">What is Machine Learning?</h1>
<p><span style="text-align:right"><span class="editsection-bracket" style="">[</span><span class="editsection" style=""><a href="https://github.com/lawrennd/talks/edit/gh-pages/_ml/includes/what-is-ml.md" target="_blank" onclick="ga('send', 'event', 'Edit Page', 'Edit', 'https://github.com/lawrennd/talks/edit/gh-pages/_ml/includes/what-is-ml.md', 13);">edit</a></span><span class="editsection-bracket" style="">]</span></span></p>
<p>What is machine learning? At its most basic level machine learning is a combination of</p>
<p><span class="math display">\[\text{data} + \text{model} \stackrel{\text{compute}}{\rightarrow} \text{prediction}\]</span></p>
<p>where <em>data</em> is our observations. They can be actively or passively acquired (meta-data). The <em>model</em> contains our assumptions, based on previous experience. That experience can be other data, it can come from transfer learning, or it can merely be our beliefs about the regularities of the universe. In humans our models include our inductive biases. The <em>prediction</em> is an action to be taken or a categorization or a quality score. The reason that machine learning has become a mainstay of artificial intelligence is the importance of predictions in artificial intelligence. The data and the model are combined through computation.</p>
<p>In practice we normally perform machine learning using two functions. To combine data with a model we typically make use of:</p>
<p><strong>a prediction function</strong> a function which is used to make the predictions. It includes our beliefs about the regularities of the universe, our assumptions about how the world works, e.g. smoothness, spatial similarities, temporal similarities.</p>
<p><strong>an objective function</strong> a function which defines the cost of misprediction. Typically it includes knowledge about the world’s generating processes (probabilistic objectives) or the costs we pay for mispredictions (empiricial risk minimization).</p>
<p>The combination of data and model through the prediction function and the objective function leads to a <em>learning algorithm</em>. The class of prediction functions and objective functions we can make use of is restricted by the algorithms they lead to. If the prediction function or the objective function are too complex, then it can be difficult to find an appropriate learning algorithm. Much of the acdemic field of machine learning is the quest for new learning algorithms that allow us to bring different types of models and data together.</p>
<p>A useful reference for state of the art in machine learning is the UK Royal Society Report, <a href="https://royalsociety.org/~/media/policy/projects/machine-learning/publications/machine-learning-report.pdf">Machine Learning: Power and Promise of Computers that Learn by Example</a>.</p>
<p>You can also check my post blog post on <a href="http://inverseprobability.com/2017/07/17/what-is-machine-learning">What is Machine Learning?</a>..</p>
<h2 id="ingredients">Ingredients</h2>
<p><span style="text-align:right"><span class="editsection-bracket" style="">[</span><span class="editsection" style=""><a href="https://github.com/lawrennd/talks/edit/gh-pages/introduction.mdtmp.md" target="_blank" onclick="ga('send', 'event', 'Edit Page', 'Edit', 'https://github.com/lawrennd/talks/edit/gh-pages/introduction.mdtmp.md', 13);">edit</a></span><span class="editsection-bracket" style="">]</span></span></p>
<p>So the three key ingredients of machine learning are a model, data and compute. Note, that this necessarily implies that an <em>algorithm</em> exists to combine the model with the data and that algorithm is what consumes the compute.</p>
<p>So how do these ingredients pan out in our recipe for deep learning? To better understand this, we’re going to add more historical context and go back go 1997 when, here in Cambridge, there was a six month programme at the Isaac Newton Institute run on Machine Learning, Neural Networks and Generalisation.</p>
<h2 id="cybernetics-neural-networks-and-the-ratio-club">Cybernetics, Neural Networks and the Ratio Club</h2>
<p><span style="text-align:right"><span class="editsection-bracket" style="">[</span><span class="editsection" style=""><a href="https://github.com/lawrennd/talks/edit/gh-pages/_ml/includes/cybernetics-ratio-club.md" target="_blank" onclick="ga('send', 'event', 'Edit Page', 'Edit', 'https://github.com/lawrennd/talks/edit/gh-pages/_ml/includes/cybernetics-ratio-club.md', 13);">edit</a></span><span class="editsection-bracket" style="">]</span></span></p>
<p>This is certainly not the first wave of excitment in neural networks. This history of neural networks predates the history of the computer, and papers on neural networks predate papers on the digital computer.</p>
<div class="figure">
<div id="russell-pitts-mcculloch-figure" class="figure-frame">
<table>
<tr>
<td width="30%">
<div class="centered centered" style="">
<img class="" src="../slides/diagrams/philosophy/Bertrand_Russell_1957.jpg" width="100%" height="auto" align="center" style="background:none; border:none; box-shadow:none; display:block; margin-left:auto; margin-right:auto;vertical-align:middle">
</div>
</td>
<td width="30%">
<div class="centered centered" style="">
<img class="" src="../slides/diagrams/ml/Lettvin_Pitts.jpg" width="100%" height="auto" align="center" style="background:none; border:none; box-shadow:none; display:block; margin-left:auto; margin-right:auto;vertical-align:middle">
</div>
</td>
<td width="30%">
<div class="centered centered" style="">
<img class="" src="../slides/diagrams/ml/warren_mcculloch.jpg" width="100%" height="auto" align="center" style="background:none; border:none; box-shadow:none; display:block; margin-left:auto; margin-right:auto;vertical-align:middle">
</div>
</td>
</tr>
</table>
</div>
<div id="russell-pitts-mcculloch-magnify" class="magnify" onclick="magnifyFigure(&#39;russell-pitts-mcculloch&#39;)">
<img class="img-button" src="{{ '/assets/images/Magnify_Large.svg' | relative_url }}" style="width:1.5ex">
</div>
<div id="russell-pitts-mcculloch-caption" class="caption-frame">
<p>Figure: Bertrand Russell (1872-1970), Walter Pitts, <em>right</em> (1923-1969), Warren McCulloch (1898-1969)</p>
</div>
</div>
<p>Specifically, one of the first papers on neural networks was written by two collaborators from logic and psychology in 1943. <a href="https://en.wikipedia.org/wiki/Walter_Pitts">Walter Pitts</a> was a child prodigy who read Russell and Whitehead’s <em>Principia Mathematica</em>. He felt he’d spotted some errors in the text and wrote to Russell in Cambridge, who replied inviting him for a visit. Pitts did not take up the offer because he was only 12 years old. But, three years later, when Russell took a sabbatical at the University of Chicago, Pitts left his home in Detroit and headed to Chicago to hear Russell speak. When Russell left Pitts stayed on studying logic. He never formally took a degree but just worked with whoever was available.</p>
<p><a href="https://en.wikipedia.org/wiki/Warren_Sturgis_McCulloch">Warren McCulloch</a> was a psychologist who moved to the University of Chicago in 1941. Overlapping interests meant that he met Pitts, who was still living an intinerant lifestyle around the campus of the University. McCulloch invited Pitts to live with his family and they began collaborating on a simple model of the neuron and how neurons might interact. The dominant ‘theory of knowledge’ at the time was <em>logic</em> and their paper attempted to show how networks of neurons (or ir paper attempted to bridge the logical foundation. Their paper, <em>A Logical Calculus of the Ideas Immanent in Nervous Activity</em> <span class="citation" data-cites="McCulloch:neuron43">(McCulloch and Pitts, 1943)</span> was published in the middle of the Second World War. It modelled the neuron as a linear threshold, and described how networks of such neurons could create logical functions. The inspiration in the paper is clear, they make use of Rudolf Carmap’s Language II <span class="citation" data-cites="Carnap-logical37">(Carnap, 1937)</span> to represent their theorem and cite the second edition of Russell and Whitehead <span class="citation" data-cites="Russell-principia25">(Russell and Whitehead, 1925)</span>.</p>
<h2 id="cybernetics">Cybernetics</h2>
<div class="figure">
<div id="maxwell-gibbs-wiener-figure" class="figure-frame">
<table>
<tr>
<td width="30%">
<div class="centered" style="">
<img class="" src="../slides/diagrams/physics/james-clerk-maxwell.png" width="100%" height="auto" align="center" style="background:none; border:none; box-shadow:none; display:block; margin-left:auto; margin-right:auto;vertical-align:middle">
</div>
</td>
<td width="30%">
<div class="centered centered" style="">
<img class="" src="../slides/diagrams/physics/j-w-gibbs.jpg" width="100%" height="auto" align="center" style="background:none; border:none; box-shadow:none; display:block; margin-left:auto; margin-right:auto;vertical-align:middle">
</div>
</td>
<td width="30%">
<div class="centered centered" style="">
<img class="" src="../slides/diagrams/physics/Norbert_wiener.jpg" width="100%" height="auto" align="center" style="background:none; border:none; box-shadow:none; display:block; margin-left:auto; margin-right:auto;vertical-align:middle">
</div>
</td>
</tr>
</table>
</div>
<div id="maxwell-gibbs-wiener-magnify" class="magnify" onclick="magnifyFigure(&#39;maxwell-gibbs-wiener&#39;)">
<img class="img-button" src="{{ '/assets/images/Magnify_Large.svg' | relative_url }}" style="width:1.5ex">
</div>
<div id="maxwell-gibbs-wiener-caption" class="caption-frame">
<p>Figure: James Clerk Maxwell (1831-1879), Josiah Willard Gibbs (1839-1903), Norbert Wiener (1894-1964)</p>
</div>
</div>
<p>After the war, this work, along with McCulloch and Pitts, was at the heart of a movement known as <em>Cybernetics</em>. A term coined by <a href="https://en.wikipedia.org/wiki/Norbert_Wiener">Norbert Wiener</a> <span class="citation" data-cites="Wiener:cybernetics48">(Wiener, 1948)</span> to reflect the wealth of work on sensing and computing. Wiener chose the term as an alternative rendering of the word <em>governor</em>. Governor comes to us form latin, but is a corruption of the Greek κυβερνήτης meaning helmsman. Wiener’s choice of the term was a nod to the importance of James Clerk Maxwell’s work on understanding surging in James Watt’s steam engine governor <span class="citation" data-cites="Maxwell:governors1867">(Maxwell, 1867)</span>. It reflected the importance that Wiener placed on <em>feedback</em> in these systems. From this strand of work came the field of <em>control theory</em>.</p>
<p>Many of the constituent ideas of Cybernetics came from the war itself. Norbert Wiener, was a Professor of Applied Mathematics at MIT. He was another child prodigy who visited Russell in Cambridge having completed his PhD at Harvard by the age of 19. But Wiener was less keen on logic than McCulloch and Pitts, he looked to stochastic processes and probability theory as the key to intelligent decision making. He rejected the need for a ‘theory of knowledge’ and preferred to think of a ‘theory of ignorance’ which was inspired by statistical mechanics, Maxwell was also a originator of the field, but Wiener wrote of Josiah Willard Gibbs as being his inspiration.</p>
<p>This nascent community was mainly based on those who were involved in war work. Wiener worked on radar systems for tracking aircraft (leading to the Wiener filter <span class="citation" data-cites="Wiener:yellow49">(Wiener, 1949)</span>). In the UK researchers such as Jack Good, Alan Turing, Donald MacKay, Ross Ashby formed the <em>Ratio Club</em>. A group of scientists interested in how the brain works and how it might be modelled. Many of these scientists also worked on radar systems or code breaking.</p>
<h2 id="analogue-and-digital">Analogue and Digital</h2>
<div class="figure">
<div id="donald-maccrimmon-mackay-figure" class="figure-frame">
<div class="centered centered" style="">
<img class="" src="../slides/diagrams/physics/dmaccrimmonmackay.jpg" width="30%" height="auto" align="center" style="background:none; border:none; box-shadow:none; display:block; margin-left:auto; margin-right:auto;vertical-align:middle">
</div>
</div>
<div id="donald-maccrimmon-mackay-magnify" class="magnify" onclick="magnifyFigure(&#39;donald-maccrimmon-mackay&#39;)">
<img class="img-button" src="{{ '/assets/images/Magnify_Large.svg' | relative_url }}" style="width:1.5ex">
</div>
<div id="donald-maccrimmon-mackay-caption" class="caption-frame">
<p>Figure: Donald M. MacKay (1922-1987), an early member of the Cybernetics community and member of the Ratio Club.</p>
</div>
</div>
<p>Donald MacKay wrote of the influence that his own work on radar had on his interest in the brain.</p>
<blockquote>
<p>… during the war I had worked on the theory of automated and electronic computing and on the theory of information, all of which are highly relevant to such things as automatic pilots and automatic gun direction. I found myself grappling with problems in the design of artificial sense organs for naval gun-directors and with the principles on which electronic circuits could be used to simulate situations in the external world so as to provide goal-directed guidance for ships, aircraft, missiles and the like.</p>
<p>Later in the 1940’s, when I was doing my Ph.D. work, there was much talk of the brain as a computer and of the early digital computers that were just making the headlines as “electronic brains.” As an analogue computer man I felt strongly convinced that the brain, whatever it was, was not a digital computer. I didn’t think it was an analogue computer either in the conventional sense.</p>
<p>But this naturally rubbed under my skin the question: well, if it is not either of these, what kind of system is it? Is there any way of following through the kind of analysis that is appropriate to their artificial automata so as to understand better the kind of system the human brain is? That was the beginning of my slippery slope into brain research.</p>
<p><em>Behind the Eye</em> pg 40. Edited version of The 1986 Gifford Lectures given by Donald M. MacKay and edited by Valerie MacKay</p>
</blockquote>
<p>Importantly, MacKay distinguishes between the <em>analogue</em> computer and the <em>digital</em> computer. As he mentions, his experience was with analogue machines. An analogue machine is <em>literally</em> an analogue. The radar systems that Wiener and MacKay both worked on were made up of electronic components such as resistors, capacitors and inductors, that together represented a physical system, such as an anti-aircraft gun and a plane. The design of the analogue computer required the engineer to simulate the real world in analogue electronics, using dualities that exist between e.g. mechanical circuits (mass, spring, damper) and electroni circuits (inductor, resistor, capacitor). The analogy between mass and a damper, between spring and a resistor and between capacitor and a damper works because the underlying mathematics is approximated with the same linear system: a second order differential equation. This mathematical analogy allowed the designer to map from the real world, through mathematics, to a virtual world where the components reflected the real world through analogy.</p>
<p>This is a quite different from the approach that McCulloch and Pitts were taking with their paper on the logical calculus of the nervous system. They were attempting to map their model of the neuron onto logic. Logical reasoning was the mainstay of the contemporary understanding of intelligence. But the components they were considering were neurons, they could only map onto the logical world because their analogy for the neuron was so simple. An ‘on’ or ‘off’ linear threshold unit. Where the synapses of the neuron were compared to a threshold in the neuron. Firing occurs when the sum of input neurons cross a threshold in the receiving neuron. These networks can then be built together in cascades.</p>
<p>In the late 1940s and early 1950s, Cyberneticists were also working on <em>digital computers</em>. The type of machines (such as Colossus, built by Tommy Flowers, and known to Turing) and the ENIAC. Indeed, by 1949 Cambridge had built its own machine, the EDSAC in the Mathematical Laboratory, inspired by “draft of a report on the EDVAC” by von Neumann.</p>
<p>Digital computers are themselves (at their heart) a series of analogue devices, but in the digital computer, the analogue is between collections of transistors and logic gates. Logic gates allow the computer to reconstruct <em>logical truth tables</em>, which Wittgenstein had popularised in his <em>Tractatus Logico-Philosophicus</em> <span class="citation" data-cites="Wittgenstein-logico21">(Wittgenstein, 1922)</span>. The mathematical operations we need can be reconstructed through logic, so this ‘analogue machine’ which we call a <em>digital computer</em> becomes capable of directly computing the mathematics we’re interested in, rather than going via the analogue machine.</p>
<h2 id="the-perceptron">The Perceptron</h2>
<div class="figure">
<div id="ashby-neumann-rosenblatt-figure" class="figure-frame">
<table>
<tr>
<td width="30%">
<div class="centered centered" style="">
<img class="" src="../slides/diagrams/ml/w-ross-ashby.jpg" width="100%" height="auto" align="center" style="background:none; border:none; box-shadow:none; display:block; margin-left:auto; margin-right:auto;vertical-align:middle">
</div>
</td>
<td width="30%">
<div class="centered" style="">
<img class="" src="../slides/diagrams/physics/JohnvonNeumann-LosAlamos.gif" width="100%" height="auto" align="center" style="background:none; border:none; box-shadow:none; display:block; margin-left:auto; margin-right:auto;vertical-align:middle">
</div>
</td>
<td width="30%">
<div class="centered centered" style="">
<img class="" src="../slides/diagrams/Frank_Rosenblatt.jpg" width="100%" height="auto" align="center" style="background:none; border:none; box-shadow:none; display:block; margin-left:auto; margin-right:auto;vertical-align:middle">
</div>
</td>
</tr>
</table>
</div>
<div id="ashby-neumann-rosenblatt-magnify" class="magnify" onclick="magnifyFigure(&#39;ashby-neumann-rosenblatt&#39;)">
<img class="img-button" src="{{ '/assets/images/Magnify_Large.svg' | relative_url }}" style="width:1.5ex">
</div>
<div id="ashby-neumann-rosenblatt-caption" class="caption-frame">
<p>Figure: W. Ross Ashby (1903-1972), John von Neumann (1903-1957), Frank Rosenblatt (1928-1971). *Photograph of W. Ross Ashby is Copyright W. Ross Ashby.</p>
</div>
</div>
<p>So the early story of Cybernetics starts with the success of analogue control, and in the domain of neural networks, analogues built. Inspired by F. Ross Ashby’s <span class="citation" data-cites="Ashby-design52">(Ashby, 1952)</span> ideas that suggested <em>random connections</em> and von Neumann <span class="citation" data-cites="Neumann-probabilistic56">(Neumann, 1956)</span>, who wrote about <em>probabilistic logics</em>, Frank Rosenblatt constructed the Perceptron <span class="citation" data-cites="Rosenblatt-perceptron58">(Rosenblatt, 1958)</span>. This was a deep neural network that recognised images from TV cameras.</p>
<p>The perceptron created a great deal of interest, but it was rapidly eclipsed by the emerging <em>digital computer</em>. Perhaps as a marker as the increasing importance of the digital computer, the Apollo program (1961-1972) had a guidance computer that was a 16 bit digital machine for navigating to the moon. It implemented Kalman filters for guidance. In signal processing there’s a shift from the 1950s to the 1960s of reseachers moving from analogue designs to designs that are suitable for implementation of digital machines.</p>
<p>That same shift was imposed on the Cybernetics community. Artificial Inteligence is often traced to the ‘summer research project’ proposed by John McCarthy at Dartmouth. But the proposal is not just notable for the introduction of the term, it is notable for the extent to which it marks a break with the Cybernetics community. Wiener’s name isn’t even mentioned in the proposal. And Cyberneticists either weren’t invited to, or couldn’t make, the event (with the notable exception of Warren McCulloch). Turing had died, and von Neumann was seriously ill with cancer. W. Ross Ashby was even in the US, but from his diaries there’s no trace of him attending. Donald MacKay was on the initial proposal, but didn’t make the event (perhaps because that summer his son, <a href="https://warwick.ac.uk/fac/sci/maths/people/staff/robert_mackay/">Robert</a>, was born).</p>
<p>One of the great ironies of modern artificial intelligence is that it is almost wholly reliant on deep neural network methodologies for the recent breakthroughs. But the dawn of the term artificial intelligence is associated with a period of around three decades when those methods (and the community that originated them) was actively marginalised.</p>
<p>There were many reasons for this, including personal enimities between Wiener and McCulloch, the untimely death of Frank Rosenblatt in a sailing accident. But regardless of these personal tragedies and tales of academic politics, the principal reason that neural networks were eclipsed was the dominance of the digital computer. From 1956 to 1986 we saw the rise of the computer from a tool for science and big business to a personal machine, available to individual researchers.</p>
<h2 id="the-connectionists">The Connectionists</h2>
<p><span style="text-align:right"><span class="editsection-bracket" style="">[</span><span class="editsection" style=""><a href="https://github.com/lawrennd/talks/edit/gh-pages/_ml/includes/connectionist-revival.md" target="_blank" onclick="ga('send', 'event', 'Edit Page', 'Edit', 'https://github.com/lawrennd/talks/edit/gh-pages/_ml/includes/connectionist-revival.md', 13);">edit</a></span><span class="editsection-bracket" style="">]</span></span></p>
<p>By the early 1980s, some disillusionment was creeping into the artificial intelligence agenda that stemmed from the Dartmouth Meeting. At the same time, an emerging group was re-examining the ideas of the Cyberneticists. This group became known as the connectionists, because of their focus on neural network models with their myriad of connections.</p>
<p>By the second half of the decade, some of their principles had come together to form a movement. Meetings including the Snowbird Workshop, Neural Informaton Processing Systems and the connectionist summer school provided a venue for these researchers to come together.</p>
<div class="figure">
<div id="-figure" class="figure-frame">
<div class="centered centered" style="">
<img class="" src="../slides/diagrams/ml/connectionist-summer-school.jpg" width="70%" height="auto" align="center" style="background:none; border:none; box-shadow:none; display:block; margin-left:auto; margin-right:auto;vertical-align:middle">
</div>
</div>
<div id="-magnify" class="magnify" onclick="magnifyFigure(&#39;&#39;)">
<img class="img-button" src="{{ '/assets/images/Magnify_Large.svg' | relative_url }}" style="width:1.5ex">
</div>
<div id="-caption" class="caption-frame">
<p>Figure: Group photo from the 1986 Connectionists’ Summer School, held at CMU in July. Included in the photo are Richard Durbin, Terry Sejnowski, Geoff Hinton, Yann LeCun, Michael I. Jordan.</p>
</div>
</div>
<p>This was also the era of cheap(er) computing. Connectionists were able to implement <em>simulators</em> of neural networks in <em>minicomputers</em> such as DEC’s PDP-11 series. These allowed new architectures to be tried. Among them was backpropagation, popularised by the “Parallel Distributed Processing” books [Rumelhart:book86], these books formed the canonical ideas on which the connectionists based their research.</p>
<blockquote>
<p>What makes people smarter than machines? They certainly are not quicker or more precise. Yet people are far better at perceiving objects in natural scenes and noting their relations, at understanding language and retrieving contextually appropriate information from memory, at making plans and carrying out contextually appropriate actions, and at a wide range of other natural cognitive tasks. People are also far better at learning to do these things more accurately and fluently through processing experience.</p>
<p>What is the basis for these differences? One answer, perhaps the classic one we might expect from artificial intelligence, is “software.” If we only had the right computer program, the argument goes, we might be able to capture the fluidity and adaptability of human information processing.</p>
<p>Certainly this answer is partially correct. There have been great breakthroughs in our understanding of cognition as a result of the development of expressive high-level computer languages and powerful algorithms. No doubt there will be more such breakthroughs in the future. However, we do not think that software is the whole story.</p>
<p>In our view, people are smarter than today’s computers because the brain employs a basic computational architecture that is more suited to deal with a central aspect of the natural information processing tasks that people are so good at.</p>
<p>J. L. McClelland, David E. Rumelhart and Geoffrey E. Hinton in <em>Parallel Distributed Processing,</em> <span class="citation" data-cites="Rumelhart:book86">Rumelhart et al. (1986)</span></p>
</blockquote>
<div class="figure">
<div id="pdp-volume-1-figure" class="figure-frame">
<div class="centered centered" style="">
<img class="" src="../slides/diagrams/ml/pdp_cover.jpg" width="40%" height="auto" align="center" style="background:none; border:none; box-shadow:none; display:block; margin-left:auto; margin-right:auto;vertical-align:middle">
</div>
</div>
<div id="pdp-volume-1-magnify" class="magnify" onclick="magnifyFigure(&#39;pdp-volume-1&#39;)">
<img class="img-button" src="{{ '/assets/images/Magnify_Large.svg' | relative_url }}" style="width:1.5ex">
</div>
<div id="pdp-volume-1-caption" class="caption-frame">
<p>Figure: Cover of the Parallel Distributed Processing edited volume <span class="citation" data-cites="Rumelhart:book86">(Rumelhart et al., 1986)</span>.</p>
</div>
</div>
<p>This led to the second wave of neural network architectures. A new journal, <em>Neural Computation</em> was launched in 1989 to cater for this new field. It’s first volume contained a new architecture: the convolutional neural networks <span class="citation" data-cites="LeCun:zip89">(Le Cun et al., 1989)</span>, developed for recognising hand written digits. It made the cover.</p>
<div class="figure">
<div id="nc-cover-lecun-figure" class="figure-frame">
<div class="centered centered" style="">
<img class="" src="../slides/diagrams/ml/nc_cover.jpg" width="40%" height="auto" align="center" style="background:none; border:none; box-shadow:none; display:block; margin-left:auto; margin-right:auto;vertical-align:middle">
</div>
</div>
<div id="nc-cover-lecun-magnify" class="magnify" onclick="magnifyFigure(&#39;nc-cover-lecun&#39;)">
<img class="img-button" src="{{ '/assets/images/Magnify_Large.svg' | relative_url }}" style="width:1.5ex">
</div>
<div id="nc-cover-lecun-caption" class="caption-frame">
<p>Figure: Cover of <em>Neural Computation</em>, Volume 1, Issue 4 containing <span class="citation" data-cites="LeCun:zip89">Le Cun et al. (1989)</span>. The cover shows examples from the U.S. Postal Service data set of handwritten digits.</p>
</div>
</div>
<p>It’s worth noting what compute and data that LeCun and collaborators had avaialble. Experiments were run on a SUN-4/260, with a CPU running at 16.67 MHz and 128 MB of RAM. It’s an impressive machine for the day. The neural network had just under 10,000 parameters and the training data consisted of 7,291 digitized training images on a 16 x 16 grid, 2007 images were retained for testing. The model had a 5% error rate on the test data.</p>
<blockquote>
<p>The first several stages of processing in our previous system (described in Denker et al. 1989) involved convolutions in which the coefficients had been laboriously hand designed. In the present system, the first two layers of the network are constrained to be convolutional, but the system automatically learns the coefficients that make up the kernels.</p>
<p>Section 5.1 in <span class="citation" data-cites="LeCun:zip89">Le Cun et al. (1989)</span></p>
</blockquote>
<p>The second half of the 1980s and the 1990s were a period of growth and innovation for the community. Recurrent neural networks that operated through time were produced including in 1997, the Long Short-Term Memory architecture <span class="citation" data-cites="Hochreiter-lstm97">(Hochreiter and Schmidhuber, 1997)</span>, a form of recurrent neural network that can deal with sequence data.</p>
<!--include{_ml/includes/what-does-machine-learning-do.md}-->
<div class="figure">
<div id="neil-newton-institute-figure" class="figure-frame">
<div class="centered centered" style="">
<img class="" src="../slides/diagrams/people/1997-08-02-neil-newton-institute.jpg" width="80%" height="auto" align="center" style="background:none; border:none; box-shadow:none; display:block; margin-left:auto; margin-right:auto;vertical-align:middle">
</div>
</div>
<div id="neil-newton-institute-magnify" class="magnify" onclick="magnifyFigure(&#39;neil-newton-institute&#39;)">
<img class="img-button" src="{{ '/assets/images/Magnify_Large.svg' | relative_url }}" style="width:1.5ex">
</div>
<div id="neil-newton-institute-caption" class="caption-frame">
<p>Figure: Neil standing outside the Newton Institute on 2nd August 1997, just after arriving for “Generalisation in Neural Networks and Machine Learning”, <a href="http://www.newton.ac.uk/files/reports/annual/ini_annual_report_97-98.pdf">see page 26-30 of this report</a>.</p>
</div>
</div>
<div class="figure">
<div id="le-net-5-figure" class="figure-frame">
<div class="centered" style="">
<img class="" src="../slides/diagrams/ml/le-net-5.gif" width="60%" height="auto" align="center" style="background:none; border:none; box-shadow:none; display:block; margin-left:auto; margin-right:auto;vertical-align:middle">
</div>
</div>
<div id="le-net-5-magnify" class="magnify" onclick="magnifyFigure(&#39;le-net-5&#39;)">
<img class="img-button" src="{{ '/assets/images/Magnify_Large.svg' | relative_url }}" style="width:1.5ex">
</div>
<div id="le-net-5-caption" class="caption-frame">
<p>Figure: Gif animation of LeNet-5 in action.</p>
</div>
</div>
<div class="figure">
<div id="le-net-5-figure" class="figure-frame">
<div class="centered" style="">
<img class="" src="../slides/diagrams/ml/le-net-translate.gif" width="60%" height="auto" align="center" style="background:none; border:none; box-shadow:none; display:block; margin-left:auto; margin-right:auto;vertical-align:middle">
</div>
</div>
<div id="le-net-5-magnify" class="magnify" onclick="magnifyFigure(&#39;le-net-5&#39;)">
<img class="img-button" src="{{ '/assets/images/Magnify_Large.svg' | relative_url }}" style="width:1.5ex">
</div>
<div id="le-net-5-caption" class="caption-frame">
<p>Figure: Gif animation of LeNet-5 in action. Here the translation invariance of the network is being tested.</p>
</div>
</div>
<div class="figure">
<div id="le-net-5-figure" class="figure-frame">
<div class="centered" style="">
<img class="" src="../slides/diagrams/ml/le-net-scale.gif" width="60%" height="auto" align="center" style="background:none; border:none; box-shadow:none; display:block; margin-left:auto; margin-right:auto;vertical-align:middle">
</div>
</div>
<div id="le-net-5-magnify" class="magnify" onclick="magnifyFigure(&#39;le-net-5&#39;)">
<img class="img-button" src="{{ '/assets/images/Magnify_Large.svg' | relative_url }}" style="width:1.5ex">
</div>
<div id="le-net-5-caption" class="caption-frame">
<p>Figure: Gif animation of LeNet-5 in action, here the scale invariance of the network is being tested.</p>
</div>
</div>
<p>But at the same meeting Vladmir Vapnik was there as was Bernhard Scholkopf. Corinna Cortes, Bernard Boser, Isabelle Guyon and Vladmir Vapnik were all instrumental in developing the Support Vector Machine.</p>
<div class="figure">
<div id="cortes-guyon-vapnik-figure" class="figure-frame">
<table>
<tr>
<td width="30%">
<div class="centered centered" style="">
<img class="" src="../slides/diagrams/ml/corinna-cortes.jpg" width="100%" height="auto" align="center" style="background:none; border:none; box-shadow:none; display:block; margin-left:auto; margin-right:auto;vertical-align:middle">
</div>
</td>
<td width="30%">
<div class="centered centered" style="">
<img class="" src="../slides/diagrams/ml/isabelle-guyon.jpg" width="100%" height="auto" align="center" style="background:none; border:none; box-shadow:none; display:block; margin-left:auto; margin-right:auto;vertical-align:middle">
</div>
</td>
<td width="30%">
<div class="centered centered" style="">
<img class="" src="../slides/diagrams/ml/vladmir-vapnik.jpg" width="100%" height="auto" align="center" style="background:none; border:none; box-shadow:none; display:block; margin-left:auto; margin-right:auto;vertical-align:middle">
</div>
</td>
</tr>
</table>
</div>
<div id="cortes-guyon-vapnik-magnify" class="magnify" onclick="magnifyFigure(&#39;cortes-guyon-vapnik&#39;)">
<img class="img-button" src="{{ '/assets/images/Magnify_Large.svg' | relative_url }}" style="width:1.5ex">
</div>
<div id="cortes-guyon-vapnik-caption" class="caption-frame">
<p>Figure: Corinna Cortes, Isabelle Guyon and Vladmir Vapnik. Three of the key people behind the support vector machine <span class="citation" data-cites="Boser-optimal92">(Boser et al., 1992, p. @Cortes:svnet95)</span>. All were based at Bell Labs in the 1990s.</p>
</div>
</div>
<p>Also attending the summer school at the Newton Institute was Bernhard Schölkopf. He had shown that, on the same USPS digits data set, the support vector machine was able to achieve an error of 4.2% <span class="citation" data-cites="Scholkopf:comparing97">(Schölkopf et al., 1997)</span>. It was also mathematically more elegant than the neural network approaches. Even on larger data sets, by incorporating the translation invariance <span class="citation" data-cites="Scholkopf:incorporating96">(Schölkopf et al., n.d.)</span>, the support vector machine was able to achieve similar error rates to convolutional neural networks.</p>
<blockquote>
<p>This work points out the necessity of having flexible “network design” software tools that ease the design of complex, specialized network architectures</p>
<p>From conclusions of <span class="citation" data-cites="LeCun:zip89">Le Cun et al. (1989)</span></p>
</blockquote>
<div class="figure">
<div id="russakovsky-li-figure" class="figure-frame">
<table>
<tr>
<td width="50%">
<div class="centered" style="">
<img class="" src="../slides/diagrams/ml/olga-russakovsky.png" width="100%" height="auto" align="center" style="background:none; border:none; box-shadow:none; display:block; margin-left:auto; margin-right:auto;vertical-align:middle">
</div>
</td>
<td width="50%">
<div class="centered" style="">
<img class="" src="../slides/diagrams/ml/fei-fei-li.png" width="100%" height="auto" align="center" style="background:none; border:none; box-shadow:none; display:block; margin-left:auto; margin-right:auto;vertical-align:middle">
</div>
</td>
</tr>
</table>
</div>
<div id="russakovsky-li-magnify" class="magnify" onclick="magnifyFigure(&#39;russakovsky-li&#39;)">
<img class="img-button" src="{{ '/assets/images/Magnify_Large.svg' | relative_url }}" style="width:1.5ex">
</div>
<div id="russakovsky-li-caption" class="caption-frame">
<p>Figure: Olga Russakovsky, Fei Fei Li. Olga and Fei Fei led the creation of the ImageNet database that enabled convolutional neural networks to show their true potential. The data base contains millions of images.</p>
</div>
</div>
<h2 id="the-third-wave">The Third Wave</h2>
<ul>
<li>Data (many data, many classes)</li>
<li>Compute (GPUs)</li>
<li>Stochastic Gradient Descent</li>
<li>Software (autograd)</li>
</ul>
<h2 id="domains-of-use">Domains of Use</h2>
<ul>
<li>Perception and Representation
<ol type="1">
<li>Speech</li>
<li>Vision</li>
<li>Language</li>
</ol></li>
</ul>
<h2 id="experience">Experience</h2>
<ul>
<li>Bringing it together:
<ul>
<li>Unsupervised pre-training</li>
<li>Initialisation and RELU</li>
<li>A Zoo of methods and models</li>
</ul></li>
</ul>
<ul>
<li>Why do they generalize well?</li>
</ul>
<h2 id="conclusion">Conclusion</h2>
<ul>
<li>Understand the principles behind:
<ul>
<li>Generalization</li>
<li>Optimization</li>
<li>Implementation (hardware)</li>
</ul></li>
<li>Differen NN Architectures</li>
</ul>
<h2 id="thanks">Thanks!</h2>
<p>For more information on these subjects and more you might want to check the following resources.</p>
<ul>
<li>twitter: <a href="https://twitter.com/lawrennd">@lawrennd</a></li>
<li>podcast: <a href="http://thetalkingmachines.com">The Talking Machines</a></li>
<li>newspaper: <a href="http://www.theguardian.com/profile/neil-lawrence">Guardian Profile Page</a></li>
<li>blog: <a href="http://inverseprobability.com/blog.html">http://inverseprobability.com</a></li>
</ul>
<h1 class="unnumbered" id="references">References</h1>
<div id="refs" class="references hanging-indent" role="doc-bibliography">
<div id="ref-Ashby-design52">
<p>Ashby, W.R., 1952. Design for a brain: The origin of adaptive behaviour. Chapman &amp; Hall.</p>
</div>
<div id="ref-Boser-optimal92">
<p>Boser, B.E., Guyon, I.M., Vapnik, V.N., 1992. A training algorithm for optimal margin classifiers, in: Proceedings of the Fifth Annual Workshop on Computational Learning Theory, COLT ’92. Association for Computing Machinery, New York, NY, USA, pp. 144–152. <a href="https://doi.org/10.1145/130385.130401">https://doi.org/10.1145/130385.130401</a></p>
</div>
<div id="ref-Carnap-logical37">
<p>Carnap, R., 1937. Locigal syntax of language. Routledge, Trench, Trubner &amp; Co Ltd.</p>
</div>
<div id="ref-Cortes:svnet95">
<p>Cortes, C., Vapnik, V.N., 1995. Support vector networks. Machine Learning 20, 273–297. <a href="https://doi.org/10.1007/BF00994018">https://doi.org/10.1007/BF00994018</a></p>
</div>
<div id="ref-Hochreiter-lstm97">
<p>Hochreiter, S., Schmidhuber, J., 1997. Long short-term memory. Neural Computation 9, 1735–1780. <a href="https://doi.org/10.1162/neco.1997.9.8.1735">https://doi.org/10.1162/neco.1997.9.8.1735</a></p>
</div>
<div id="ref-LeCun:zip89">
<p>Le Cun, Y., Boser, B.E., Denker, J.S., Henderson, D., Howard, R.E., Hubbard, W., Jackel, L.D., 1989. Backpropagation applied to handwritten zip code recognition. Neural Computation 1, 541–551. <a href="https://doi.org/10.1162/neco.1989.1.4.541">https://doi.org/10.1162/neco.1989.1.4.541</a></p>
</div>
<div id="ref-Maxwell:governors1867">
<p>Maxwell, J.C., 1867. On governors. Proceedings of the Royal Society of London 16, 270–283.</p>
</div>
<div id="ref-McCulloch:neuron43">
<p>McCulloch, W.S., Pitts, W., 1943. A logical calculus of the ideas immanent in nervous activity. Bulletin of Mathematical Biophysics 5, 115–133. <a href="https://doi.org/10.1007/BF02478259">https://doi.org/10.1007/BF02478259</a></p>
</div>
<div id="ref-Neumann-probabilistic56">
<p>Neumann, J. von, 1956. Probabilistic logics and the synthesis of reliable organisms from unreliable components, in: Shannon, C.E., McCarthy, J. (Eds.), Automata Studies. Princeton University Press, Princeton.</p>
</div>
<div id="ref-Rosenblatt-perceptron58">
<p>Rosenblatt, F., 1958. The perceptron: A probabilistic model for information storage and organization in the brain. Psychological Review 65, 386–408. <a href="https://doi.org/10.1037/h0042519">https://doi.org/10.1037/h0042519</a></p>
</div>
<div id="ref-Rumelhart:book86">
<p>Rumelhart, D.E., McClelland, J.L., the PDP Research Group, 1986. Parallel distributed programming: Explorations in the microstructure of cognition. mit, Cambridge, MA.</p>
</div>
<div id="ref-Russell-principia25">
<p>Russell, B., Whitehead, A.N., 1925. Principia mathematica, 2nd ed. Cambridge University Press.</p>
</div>
<div id="ref-Scholkopf:incorporating96">
<p>Schölkopf, B., Burges, C.J.C., Vapnik, V.N., n.d. Incorporating invariances in support vector learning machines, in:. pp. 47–52. <a href="https://doi.org/10.1007/3-540-61510-5_12">https://doi.org/10.1007/3-540-61510-5_12</a></p>
</div>
<div id="ref-Scholkopf:comparing97">
<p>Schölkopf, B., Sung, K.-K., Burges, C.J.C., Girosi, F., Niyogi, P., Poggio, T., Vapnik, V.N., 1997. Comparing support vector machines with Gaussian kernels to radial basis function classifiers. IEEE Transactions on Signal Processing 45, 2758–2765. <a href="https://doi.org/10.1109/78.650102">https://doi.org/10.1109/78.650102</a></p>
</div>
<div id="ref-Wiener:yellow49">
<p>Wiener, N., 1949. The extrapolation, interpolation and smoothing of stationary time series with engineering applications. wiley.</p>
</div>
<div id="ref-Wiener:cybernetics48">
<p>Wiener, N., 1948. Cybernetics: Control and communication in the animal and the machine. MIT Press, Cambridge, MA.</p>
</div>
<div id="ref-Wittgenstein-logico21">
<p>Wittgenstein, L., 1922. Tractatus logico-philosophicus. Kegan Paul.</p>
</div>
</div>

