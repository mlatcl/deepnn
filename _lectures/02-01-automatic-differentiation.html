---
title: "More Generalisation and Automatic Differentiation"
venue: "LT1, William Gates Building"
abstract: "<p>This lecture will cover the foundations of automatic
differentiation as well as the different frameworks that exist for
building models.</p>"
author:
- given: Ferenc
  family: Huszár
  url: https://www.inference.vc/about/
  institute: 
  twitter: 
  gscholar: 
  orcid: 
edit_url: https://github.com/mlatcl/deepnn/edit/gh-pages/_lamd/automatic-differentiation.md
date: 2022-01-27
published: 2022-01-27
time: "14:00"
week: 2
session: 1
pptx: 02-01-automatic-differentiation.pptx
hackmdslides: fhuszar/H1WZ70kl_#
hackmdnotes: fhuszar/SyHTInWeu
hackmdworksheet: fhuszar/S1UdOvZe_
docx: 02-01-automatic-differentiation.docx
youtube: "-9O5obQZUn0"
layout: lecture
categories:
- notes
---



<!-- Do not edit this file locally. -->
<!---->
<!-- Do not edit this file locally. -->
<!-- Do not edit this file locally. -->
<!-- The last names to be defined. Should be defined entirely in terms of macros from above-->
<!--

-->
<ul>
<li><p><a href="https://hackmd.io/@fhuszar/H1WZ70kl_#">Slides are
here</a></p></li>
<li><p><a href="https://hackmd.io/@fhuszar/S1UdOvZe_">Approximating with
ReLU notes are here</a></p></li>
<li><p><a href="https://hackmd.io/@fhuszar/SyHTInWeu">Automatic
Differentiaton notes are here</a></p></li>
<li><p><a
href="https://colab.research.google.com/drive/1qioPLq-dxOwudPKXU3MxpHr2s4Su3dxI?usp=sharing">Google
Colab Is Available Here</a></p></li>
<li><p>Nice review paper from <a
href="https://jmlr.org/papers/v18/17-468.html"
class="uri">https://jmlr.org/papers/v18/17-468.html</a> <span
class="citation" data-cites="Baydin-autodiff18">Baydin et al.
(2018)</span> (See Figure 2)</p></li>
</ul>
<h2 id="reddit-group">Reddit Group</h2>
<p><a
href="https://www.reddit.com/r/CST_DeepNN/">https://www.reddit.com/r/CST_DeepNN/</a></p>
<p>{ ## Approximation</p>
<h3 id="basic-multilayer-perceptron">Basic Multilayer Perceptron</h3>
<p><span class="math display">\[\begin{align}
f_l(x) &amp;= \phi(W_l f_{l-1}(x) + b_l)\\
f_0(x) &amp;= x
\end{align}\]</span></p>
<h3 id="basic-multilayer-perceptron-1">Basic Multilayer Perceptron</h3>
<p><span class="math display">\[
\small
f_L(x) = \phi\left(b_L + W_L \phi\left(b_{L-1} + W_{L-1} \phi\left(
\cdots \phi\left(b_1 + W_1 x\right) \cdots \right)\right)\right)
\]</span></p>
<h3 id="rectified-linear-unit">Rectified Linear Unit</h3>
<p><span class="math display">\[
\phi(x) = \left\{\matrix{0&amp;\text{when }x\leq 0\\x&amp;\text{when
}x&gt;0}\right.
\]</span></p>
<p><img src="https://i.imgur.com/SxKdrzb.png" /></p>
<h3 id="what-can-these-networks-represent">What can these networks
represent?</h3>
<p><span class="math display">\[
\operatorname{ReLU}(\mathbf{w}_1x - \mathbf{b}_1)
\]</span></p>
<p><img src="https://i.imgur.com/rN5wRVJ.png" /></p>
<h3 id="what-can-these-networks-represent-1">What can these networks
represent?</h3>
<p><span class="math display">\[
f(x) = \mathbf{w}^T_2 \operatorname{ReLU}(\mathbf{w}_1x - \mathbf{b}_1)
\]</span></p>
<p><img src="https://i.imgur.com/kX3nuYg.png" /></p>
<h3 id="single-hidden-layer">Single hidden layer</h3>
<p>number of kinks <span class="math inline">\(\approx O(\)</span> width
of network <span class="math inline">\()\)</span></p>
<h3 id="example-sawtooth-network">Example: “sawtooth” network</h3>
<p><span class="math display">\[\begin{align}
f_l(x) &amp;= 2\vert f_{l-1}(x)\vert - 2 \\
f_0(x) &amp;= x
\end{align}\]</span></p>
<h3 id="sawtooth-network">Sawtooth network</h3>
<p><span class="math display">\[\begin{align}
f_l(x) &amp;= 2 \operatorname{ReLU}(f_{l-1}(x)) + 2
\operatorname{ReLU}(-f_{l-1}(x)) - 2\\
f_0(x) &amp;= x
\end{align}\]</span></p>
<h3 id="layer-network"><span class="math inline">\(0\)</span>-layer
network</h3>
<p><img src="https://i.imgur.com/pucqIVN.png" /></p>
<h3 id="layer-network-1"><span class="math inline">\(1\)</span>-layer
network</h3>
<p><img src="https://i.imgur.com/YOTtTY7.png" /></p>
<h3 id="layer-network-2"><span class="math inline">\(2\)</span>-layer
network</h3>
<p><img src="https://i.imgur.com/reii7O5.png" /></p>
<h3 id="layer-network-3"><span class="math inline">\(3\)</span>-layer
network</h3>
<p><img src="https://i.imgur.com/J6KiUHI.png" /></p>
<h3 id="layer-network-4"><span class="math inline">\(4\)</span>-layer
network</h3>
<p><img src="https://i.imgur.com/fHTZhU0.png" /></p>
<h3 id="layer-network-5"><span class="math inline">\(5\)</span>-layer
network</h3>
<p><img src="https://i.imgur.com/ni4QV2b.png" /></p>
<h3 id="deep-relu-networks">Deep ReLU networks</h3>
<p>number of kinks <span class="math inline">\(\approx O(2^\text{depth
of network})\)</span></p>
<h3 id="in-higher-dimensions">In higher dimensions</h3>
<p><img src="https://i.imgur.com/0NVHFEN.png" /></p>
<h3 id="in-higher-dimensions-1">In higher dimensions</h3>
<p><img src="https://i.imgur.com/DJtv5Yj.jpg" /></p>
<h3 id="approximation-summary">Approximation: summary</h3>
<ul>
<li>depth increases model complexity more than width</li>
<li>model clas defined by deep networks is VERY LARGE</li>
<li>both an advantage, but and cause for concern</li>
<li>“complex models don’t generalize”</li>
</ul>
<h2 id="generalization">Generalization</h2>
<h2 id="generalization-1">Generalization</h2>
<p><img src="https://i.imgur.com/Tu5SHpr.png" /></p>
<h2 id="generalization-2">Generalization</h2>
<p><img src="https://i.imgur.com/8bkhxAv.png" /></p>
<h2 id="generalization-3">Generalization</h2>
<p><img src="https://i.imgur.com/YHedAr6.png" /></p>
<h2 id="generalization-deep-nets">Generalization: deep nets</h2>
<p><img src="https://i.imgur.com/bfyRBsx.png" /></p>
<h2 id="generalization-deep-nets-1">Generalization: deep nets</h2>
<p><img src="https://i.imgur.com/fzLYvHe.png" /></p>
<h2 id="generalization-summary">Generalization: summary</h2>
<ul>
<li><strong>classical view:</strong> generalization is property of model
class and loss function</li>
<li><strong>new view:</strong> it is also a property of the optimization
algorithm</li>
</ul>
<h2 id="generalization-4">Generalization</h2>
<ul>
<li>optimization is core to deep learning</li>
<li>new tools and insights:
<ul>
<li>infinite width neural networks</li>
<li>neural tangent kernel <a
href="https://arxiv.org/abs/1806.07572">(Jacot et al, 2018)</a></li>
<li>deep linear models (<a
href="https://arxiv.org/abs/1905.13655">e.g. Arora et al, 2019</a>)</li>
<li>importance of initialization</li>
<li>effect of gradient noise</li>
</ul></li>
</ul>
<h2 id="gradient-based-optimization">Gradient-based optimization</h2>
<p><span class="math display">\[
\mathcal{L}(\theta) = \sum_{n=1}^N \ell(y_n, f(x_n, \theta))
\]</span></p>
<p><span class="math display">\[
\theta_{t+1} = \theta_t - \eta \nabla_\theta \mathcal{L}(\theta)
\]</span></p>
<h3 id="basic-multilayer-perceptron-2">Basic Multilayer Perceptron</h3>
<p><span class="math display">\[
\small
f_L(x) = \phi\left(b_L + W_L \phi\left(b_{L-1} + W_{L-1} \phi\left(
\cdots \phi\left(b_1 + W_1 x\right) \cdots \right)\right)\right)
\]</span></p>
<h2 id="general-deep-function-composition">General deep function
composition</h2>
<p><span class="math display">\[
f_L(f_{L-1}(\cdots f_1(\mathbb{w})))
\]</span></p>
<p>How do I calculate the derivative of <span
class="math inline">\(f_L(\mathbb{w})\)</span> with respect to <span
class="math inline">\(\mathbb{w}\)</span>?</p>
<h2 id="chain-rule">Chain rule</h2>
<p><span class="math display">\[
\frac{\partial \mathbf{f}_L}{\partial \mathbb{w}} = \frac{\partial
\mathbf{f}_L}{\partial \mathbf{f}_{L-1}} \frac{\partial
\mathbf{f}_{L-1}}{\partial \mathbf{f}_{L-2}} \frac{\partial
\mathbf{f}_{L-2}}{\partial \mathbf{f}_{L-3}} \cdots \frac{\partial
\mathbf{f}_3}{\partial \mathbf{f}_{2}} \frac{\partial
\mathbf{f}_2}{\partial \mathbf{f}_{1}} \frac{\partial
\mathbf{f}_1}{\partial \mathbf{w}}
\]</span></p>
<h2 id="how-to-evaluate-this">How to evaluate this?</h2>
<p><span class="math display">\[
\frac{\partial \mathbf{f}_L}{\partial \mathbb{w}} = \frac{\partial
\mathbf{f}_L}{\partial \mathbf{f}_{L-1}} \frac{\partial
\mathbf{f}_{L-1}}{\partial \mathbf{f}_{L-2}} \frac{\partial
\mathbf{f}_{L-2}}{\partial \mathbf{f}_{L-3}} \cdots \frac{\partial
\mathbf{f}_3}{\partial \mathbf{f}_{2}} \frac{\partial
\mathbf{f}_2}{\partial \mathbf{f}_{1}} \frac{\partial
\mathbf{f}_1}{\partial \mathbf{w}}
\]</span></p>
<p><span class="math display">\[
\small
\frac{\partial \mathbf{f}_L}{\partial \mathbb{w}} = \frac{\partial
\mathbf{f}_L}{\partial \mathbf{f}_{L-1}} \left( \frac{\partial
\mathbf{f}_{L-1}}{\partial \mathbf{f}_{L-2}} \left( \frac{\partial
\mathbf{f}_{L-2}}{\partial \mathbf{f}_{L-3}} \cdots \left(
\frac{\partial \mathbf{f}_3}{\partial \mathbf{f}_{2}} \left(
\frac{\partial \mathbf{f}_2}{\partial \mathbf{f}_{1}} \frac{\partial
\mathbf{f}_1}{\partial \mathbf{w}} \right) \right) \cdots \right)
\right)
\]</span></p>
<h2 id="or-like-this">Or like this?</h2>
<p><span class="math display">\[
\frac{\partial \mathbf{f}_L}{\partial \mathbb{w}} = \frac{\partial
\mathbf{f}_L}{\partial \mathbf{f}_{L-1}} \frac{\partial
\mathbf{f}_{L-1}}{\partial \mathbf{f}_{L-2}} \frac{\partial
\mathbf{f}_{L-2}}{\partial \mathbf{f}_{L-3}} \cdots \frac{\partial
\mathbf{f}_3}{\partial \mathbf{f}_{2}} \frac{\partial
\mathbf{f}_2}{\partial \mathbf{f}_{1}} \frac{\partial
\mathbf{f}_1}{\partial \mathbf{w}}
\]</span></p>
<p><span class="math display">\[
\small
\frac{\partial \mathbf{f}_L}{\partial \mathbb{w}} = \left( \left( \cdots
\left( \left( \frac{\partial \mathbf{f}_L}{\partial \mathbf{f}_{L-1}}
\frac{\partial \mathbf{f}_{L-1}}{\partial \mathbf{f}_{L-2}}  \right)
\frac{\partial \mathbf{f}_{L-2}}{\partial \mathbf{f}_{L-3}} \right)
\cdots \frac{\partial \mathbf{f}_3}{\partial \mathbf{f}_{2}} \right)
\frac{\partial \mathbf{f}_2}{\partial \mathbf{f}_{1}} \right)
\frac{\partial \mathbf{f}_1}{\partial \mathbf{w}}
\]</span></p>
<h2 id="or-in-a-funky-way">Or in a funky way?</h2>
<p><span class="math display">\[
\frac{\partial \mathbf{f}_L}{\partial \mathbb{w}} = \frac{\partial
\mathbf{f}_L}{\partial \mathbf{f}_{L-1}} \frac{\partial
\mathbf{f}_{L-1}}{\partial \mathbf{f}_{L-2}} \frac{\partial
\mathbf{f}_{L-2}}{\partial \mathbf{f}_{L-3}} \cdots \frac{\partial
\mathbf{f}_3}{\partial \mathbf{f}_{2}} \frac{\partial
\mathbf{f}_2}{\partial \mathbf{f}_{1}} \frac{\partial
\mathbf{f}_1}{\partial \mathbf{w}}
\]</span></p>
<p><span class="math display">\[
\small
\frac{\partial \mathbf{f}_L}{\partial \mathbb{w}} = \frac{\partial
\mathbf{f}_L}{\partial \mathbf{f}_{L-1}} \left( \left( \left(
\frac{\partial \mathbf{f}_{L-1}}{\partial \mathbf{f}_{L-2}}  \right)
\frac{\partial \mathbf{f}_{L-2}}{\partial \mathbf{f}_{L-3}} \right)
\left( \left( \cdots \frac{\partial \mathbf{f}_3}{\partial
\mathbf{f}_{2}} \right) \frac{\partial \mathbf{f}_2}{\partial
\mathbf{f}_{1}} \right) \right)\frac{\partial \mathbf{f}_1}{\partial
\mathbf{w}}
\]</span></p>
<h2 id="automatic-differentiation">Automatic differentiation</h2>
<p><strong>Forward-mode</strong></p>
<p><span class="math display">\[
\small
\frac{\partial \mathbf{f}_L}{\partial \mathbb{w}} = \frac{\partial
\mathbf{f}_L}{\partial \mathbf{f}_{L-1}} \left( \frac{\partial
\mathbf{f}_{L-1}}{\partial \mathbf{f}_{L-2}} \left( \frac{\partial
\mathbf{f}_{L-2}}{\partial \mathbf{f}_{L-3}} \cdots \left(
\frac{\partial \mathbf{f}_3}{\partial \mathbf{f}_{2}} \left(
\frac{\partial \mathbf{f}_2}{\partial \mathbf{f}_{1}} \frac{\partial
\mathbf{f}_1}{\partial \mathbf{w}} \right) \right) \cdots \right)
\right)
\]</span></p>
<p>Cost: <span class="math display">\[
\small
d_0d_1d_2 + d_0d_2d_3 + \ldots + d_0d_{L-1}d_L = d_0
\sum_{l=2}^{L}d_ld_{l-1}
\]</span></p>
<h2 id="automatic-differentiation-1">Automatic differentiation</h2>
<p><strong>Reverse-mode</strong></p>
<p><span class="math display">\[
\small
\frac{\partial \mathbf{f}_L}{\partial \mathbb{w}} = \left( \left( \cdots
\left( \left( \frac{\partial \mathbf{f}_L}{\partial \mathbf{f}_{L-1}}
\frac{\partial \mathbf{f}_{L-1}}{\partial \mathbf{f}_{L-2}}  \right)
\frac{\partial \mathbf{f}_{L-2}}{\partial \mathbf{f}_{L-3}} \right)
\cdots \frac{\partial \mathbf{f}_3}{\partial \mathbf{f}_{2}} \right)
\frac{\partial \mathbf{f}_2}{\partial \mathbf{f}_{1}} \right)
\frac{\partial \mathbf{f}_1}{\partial \mathbf{w}}
\]</span></p>
<p>Cost: <span class="math display">\[
\small
d_Ld_{L-1}d_{L-2} + d_{L}d_{L-2}d_{L-3} + \ldots + d_Ld_{1}d_0 = d_L
\sum_{l=0}^{L-2}d_ld_{l+1}
\]</span></p>
<h2 id="automatic-differentiation-2">Automatic differentiation</h2>
<ul>
<li>in deep learning we’re most interested in scalar objectives</li>
<li><span class="math inline">\(d_L=1\)</span>, consequently, backward
mode is always optimal</li>
<li>in the context of neural networks:
<strong>backpropagation</strong></li>
<li>backprop has higher memory cost than forwardprop</li>
</ul>
<h2 id="example-calculating-a-hessian">Example: calculating a
Hessian</h2>
<p><span class="math display">\[
H(\mathbb{w}) =
\frac{\partial^2}{\partial\mathbf{w}\partial\mathbf{w}^T} L(\mathbf{w})
= \frac{\partial}{\partial\mathbf{w}} \mathbf{g}(\mathbf{w})
\]</span></p>
<h2 id="example-hessian-vector-product">Example: Hessian-vector
product</h2>
<p><span class="math display">\[
\mathbf{v}^TH(\mathbf{w}) = \frac{\partial}{\partial\mathbf{w}} \left(
\mathbf{v}^T \mathbf{g}(\mathbf{w}) \right)
\]</span></p>
<p>}</p>
<div id="refs" class="references csl-bib-body hanging-indent"
role="list">
<div id="ref-Baydin-autodiff18" class="csl-entry" role="listitem">
Baydin, A.G., Pearlmutter, B.A., Radul, A.A., Siskind, J.M., 2018. <a
href="http://jmlr.org/papers/v18/17-468.html">Automatic differentiation
in machine learning: A survey</a>. Journal of Machine Learning Research
18, 1–43.
</div>
</div>

