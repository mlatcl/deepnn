---
title: "Generalization and Neural Networks"
venue: "LT1, William Gates Building"
abstract: "<p>This lecture will cover generalization in machine learning
with a particular focus on neural architectures. We will review
classical generalization and explore what’s different about neural
network models.</p>"
author:
- given: Neil D.
  family: Lawrence
  url: http://inverseprobability.com
  institute: 
  twitter: 
  gscholar: 
  orcid: 
edit_url: https://github.com/mlatcl/deepnn/edit/gh-pages/_lamd/generalisation-and-neural-networks.md
date: 2022-01-25
published: 2022-01-25
time: "14:00"
week: 1
session: 2
reveal: 01-02-generalisation-and-neural-networks.slides.html
ipynb: 01-02-generalisation-and-neural-networks.ipynb
pptx: 01-02-generalisation-and-neural-networks.pptx
hackmdslides: fhuszar/r1HxvooMd#/
docx: 01-02-generalisation-and-neural-networks.docx
youtube: "WaauyjcSNhc"
layout: lecture
categories:
- notes
---



<!-- Do not edit this file locally. -->
<!---->
<!-- Do not edit this file locally. -->
<!-- Do not edit this file locally. -->
<!-- The last names to be defined. Should be defined entirely in terms of macros from above-->
<!--

-->
<h2 id="setup">Setup</h2>
<div style="text-align:right">
<span class="editsection-bracket" style="">[</span><span
class="editsection"
style=""><a href="https://github.com/lawrennd/snippets/edit/main/_deepnn/includes/deepnn-notebook-setup.md" target="_blank" onclick="ga('send', 'event', 'Edit Page', 'Edit', 'https://github.com/lawrennd/snippets/edit/main/_deepnn/includes/deepnn-notebook-setup.md', 13);">edit</a></span><span class="editsection-bracket" style="">]</span>
</div>
<p>First we download some libraries and files to support the
notebook.</p>
<!--setupplotcode{import seaborn as sns
sns.set_style('darkgrid')
sns.set_context('paper')
sns.set_palette('colorblind')}-->
<h2 id="pods">pods</h2>
<div style="text-align:right">
<span class="editsection-bracket" style="">[</span><span
class="editsection"
style=""><a href="https://github.com/lawrennd/snippets/edit/main/_software/includes/pods-software.md" target="_blank" onclick="ga('send', 'event', 'Edit Page', 'Edit', 'https://github.com/lawrennd/snippets/edit/main/_software/includes/pods-software.md', 13);">edit</a></span><span class="editsection-bracket" style="">]</span>
</div>
<p>In Sheffield we created a suite of software tools for ‘Open Data
Science’. Open data science is an approach to sharing code, models and
data that should make it easier for companies, health professionals and
scientists to gain access to data science techniques.</p>
<p>You can also check this blog post on <a
href="http://inverseprobability.com/2014/07/01/open-data-science">Open
Data Science</a>.</p>
<p>The software can be installed using</p>
<div class="sourceCode" id="cb1"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb1-1"><a href="#cb1-1" aria-hidden="true" tabindex="-1"></a><span class="op">%</span>pip install pods</span></code></pre></div>
<p>from the command prompt where you can access your python
installation.</p>
<p>The code is also available on GitHub: <a
href="https://github.com/lawrennd/ods"
class="uri">https://github.com/lawrennd/ods</a></p>
<p>Once <code>pods</code> is installed, it can be imported in the usual
manner.</p>
<div class="sourceCode" id="cb2"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb2-1"><a href="#cb2-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> pods</span></code></pre></div>
<h2 id="notutils">notutils</h2>
<div style="text-align:right">
<span class="editsection-bracket" style="">[</span><span
class="editsection"
style=""><a href="https://github.com/lawrennd/snippets/edit/main/_software/includes/notutils-software.md" target="_blank" onclick="ga('send', 'event', 'Edit Page', 'Edit', 'https://github.com/lawrennd/snippets/edit/main/_software/includes/notutils-software.md', 13);">edit</a></span><span class="editsection-bracket" style="">]</span>
</div>
<p>This small package is a helper package for various notebook utilities
used below.</p>
<p>The software can be installed using</p>
<div class="sourceCode" id="cb3"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb3-1"><a href="#cb3-1" aria-hidden="true" tabindex="-1"></a><span class="op">%</span>pip install notutils</span></code></pre></div>
<p>from the command prompt where you can access your python
installation.</p>
<p>The code is also available on GitHub: <a
href="https://github.com/lawrennd/notutils"
class="uri">https://github.com/lawrennd/notutils</a></p>
<p>Once <code>notutils</code> is installed, it can be imported in the
usual manner.</p>
<div class="sourceCode" id="cb4"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb4-1"><a href="#cb4-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> notutils</span></code></pre></div>
<h2 id="mlai">mlai</h2>
<div style="text-align:right">
<span class="editsection-bracket" style="">[</span><span
class="editsection"
style=""><a href="https://github.com/lawrennd/snippets/edit/main/_software/includes/mlai-software.md" target="_blank" onclick="ga('send', 'event', 'Edit Page', 'Edit', 'https://github.com/lawrennd/snippets/edit/main/_software/includes/mlai-software.md', 13);">edit</a></span><span class="editsection-bracket" style="">]</span>
</div>
<p>The <code>mlai</code> software is a suite of helper functions for
teaching and demonstrating machine learning algorithms. It was first
used in the Machine Learning and Adaptive Intelligence course in
Sheffield in 2013.</p>
<p>The software can be installed using</p>
<div class="sourceCode" id="cb5"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb5-1"><a href="#cb5-1" aria-hidden="true" tabindex="-1"></a><span class="op">%</span>pip install mlai</span></code></pre></div>
<p>from the command prompt where you can access your python
installation.</p>
<p>The code is also available on GitHub: <a
href="https://github.com/lawrennd/mlai"
class="uri">https://github.com/lawrennd/mlai</a></p>
<p>Once <code>mlai</code> is installed, it can be imported in the usual
manner.</p>
<div class="sourceCode" id="cb6"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb6-1"><a href="#cb6-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> mlai</span></code></pre></div>
<h2 id="quadratic-loss-and-linear-system">Quadratic Loss and Linear
System</h2>
<p>We will consider a simplified system, to remind us of some of the
linear algebra involved, and introduce some of the fundamental
issues.</p>
<h2 id="expected-loss">Expected Loss</h2>
<div style="text-align:right">
<span class="editsection-bracket" style="">[</span><span
class="editsection"
style=""><a href="https://github.com/lawrennd/snippets/edit/main/_ml/includes/expected-loss.md" target="_blank" onclick="ga('send', 'event', 'Edit Page', 'Edit', 'https://github.com/lawrennd/snippets/edit/main/_ml/includes/expected-loss.md', 13);">edit</a></span><span class="editsection-bracket" style="">]</span>
</div>
<p>Our objective function so far has been the negative log likelihood,
which we have minimized (via the sum of squares error) to obtain our
model. However, there is an alternative perspective on an objective
function, that of a <em>loss function</em>. A loss function is a cost
function associated with the penalty you might need to pay for a
particular incorrect decision. One approach to machine learning involves
specifying a loss function and considering how much a particular model
is likely to cost us across its lifetime. We can represent this with an
expectation. If our loss function is given as <span
class="math inline">\(L(y, x, \mathbf{ w})\)</span> for a particular
model that predicts <span class="math inline">\(y\)</span> given <span
class="math inline">\(x\)</span> and <span
class="math inline">\(\mathbf{ w}\)</span> then we are interested in
minimizing the expected loss under the likely distribution of <span
class="math inline">\(y\)</span> and <span
class="math inline">\(x\)</span>. To understand this formally we define
the <em>true</em> distribution of the data samples, <span
class="math inline">\(y\)</span>, <span
class="math inline">\(x\)</span>. This is a particular distribution that
we don’t typically have access to. To represent it we define a variant
of the letter ‘P’, <span class="math inline">\(\mathbb{P}(y,
x)\)</span>. If we genuinely pay <span class="math inline">\(L(y, x,
\mathbf{ w})\)</span> for every mistake we make, and the future test
data is genuinely drawn from <span class="math inline">\(\mathbb{P}(y,
x)\)</span> then we can define our expected loss, or risk, to be, <span
class="math display">\[
R(\mathbf{ w}) = \int L(y, x, \mathbf{ w}) \mathbb{P}(y, x) \text{d}y
\text{d}x.
\]</span> Of course, in practice, this value can’t be computed
<em>but</em> it serves as a reminder of what it is we are aiming to
minimize and under certain circumstances it can be approximated.</p>
<h2 id="sample-based-approximations">Sample-Based Approximations</h2>
<p>A sample-based approximation to an expectation involves replacing the
true expectation with a sum over samples from the distribution. <span
class="math display">\[
\int f(z) p(z) \text{d}z\approx \frac{1}{s}\sum_{i=1}^s f(z_i).
\]</span> if <span class="math inline">\(\{z_i\}_{i=1}^s\)</span> are a
set of <span class="math inline">\(s\)</span> independent and
identically distributed samples from the distribution <span
class="math inline">\(p(z)\)</span>. This approximation becomes better
for larger <span class="math inline">\(s\)</span>, although the <em>rate
of convergence</em> to the true integral will be very dependent on the
distribution <span class="math inline">\(p(z)\)</span> <em>and</em> the
function <span class="math inline">\(f(z)\)</span>.</p>
<p>That said, this means we can approximate our true integral with the
sum, <span class="math display">\[
R(\mathbf{ w}) \approx \frac{1}{n}\sum_{i=1}^{n} L(y_i, x_i, \mathbf{
w}).
\]</span></p>
<h2 id="laplaces-idea">Laplace’s Idea</h2>
<div style="text-align:right">
<span class="editsection-bracket" style="">[</span><span
class="editsection"
style=""><a href="https://github.com/lawrennd/snippets/edit/main/_ml/includes/linear-regression-log-likelihood.md" target="_blank" onclick="ga('send', 'event', 'Edit Page', 'Edit', 'https://github.com/lawrennd/snippets/edit/main/_ml/includes/linear-regression-log-likelihood.md', 13);">edit</a></span><span class="editsection-bracket" style="">]</span>
</div>
<p>Laplace had the idea to augment the observations by noise, that is
equivalent to considering a probability density whose mean is given by
the <em>prediction function</em> <span
class="math display">\[p\left(y_i|x_i\right)=\frac{1}{\sqrt{2\pi\sigma^2}}\exp\left(-\frac{\left(y_i-f\left(x_i\right)\right)^{2}}{2\sigma^2}\right).\]</span></p>
<p>This is known as <em>stochastic process</em>. It is a function that
is corrupted by noise. Laplace didn’t suggest the Gaussian density for
that purpose, that was an innovation from Carl Friederich Gauss, which
is what gives the Gaussian density its name.</p>
<h2 id="height-as-a-function-of-weight">Height as a Function of
Weight</h2>
<p>In the standard Gaussian, parameterized by mean and variance, make
the mean a linear function of an <em>input</em>.</p>
<p>This leads to a regression model. <span class="math display">\[
\begin{align*}
  y_i=&amp;f\left(x_i\right)+\epsilon_i,\\
         \epsilon_i \sim &amp; \mathcal{N}\left(0,\sigma^2\right).
  \end{align*}
\]</span></p>
<p>Assume <span class="math inline">\(y_i\)</span> is height and <span
class="math inline">\(x_i\)</span> is weight.</p>
<h2 id="olympic-marathon-data">Olympic Marathon Data</h2>
<div style="text-align:right">
<span class="editsection-bracket" style="">[</span><span
class="editsection"
style=""><a href="https://github.com/lawrennd/snippets/edit/main/_datasets/includes/olympic-marathon-data.md" target="_blank" onclick="ga('send', 'event', 'Edit Page', 'Edit', 'https://github.com/lawrennd/snippets/edit/main/_datasets/includes/olympic-marathon-data.md', 13);">edit</a></span><span class="editsection-bracket" style="">]</span>
</div>
<table>
<tr>
<td width="70%">
<ul>
<li>Gold medal times for Olympic Marathon since 1896.</li>
<li>Marathons before 1924 didn’t have a standardized distance.</li>
<li>Present results using pace per km.</li>
<li>In 1904 Marathon was badly organized leading to very slow
times.</li>
</ul>
</td>
<td width="30%">
<div class="centered" style="">
<img class="" src="https://mlatcl.github.io/deepnn/./slides/diagrams//Stephen_Kiprotich.jpg" width="100%" height="auto" align="center" style="background:none; border:none; box-shadow:none; display:block; margin-left:auto; margin-right:auto;vertical-align:middle">
</div>
<small>Image from Wikimedia Commons <a href="http://bit.ly/16kMKHQ"
class="uri">http://bit.ly/16kMKHQ</a></small>
</td>
</tr>
</table>
<p>The first thing we will do is load a standard data set for regression
modelling. The data consists of the pace of Olympic Gold Medal Marathon
winners for the Olympics from 1896 to present. Let’s load in the data
and plot.</p>
<div class="sourceCode" id="cb7"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb7-1"><a href="#cb7-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb7-2"><a href="#cb7-2" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> pods</span></code></pre></div>
<div class="sourceCode" id="cb8"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb8-1"><a href="#cb8-1" aria-hidden="true" tabindex="-1"></a>data <span class="op">=</span> pods.datasets.olympic_marathon_men()</span>
<span id="cb8-2"><a href="#cb8-2" aria-hidden="true" tabindex="-1"></a>x <span class="op">=</span> data[<span class="st">&#39;X&#39;</span>]</span>
<span id="cb8-3"><a href="#cb8-3" aria-hidden="true" tabindex="-1"></a>y <span class="op">=</span> data[<span class="st">&#39;Y&#39;</span>]</span>
<span id="cb8-4"><a href="#cb8-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-5"><a href="#cb8-5" aria-hidden="true" tabindex="-1"></a>offset <span class="op">=</span> y.mean()</span>
<span id="cb8-6"><a href="#cb8-6" aria-hidden="true" tabindex="-1"></a>scale <span class="op">=</span> np.sqrt(y.var())</span>
<span id="cb8-7"><a href="#cb8-7" aria-hidden="true" tabindex="-1"></a>yhat <span class="op">=</span> (y <span class="op">-</span> offset)<span class="op">/</span>scale</span></code></pre></div>
<div class="figure">
<div id="olympic-marathon-figure" class="figure-frame">
<object class="svgplot " data="https://mlatcl.github.io/deepnn/./slides/diagrams//datasets/olympic-marathon.svg" width="80%" style=" ">
</object>
</div>
<div id="olympic-marathon-magnify" class="magnify"
onclick="magnifyFigure(&#39;olympic-marathon&#39;)">
<img class="img-button" src="{{ '/assets/images/Magnify_Large.svg' | relative_url }}" style="width:1.5ex">
</div>
<div id="olympic-marathon-caption" class="caption-frame">
<p>Figure: Olympic marathon pace times since 1896.</p>
</div>
</div>
<p>Things to notice about the data include the outlier in 1904, in that
year the Olympics was in St Louis, USA. Organizational problems and
challenges with dust kicked up by the cars following the race meant that
participants got lost, and only very few participants completed. More
recent years see more consistently quick marathons.</p>
<h2 id="running-example-olympic-marathons">Running Example: Olympic
Marathons</h2>
<div style="text-align:right">
<span class="editsection-bracket" style="">[</span><span
class="editsection"
style=""><a href="https://github.com/lawrennd/snippets/edit/main/_ml/includes/olympic-marathon-linear-regression.md" target="_blank" onclick="ga('send', 'event', 'Edit Page', 'Edit', 'https://github.com/lawrennd/snippets/edit/main/_ml/includes/olympic-marathon-linear-regression.md', 13);">edit</a></span><span class="editsection-bracket" style="">]</span>
</div>
<p>Note that <code>x</code> and <code>y</code> are not
<code>pandas</code> data frames for this example, they are just arrays
of dimensionality <span class="math inline">\(n\times 1\)</span>, where
<span class="math inline">\(n\)</span> is the number of data.</p>
<p>The aim of this lab is to have you coding linear regression in
python. We will do it in two ways, once using iterative updates
(coordinate ascent) and then using linear algebra. The linear algebra
approach will not only work much better, it is also easy to extend to
multiple input linear regression and <em>non-linear</em> regression
using basis functions.</p>
<h2 id="maximum-likelihood-iterative-solution">Maximum Likelihood:
Iterative Solution</h2>
<p>Now we will take the maximum likelihood approach we derived in the
lecture to fit a line, <span class="math inline">\(y_i=mx_i +
c\)</span>, to the data you’ve plotted. We are trying to minimize the
error function: <span class="math display">\[
L(m, c) =  \sum_{i=1}^n(y_i-mx_i-c)^2
\]</span> with respect to <span class="math inline">\(m\)</span>, <span
class="math inline">\(c\)</span> and <span
class="math inline">\(\sigma^2\)</span>. We can start with an initial
guess for <span class="math inline">\(m\)</span>,</p>
<div class="sourceCode" id="cb9"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb9-1"><a href="#cb9-1" aria-hidden="true" tabindex="-1"></a>m <span class="op">=</span> <span class="op">-</span><span class="fl">0.4</span></span>
<span id="cb9-2"><a href="#cb9-2" aria-hidden="true" tabindex="-1"></a>c <span class="op">=</span> <span class="dv">80</span></span></code></pre></div>
<p>Then we use the maximum likelihood update to find an estimate for the
offset, <span class="math inline">\(c\)</span>.</p>
<h2 id="quadratic-loss">Quadratic Loss</h2>
<div style="text-align:right">
<span class="editsection-bracket" style="">[</span><span
class="editsection"
style=""><a href="https://github.com/lawrennd/snippets/edit/main/_ml/includes/linear-regression-direct-solution.md" target="_blank" onclick="ga('send', 'event', 'Edit Page', 'Edit', 'https://github.com/lawrennd/snippets/edit/main/_ml/includes/linear-regression-direct-solution.md', 13);">edit</a></span><span class="editsection-bracket" style="">]</span>
</div>
<p>Now we’ve identified the empirical risk with the loss, we’ll use
<span class="math inline">\(L(\mathbf{ w})\)</span> to represent our
objective function. <span class="math display">\[
L(\mathbf{ w}) = \sum_{i=1}^n\left(y_i - f(\mathbf{ x}_i, \mathbf{
w})\right)^2
\]</span> gives us our objective.</p>
<p>In the case of the linear prediction function, we can substitute
<span class="math inline">\(f(\mathbf{ x}_i, \mathbf{ w}) = \mathbf{
w}^\top \mathbf{ x}_i\)</span>. <span class="math display">\[
L(\mathbf{ w}) = \sum_{i=1}^n\left(y_i - \mathbf{ w}^\top \mathbf{
x}_i\right)^2
\]</span> To compute the gradient of the objective, we first expand the
brackets.</p>
<h2 id="bracket-expansion">Bracket Expansion</h2>
<p><span class="math display">\[
\begin{align*}
  L(\mathbf{ w},\sigma^2)  = &amp; \sum
_{i=1}^{n}y_i^{2}- 2\sum
_{i=1}^{n}y_i\mathbf{ w}^{\top}\mathbf{ x}_i\\&amp;+\sum
_{i=1}^{n}\mathbf{ w}^{\top}\mathbf{ x}_i\mathbf{ x}_i^{\top}\mathbf{
w}\\
    = &amp; \sum_{i=1}^{n}y_i^{2}-
2 \mathbf{ w}^\top\sum_{i=1}^{n}\mathbf{ x}_iy_i\\&amp;+
\mathbf{ w}^{\top}\left[\sum
_{i=1}^{n}\mathbf{ x}_i\mathbf{ x}_i^{\top}\right]\mathbf{ w}.
\end{align*}
\]</span></p>
<h1 id="solution-with-linear-algebra">Solution with Linear Algebra</h1>
<p>In this section we’re going compute the minimum of the quadratic loss
with respect to the parameters. When we do this, we’ll also review
<em>linear algebra</em>. We will represent all our errors and functions
in the form of matrices and vectors.</p>
<p>Linear algebra is just a shorthand for performing lots of
multiplications and additions simultaneously. What does it have to do
with our system then? Well, the first thing to note is that the classic
linear function we fit for a one-dimensional regression has the form:
<span class="math display">\[
f(x) = mx + c
\]</span> the classical form for a straight line. From a linear
algebraic perspective, we are looking for multiplications and additions.
We are also looking to separate our parameters from our data. The data
is the <em>givens</em>. In French the word is données literally
translated means <em>givens</em> that’s great, because we don’t need to
change the data, what we need to change are the parameters (or
variables) of the model. In this function the data comes in through
<span class="math inline">\(x\)</span>, and the parameters are <span
class="math inline">\(m\)</span> and <span
class="math inline">\(c\)</span>.</p>
<p>What we’d like to create is a vector of parameters and a vector of
data. Then we could represent the system with vectors that represent the
data, and vectors that represent the parameters.</p>
<p>We look to turn the multiplications and additions into a linear
algebraic form, we have one multiplication (<span
class="math inline">\(m\times c\)</span>) and one addition (<span
class="math inline">\(mx + c\)</span>). But we can turn this into an
inner product by writing it in the following way, <span
class="math display">\[
f(x) = m \times x +
c \times 1,
\]</span> in other words, we’ve extracted the unit value from the
offset, <span class="math inline">\(c\)</span>. We can think of this
unit value like an extra item of data, because it is always given to us,
and it is always set to 1 (unlike regular data, which is likely to
vary!). We can therefore write each input data location, <span
class="math inline">\(\mathbf{ x}\)</span>, as a vector <span
class="math display">\[
\mathbf{ x}= \begin{bmatrix} 1\\ x\end{bmatrix}.
\]</span></p>
<p>Now we choose to also turn our parameters into a vector. The
parameter vector will be defined to contain <span
class="math display">\[
\mathbf{ w}= \begin{bmatrix} c \\ m\end{bmatrix}
\]</span> because if we now take the inner product between these two
vectors we recover <span class="math display">\[
\mathbf{ x}\cdot\mathbf{ w}= 1 \times c + x \times m = mx + c
\]</span> In <code>numpy</code> we can define this vector as follows</p>
<div class="sourceCode" id="cb10"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb10-1"><a href="#cb10-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span></code></pre></div>
<div class="sourceCode" id="cb11"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb11-1"><a href="#cb11-1" aria-hidden="true" tabindex="-1"></a><span class="co"># define the vector w</span></span>
<span id="cb11-2"><a href="#cb11-2" aria-hidden="true" tabindex="-1"></a>w <span class="op">=</span> np.zeros(shape<span class="op">=</span>(<span class="dv">2</span>, <span class="dv">1</span>))</span>
<span id="cb11-3"><a href="#cb11-3" aria-hidden="true" tabindex="-1"></a>w[<span class="dv">0</span>] <span class="op">=</span> m</span>
<span id="cb11-4"><a href="#cb11-4" aria-hidden="true" tabindex="-1"></a>w[<span class="dv">1</span>] <span class="op">=</span> c</span></code></pre></div>
<p>This gives us the equivalence between original operation and an
operation in vector space. Whilst the notation here isn’t a lot shorter,
the beauty is that we will be able to add as many features as we like
and keep the same representation. In general, we are now moving to a
system where each of our predictions is given by an inner product. When
we want to represent a linear product in linear algebra, we tend to do
it with the transpose operation, so since we have <span
class="math inline">\(\mathbf{a}\cdot\mathbf{b} =
\mathbf{a}^\top\mathbf{b}\)</span> we can write <span
class="math display">\[
f(\mathbf{ x}_i) = \mathbf{ x}_i^\top\mathbf{ w}.
\]</span> Where we’ve assumed that each data point, <span
class="math inline">\(\mathbf{ x}_i\)</span>, is now written by
appending a 1 onto the original vector <span class="math display">\[
\mathbf{ x}_i = \begin{bmatrix}
1 \\
x_i
\end{bmatrix}
\]</span></p>
<h1 id="design-matrix">Design Matrix</h1>
<p>We can do this for the entire data set to form a <a
href="http://en.wikipedia.org/wiki/Design_matrix"><em>design
matrix</em></a> <span class="math inline">\(\boldsymbol{ \Phi}\)</span>,
<span class="math display">\[
\boldsymbol{ \Phi}
= \begin{bmatrix}
\mathbf{ x}_1^\top \\\
\mathbf{ x}_2^\top \\\
\vdots \\\
\mathbf{ x}_n^\top
\end{bmatrix} = \begin{bmatrix}
1 &amp; x_1 \\\
1 &amp; x_2 \\\
\vdots
&amp; \vdots \\\
1 &amp; x_n
\end{bmatrix},
\]</span> which in <code>numpy</code> can be done with the following
commands:</p>
<div class="sourceCode" id="cb12"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb12-1"><a href="#cb12-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span></code></pre></div>
<div class="sourceCode" id="cb13"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb13-1"><a href="#cb13-1" aria-hidden="true" tabindex="-1"></a>Phi <span class="op">=</span> np.hstack((np.ones_like(x), x))</span>
<span id="cb13-2"><a href="#cb13-2" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(Phi)</span></code></pre></div>
<h2 id="writing-the-objective-with-linear-algebra">Writing the Objective
with Linear Algebra</h2>
<p>When we think of the objective function, we can think of it as the
errors where the error is defined in a similar way to what it was in
Legendre’s day <span class="math inline">\(y_i - f(\mathbf{
x}_i)\)</span>, in statistics these errors are also sometimes called <a
href="http://en.wikipedia.org/wiki/Errors_and_residuals_in_statistics"><em>residuals</em></a>.
So, we can think as the objective and the prediction function as two
separate parts, first we have, <span class="math display">\[
L(\mathbf{ w}) = \sum_{i=1}^n(y_i - f(\mathbf{ x}_i; \mathbf{ w}))^2,
\]</span> where we’ve made the function <span
class="math inline">\(f(\cdot)\)</span>’s dependence on the parameters
<span class="math inline">\(\mathbf{ w}\)</span> explicit in this
equation. Then we have the definition of the function itself, <span
class="math display">\[
f(\mathbf{ x}_i; \mathbf{ w}) = \mathbf{ x}_i^\top \mathbf{ w}.
\]</span> Let’s look again at these two equations and see if we can
identify any inner products. The first equation is a sum of squares,
which is promising. Any sum of squares can be represented by an inner
product, <span class="math display">\[
a = \sum_{i=1}^{k} b^2_i = \mathbf{b}^\top\mathbf{b}.
\]</span> If we wish to represent <span class="math inline">\(L(\mathbf{
w})\)</span> in this way, all we need to do is convert the sum operator
to an inner product. We can get a vector from that sum operator by
placing both <span class="math inline">\(y_i\)</span> and <span
class="math inline">\(f(\mathbf{ x}_i; \mathbf{ w})\)</span> into
vectors, which we do by defining <span class="math display">\[
\mathbf{ y}= \begin{bmatrix}y_1\\ y_2\\ \vdots \\ y_n\end{bmatrix}
\]</span> and defining <span class="math display">\[
\mathbf{ f}(\mathbf{ x}_1; \mathbf{ w}) = \begin{bmatrix}f(\mathbf{
x}_1; \mathbf{ w})\\ f(\mathbf{ x}_2; \mathbf{ w})\\ \vdots \\
f(\mathbf{ x}_n; \mathbf{ w})\end{bmatrix}.
\]</span> The second of these is a vector-valued function. This term may
appear intimidating, but the idea is straightforward. A vector valued
function is simply a vector whose elements are themselves defined as
<em>functions</em>, i.e., it is a vector of functions, rather than a
vector of scalars. The idea is so straightforward, that we are going to
ignore it for the moment, and barely use it in the derivation. But it
will reappear later when we introduce <em>basis functions</em>. So, we
will for the moment ignore the dependence of <span
class="math inline">\(\mathbf{ f}\)</span> on <span
class="math inline">\(\mathbf{ w}\)</span> and <span
class="math inline">\(\boldsymbol{ \Phi}\)</span> and simply summarise
it by a vector of numbers <span class="math display">\[
\mathbf{ f}= \begin{bmatrix}f_1\\f_2\\
\vdots \\ f_n\end{bmatrix}.
\]</span> This allows us to write our objective in the folowing, linear
algebraic form, <span class="math display">\[
L(\mathbf{ w}) = (\mathbf{ y}- \mathbf{ f})^\top(\mathbf{ y}- \mathbf{
f})
\]</span> from the rules of inner products. But what of our matrix <span
class="math inline">\(\boldsymbol{ \Phi}\)</span> of input data? At this
point, we need to dust off <a
href="http://en.wikipedia.org/wiki/Matrix_multiplication"><em>matrix-vector
multiplication</em></a>. Matrix multiplication is simply a convenient
way of performing many inner products together, and it’s exactly what we
need to summarize the operation <span class="math display">\[
f_i = \mathbf{ x}_i^\top\mathbf{ w}.
\]</span> This operation tells us that each element of the vector <span
class="math inline">\(\mathbf{ f}\)</span> (our vector valued function)
is given by an inner product between <span
class="math inline">\(\mathbf{ x}_i\)</span> and <span
class="math inline">\(\mathbf{ w}\)</span>. In other words, it is a
series of inner products. Let’s look at the definition of matrix
multiplication, it takes the form <span class="math display">\[
\mathbf{c} = \mathbf{B}\mathbf{a},
\]</span> where <span class="math inline">\(\mathbf{c}\)</span> might be
a <span class="math inline">\(k\)</span> dimensional vector (which we
can interpret as a <span class="math inline">\(k\times 1\)</span>
dimensional matrix), and <span class="math inline">\(\mathbf{B}\)</span>
is a <span class="math inline">\(k\times k\)</span> dimensional matrix
and <span class="math inline">\(\mathbf{a}\)</span> is a <span
class="math inline">\(k\)</span> dimensional vector (<span
class="math inline">\(k\times 1\)</span> dimensional matrix).</p>
<p>The result of this multiplication is of the form <span
class="math display">\[
\begin{bmatrix}c_1\\c_2 \\ \vdots \\
a_k\end{bmatrix} =
\begin{bmatrix} b_{1,1} &amp; b_{1, 2} &amp; \dots &amp; b_{1, k} \\
b_{2, 1} &amp; b_{2, 2} &amp; \dots &amp; b_{2, k} \\
\vdots &amp; \vdots &amp; \ddots &amp; \vdots \\
b_{k, 1} &amp; b_{k, 2} &amp; \dots &amp; b_{k, k} \end{bmatrix}
\begin{bmatrix}a_1\\a_2 \\
\vdots\\ c_k\end{bmatrix} = \begin{bmatrix} b_{1, 1}a_1 + b_{1, 2}a_2 +
\dots +
b_{1, k}a_k\\
b_{2, 1}a_1 + b_{2, 2}a_2 + \dots + b_{2, k}a_k \\
\vdots\\
b_{k, 1}a_1 + b_{k, 2}a_2 + \dots + b_{k, k}a_k\end{bmatrix}.
\]</span> We see that each element of the result, <span
class="math inline">\(\mathbf{a}\)</span> is simply the inner product
between each <em>row</em> of <span
class="math inline">\(\mathbf{B}\)</span> and the vector <span
class="math inline">\(\mathbf{c}\)</span>. Because we have defined each
element of <span class="math inline">\(\mathbf{ f}\)</span> to be given
by the inner product between each <em>row</em> of the design matrix and
the vector <span class="math inline">\(\mathbf{ w}\)</span> we now can
write the full operation in one matrix multiplication,</p>
<p><span class="math display">\[
\mathbf{ f}= \boldsymbol{ \Phi}\mathbf{ w}.
\]</span></p>
<div class="sourceCode" id="cb14"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb14-1"><a href="#cb14-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span></code></pre></div>
<div class="sourceCode" id="cb15"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb15-1"><a href="#cb15-1" aria-hidden="true" tabindex="-1"></a>f <span class="op">=</span> Phi<span class="op">@</span>w <span class="co"># The @ sign performs matrix multiplication</span></span></code></pre></div>
<p>Combining this result with our objective function, <span
class="math display">\[
L(\mathbf{ w}) = (\mathbf{ y}- \mathbf{ f})^\top(\mathbf{ y}- \mathbf{
f})
\]</span> we find we have defined the <em>model</em> with two equations.
One equation tells us the form of our predictive function and how it
depends on its parameters, the other tells us the form of our objective
function.</p>
<div class="sourceCode" id="cb16"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb16-1"><a href="#cb16-1" aria-hidden="true" tabindex="-1"></a>resid <span class="op">=</span> (y<span class="op">-</span>f)</span>
<span id="cb16-2"><a href="#cb16-2" aria-hidden="true" tabindex="-1"></a>E <span class="op">=</span> np.dot(resid.T, resid) <span class="co"># matrix multiplication on a single vector is equivalent to a dot product.</span></span>
<span id="cb16-3"><a href="#cb16-3" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">&quot;Error function is:&quot;</span>, E)</span></code></pre></div>
<h1 id="objective-optimization">Objective Optimization</h1>
<div style="text-align:right">
<span class="editsection-bracket" style="">[</span><span
class="editsection"
style=""><a href="https://github.com/lawrennd/snippets/edit/main/_ml/includes/linear-regression-objective-optimisation.md" target="_blank" onclick="ga('send', 'event', 'Edit Page', 'Edit', 'https://github.com/lawrennd/snippets/edit/main/_ml/includes/linear-regression-objective-optimisation.md', 13);">edit</a></span><span class="editsection-bracket" style="">]</span>
</div>
<p>Our <em>model</em> has now been defined with two equations: the
prediction function and the objective function. Now we will use
multivariate calculus to define an <em>algorithm</em> to fit the model.
The separation between model and algorithm is important and is often
overlooked. Our model contains a function that shows how it will be used
for prediction, and a function that describes the objective function we
need to optimize to obtain a good set of parameters.</p>
<p>The model linear regression model we have described is still the same
as the one we fitted above with a coordinate ascent algorithm. We have
only played with the notation to obtain the same model in a matrix and
vector notation. However, we will now fit this model with a different
algorithm, one that is much faster. It is such a widely used algorithm
that from the end user’s perspective it doesn’t even look like an
algorithm, it just appears to be a single operation (or function).
However, underneath the computer calls an algorithm to find the
solution. Further, the algorithm we obtain is very widely used, and
because of this it turns out to be highly optimized.</p>
<p>Once again, we are going to try and find the stationary points of our
objective by finding the <em>stationary points</em>. However, the
stationary points of a multivariate function, are a little bit more
complex to find. As before we need to find the point at which the
gradient is zero, but now we need to use <em>multivariate calculus</em>
to find it. This involves learning a few additional rules of
differentiation (that allow you to do the derivatives of a function with
respect to vector), but in the end it makes things quite a bit easier.
We define vectorial derivatives as follows, <span
class="math display">\[
\frac{\text{d}L(\mathbf{ w})}{\text{d}\mathbf{ w}} =
\begin{bmatrix}\frac{\text{d}L(\mathbf{
w})}{\text{d}w_1}\\\frac{\text{d}L(\mathbf{
w})}{\text{d}w_2}\end{bmatrix}.
\]</span> where <span class="math inline">\(\frac{\text{d}L(\mathbf{
w})}{\text{d}w_1}\)</span> is the <a
href="http://en.wikipedia.org/wiki/Partial_derivative">partial
derivative</a> of the error function with respect to <span
class="math inline">\(w_1\)</span>.</p>
<p>Differentiation through multiplications and additions is relatively
straightforward, and since linear algebra is just multiplication and
addition, then its rules of differentiation are quite straightforward
too, but slightly more complex than regular derivatives.</p>
<h2 id="multivariate-derivatives">Multivariate Derivatives</h2>
<p>We will need two rules of multivariate or <em>matrix</em>
differentiation. The first is differentiation of an inner product. By
remembering that the inner product is made up of multiplication and
addition, we can hope that its derivative is quite straightforward, and
so it proves to be. We can start by thinking about the definition of the
inner product, <span class="math display">\[
\mathbf{a}^\top\mathbf{z} = \sum_{i} a_i
z_i,
\]</span> which if we were to take the derivative with respect to <span
class="math inline">\(z_k\)</span> would simply return the gradient of
the one term in the sum for which the derivative was non-zero, that of
<span class="math inline">\(a_k\)</span>, so we know that <span
class="math display">\[
\frac{\text{d}}{\text{d}z_k} \mathbf{a}^\top \mathbf{z} = a_k
\]</span> and by our definition for multivariate derivatives, we can
simply stack all the partial derivatives of this form in a vector to
obtain the result that <span class="math display">\[
\frac{\text{d}}{\text{d}\mathbf{z}}
\mathbf{a}^\top \mathbf{z} = \mathbf{a}.
\]</span> The second rule that’s required is differentiation of a
‘matrix quadratic’. A scalar quadratic in <span
class="math inline">\(z\)</span> with coefficient <span
class="math inline">\(c\)</span> has the form <span
class="math inline">\(cz^2\)</span>. If <span
class="math inline">\(\mathbf{z}\)</span> is a <span
class="math inline">\(k\times 1\)</span> vector and <span
class="math inline">\(\mathbf{C}\)</span> is a <span
class="math inline">\(k \times k\)</span> <em>matrix</em> of
coefficients then the matrix quadratic form is written as <span
class="math inline">\(\mathbf{z}^\top \mathbf{C}\mathbf{z}\)</span>,
which is itself a <em>scalar</em> quantity, but it is a function of a
<em>vector</em>.</p>
<h3 id="matching-dimensions-in-matrix-multiplications">Matching
Dimensions in Matrix Multiplications</h3>
<p>There’s a trick for telling a multiplication leads to a scalar
result. When you are doing mathematics with matrices, it’s always worth
pausing to perform a quick sanity check on the dimensions. Matrix
multplication only works when the dimensions match. To be precise, the
‘inner’ dimension of the matrix must match. What is the inner dimension?
If we multiply two matrices <span
class="math inline">\(\mathbf{A}\)</span> and <span
class="math inline">\(\mathbf{B}\)</span>, the first of which has <span
class="math inline">\(k\)</span> rows and <span
class="math inline">\(\ell\)</span> columns and the second of which has
<span class="math inline">\(p\)</span> rows and <span
class="math inline">\(q\)</span> columns, then we can check whether the
multiplication works by writing the dimensionalities next to each other,
<span class="math display">\[
\mathbf{A} \mathbf{B} \rightarrow (k \times
\underbrace{\ell)(p}_\text{inner dimensions} \times q) \rightarrow
(k\times q).
\]</span> The inner dimensions are the two inside dimensions, <span
class="math inline">\(\ell\)</span> and <span
class="math inline">\(p\)</span>. The multiplication will only work if
<span class="math inline">\(\ell=p\)</span>. The result of the
multiplication will then be a <span class="math inline">\(k\times
q\)</span> matrix: this dimensionality comes from the ‘outer
dimensions’. Note that matrix multiplication is not <a
href="http://en.wikipedia.org/wiki/Commutative_property"><em>commutative</em></a>.
And if you change the order of the multiplication, <span
class="math display">\[
\mathbf{B} \mathbf{A} \rightarrow (\ell \times
\underbrace{k)(q}_\text{inner dimensions} \times p) \rightarrow (\ell
\times p).
\]</span> Firstly, it may no longer even work, because now the condition
is that <span class="math inline">\(k=q\)</span>, and secondly the
result could be of a different dimensionality. An exception is if the
matrices are square matrices (e.g., same number of rows as columns) and
they are both <em>symmetric</em>. A symmetric matrix is one for which
<span class="math inline">\(\mathbf{A}=\mathbf{A}^\top\)</span>, or
equivalently, <span class="math inline">\(a_{i,j} = a_{j,i}\)</span> for
all <span class="math inline">\(i\)</span> and <span
class="math inline">\(j\)</span>.</p>
<p>For applying and developing machine learning algorithms you should
get familiar with working with matrices and vectors. You should have
come across them before, but you may not have used them as extensively
as we are doing now. It’s worth getting used to using this trick to
check your work and ensure you know what the dimension of an output
matrix should be. For our matrix quadratic form, it turns out that we
can see it as a special type of inner product. <span
class="math display">\[
\mathbf{z}^\top\mathbf{C}\mathbf{z} \rightarrow (1\times
\underbrace{k) (k}_\text{inner dimensions}\times k) (k\times 1)
\rightarrow
\mathbf{b}^\top\mathbf{z}
\]</span> where <span class="math inline">\(\mathbf{b} =
\mathbf{C}\mathbf{z}\)</span> so therefore the result is a scalar, <span
class="math display">\[
\mathbf{b}^\top\mathbf{z} \rightarrow
(1\times \underbrace{k) (k}_\text{inner dimensions}\times 1) \rightarrow
(1\times 1)
\]</span> where a <span class="math inline">\((1\times 1)\)</span>
matrix is recognised as a scalar.</p>
<p>This implies that we should be able to differentiate this form, and
indeed the rule for its differentiation is slightly more complex than
the inner product, but still quite simple, <span class="math display">\[
\frac{\text{d}}{\text{d}\mathbf{z}}
\mathbf{z}^\top\mathbf{C}\mathbf{z}= \mathbf{C}\mathbf{z} +
\mathbf{C}^\top
\mathbf{z}.
\]</span> Note that in the special case where <span
class="math inline">\(\mathbf{C}\)</span> is symmetric then we have
<span class="math inline">\(\mathbf{C} = \mathbf{C}^\top\)</span> and
the derivative simplifies to <span class="math display">\[
\frac{\text{d}}{\text{d}\mathbf{z}} \mathbf{z}^\top\mathbf{C}\mathbf{z}=
2\mathbf{C}\mathbf{z}.
\]</span></p>
<h2 id="differentiate-the-objective">Differentiate the Objective</h2>
<p>First, we need to compute the full objective by substituting our
prediction function into the objective function to obtain the objective
in terms of <span class="math inline">\(\mathbf{ w}\)</span>. Doing this
we obtain <span class="math display">\[
L(\mathbf{ w})= (\mathbf{ y}- \boldsymbol{ \Phi}\mathbf{ w})^\top
(\mathbf{ y}- \boldsymbol{ \Phi}\mathbf{ w}).
\]</span> We now need to differentiate this <em>quadratic form</em> to
find the minimum. We differentiate with respect to the <em>vector</em>
<span class="math inline">\(\mathbf{ w}\)</span>. But before we do that,
we’ll expand the brackets in the quadratic form to obtain a series of
scalar terms. The rules for bracket expansion across the vectors are
similar to those for the scalar system giving, <span
class="math display">\[
(\mathbf{a} - \mathbf{b})^\top
(\mathbf{c} - \mathbf{d}) = \mathbf{a}^\top \mathbf{c} - \mathbf{a}^\top
\mathbf{d} - \mathbf{b}^\top \mathbf{c} + \mathbf{b}^\top \mathbf{d}
\]</span> which substituting for <span class="math inline">\(\mathbf{a}
= \mathbf{c} = \mathbf{ y}\)</span> and <span
class="math inline">\(\mathbf{b}=\mathbf{d} = \boldsymbol{ \Phi}\mathbf{
w}\)</span> gives <span class="math display">\[
L(\mathbf{ w})=
\mathbf{ y}^\top\mathbf{ y}- 2\mathbf{ y}^\top\boldsymbol{ \Phi}\mathbf{
w}+
\mathbf{ w}^\top\boldsymbol{ \Phi}^\top\boldsymbol{ \Phi}\mathbf{ w}
\]</span> where we used the fact that <span
class="math inline">\(\mathbf{ y}^\top\boldsymbol{ \Phi}\mathbf{
w}=\mathbf{ w}^\top\boldsymbol{ \Phi}^\top\mathbf{ y}\)</span>.</p>
<p>Now we can use our rules of differentiation to compute the derivative
of this form, which is, <span class="math display">\[
\frac{\text{d}}{\text{d}\mathbf{ w}}L(\mathbf{ w})=- 2\boldsymbol{
\Phi}^\top \mathbf{ y}+
2\boldsymbol{ \Phi}^\top\boldsymbol{ \Phi}\mathbf{ w},
\]</span> where we have exploited the fact that <span
class="math inline">\(\boldsymbol{ \Phi}^\top\boldsymbol{ \Phi}\)</span>
is symmetric to obtain this result.</p>
<h3 id="exercise-1">Exercise 1</h3>
<p>Use the equivalence between our vector and our matrix formulations of
linear regression, alongside our definition of vector derivates, to
match the gradients we’ve computed directly for <span
class="math inline">\(\frac{\text{d}L(c, m)}{\text{d}c}\)</span> and
<span class="math inline">\(\frac{\text{d}L(c, m)}{\text{d}m}\)</span>
to those for <span class="math inline">\(\frac{\text{d}L(\mathbf{
w})}{\text{d}\mathbf{ w}}\)</span>.</p>
<h1 id="update-equation-for-global-optimum">Update Equation for Global
Optimum</h1>
<p>We need to find the minimum of our objective function. Using our
objective function, we can minimize for our parameter vector <span
class="math inline">\(\mathbf{ w}\)</span>. Firstly, we seek stationary
points by find parameter vectors that solve for when the gradients are
zero, <span class="math display">\[
\mathbf{0}=- 2\boldsymbol{ \Phi}^\top
\mathbf{ y}+ 2\boldsymbol{ \Phi}^\top\boldsymbol{ \Phi}\mathbf{ w},
\]</span> where <span class="math inline">\(\mathbf{0}\)</span> is a
<em>vector</em> of zeros. Rearranging this equation, we find the
solution to be <span class="math display">\[
\boldsymbol{ \Phi}^\top \boldsymbol{ \Phi}\mathbf{ w}= \boldsymbol{
\Phi}^\top
\mathbf{ y}
\]</span> which is a matrix equation of the familiar form <span
class="math inline">\(\mathbf{A}\mathbf{x} = \mathbf{b}\)</span>.</p>
<h2 id="solving-the-multivariate-system">Solving the Multivariate
System</h2>
<p>The solution for <span class="math inline">\(\mathbf{ w}\)</span> can
be written mathematically in terms of a matrix inverse of <span
class="math inline">\(\boldsymbol{ \Phi}^\top\boldsymbol{
\Phi}\)</span>, but computation of a matrix inverse requires an
algorithm to resolve it. You’ll know this if you had to invert, by hand,
a <span class="math inline">\(3\times 3\)</span> matrix in high school.
From a numerical stability perspective, it is also best not to compute
the matrix inverse directly, but rather to ask the computer to
<em>solve</em> the system of linear equations given by <span
class="math display">\[
\boldsymbol{ \Phi}^\top\boldsymbol{ \Phi}\mathbf{ w}= \boldsymbol{
\Phi}^\top\mathbf{ y}
\]</span> for <span class="math inline">\(\mathbf{ w}\)</span>.</p>
<h2 id="multivariate-linear-regression">Multivariate Linear
Regression</h2>
<p>A major advantage of the new system is that we can build a linear
regression on a multivariate system. The matrix calculus didn’t specify
what the length of the vector <span class="math inline">\(\mathbf{
x}\)</span> should be, or equivalently the size of the design
matrix.</p>
<h2 id="hessian-matrix">Hessian Matrix</h2>
<div style="text-align:right">
<span class="editsection-bracket" style="">[</span><span
class="editsection"
style=""><a href="https://github.com/lawrennd/snippets/edit/main/_ml/includes/linear-regression-hessian.md" target="_blank" onclick="ga('send', 'event', 'Edit Page', 'Edit', 'https://github.com/lawrennd/snippets/edit/main/_ml/includes/linear-regression-hessian.md', 13);">edit</a></span><span class="editsection-bracket" style="">]</span>
</div>
<p>We can also compute the <a
href="https://en.wikipedia.org/wiki/Hessian_matrix">Hessian matrix</a>,
the curvature of the loss function. To do this we take the second
derivative of the loss function with respect to the parameter vector,
<span class="math display">\[
\frac{\text{d}^2}{\text{d}\mathbf{ w}\text{d}\mathbf{ w}^\top}
L(\mathbf{ w}) = 2\boldsymbol{ \Phi}^\top\boldsymbol{ \Phi}.
\]</span> So, we see that the curvature is only dependent on the design
matrix.</p>
<p>Note that for this linear model the curvature is <em>not</em>
dependent on the values of the parameter vector, <span
class="math inline">\(\mathbf{ w}\)</span>, or indeed on the
<em>response</em> variables, <span class="math inline">\(\mathbf{
y}\)</span>. This is unusual, in general the curvature will depend on
the parameters and the response variables. The linear model with
quadratic loss is a special case because the overall loss function has a
<em>quadratic form</em> which is the unique form with constant curvature
across the whole space.</p>
<p>This is one reason why linear models are so easy to work with.</p>
<p>Because the curvature is constant everywhere, we know that the
curvature at the minimum is given by <span
class="math inline">\(2\boldsymbol{ \Phi}^\top\boldsymbol{
\Phi}\)</span>.</p>
<p>From univariate calculus you might recall that the optimum is a
maximum if the curvature is negative, and a minimum if the curvature is
positive. A similar theorem holds for multivariate calculus, but now the
curvature must be <em>positive definite</em> for the point to be a
minimum. The constant curvature also shows us also that the minimum is
<em>unique</em>.</p>
<p>Positive definite means that for any two vectors, <span
class="math inline">\(\mathbf{u}\)</span> of unit length <span
class="math inline">\(\mathbf{u}^\top\mathbf{u}\)</span> we have that,
<span class="math display">\[
\mathbf{u}^\top\mathbf{A} \mathbf{u} &gt; 0 \quad \forall \quad
\mathbf{u} \quad \text{with}\quad \mathbf{u}^\top\mathbf{u}=1
\]</span> The matrix <span class="math inline">\(\boldsymbol{
\Phi}^\top\boldsymbol{ \Phi}\)</span> (where we’ve dropped the 2) will
satisfy this condition as long as the columns of <span
class="math inline">\(\boldsymbol{ \Phi}\)</span> are <em>linearly
independent</em> and the number of basis functions is less or equal to
the number of data points.</p>
<h2 id="eigendecomposition-of-hessian">Eigendecomposition of
Hessian</h2>
<p>Applying a vector <span class="math inline">\(\mathbf{u}\)</span> to
the Hessian matrix gives us the curvature in a particular direction. So
we can use this to look at the shape of the minimum by projecting onto
the different directions, <span
class="math inline">\(\mathbf{u}\)</span>.</p>
<p>Recall the eigendecomposition of a matrix, <span
class="math display">\[
\mathbf{A}\mathbf{u} = \lambda\mathbf{u}
\]</span> If we allow <span class="math inline">\(\mathbf{u}_i\)</span>
to be an <em>eigenvector</em> of <span
class="math inline">\(\boldsymbol{ \Phi}^\top\boldsymbol{ \Phi}\mathbf{
w}\)</span> then the curvature in that direction is given by the
corresponding eigenvalue, <span
class="math inline">\(\lambda_i\)</span>. <span class="math display">\[
\boldsymbol{ \Phi}^\top\boldsymbol{ \Phi}\mathbf{u}_i = \lambda_i
\mathbf{u}_i.
\]</span></p>
<p>The eigendecomposition of the Hessian is a convenient representation
of the nature of these minima. The principal eigenvector (the one
associated with the largest eigenvalue), <span
class="math inline">\(\mathbf{u}_1\)</span> is associated with the
direction of <em>highest curvature</em>. While the minor eigenvector
shows us the flattest direction, where the curvature is smallest.</p>
<h1 id="nigeria-nmis-data">Nigeria NMIS Data</h1>
<div style="text-align:right">
<span class="editsection-bracket" style="">[</span><span
class="editsection"
style=""><a href="https://github.com/lawrennd/snippets/edit/main/_datasets/includes/nigeria-nmis-data.md" target="_blank" onclick="ga('send', 'event', 'Edit Page', 'Edit', 'https://github.com/lawrennd/snippets/edit/main/_datasets/includes/nigeria-nmis-data.md', 13);">edit</a></span><span class="editsection-bracket" style="">]</span>
</div>
<p>As an example data set we will use Nigerian Millennium Development
Goals Information System Health Facility <span class="citation"
data-cites="Nigeria-nmis14">(The Office of the Senior Special Assistant
to the President on the Millennium Development Goals (OSSAP-MDGs) and
Columbia University, 2014)</span>. It can be found here <a
href="https://energydata.info/dataset/nigeria-nmis-education-facility-data-2014"
class="uri">https://energydata.info/dataset/nigeria-nmis-education-facility-data-2014</a>.</p>
<p>Taking from the information on the site,</p>
<blockquote>
<p>The Nigeria MDG (Millennium Development Goals) Information System –
NMIS health facility data is collected by the Office of the Senior
Special Assistant to the President on the Millennium Development Goals
(OSSAP-MDGs) in partner with the Sustainable Engineering Lab at Columbia
University. A rigorous, geo-referenced baseline facility inventory
across Nigeria is created spanning from 2009 to 2011 with an additional
survey effort to increase coverage in 2014, to build Nigeria’s first
nation-wide inventory of health facility. The database includes 34,139
health facilities info in Nigeria.</p>
<p>The goal of this database is to make the data collected available to
planners, government officials, and the public, to be used to make
strategic decisions for planning relevant interventions.</p>
<p>For data inquiry, please contact Ms. Funlola Osinupebi, Performance
Monitoring &amp; Communications, Advisory Power Team, Office of the Vice
President at funlola.osinupebi@aptovp.org</p>
<p>To learn more, please visit <a
href="http://csd.columbia.edu/2014/03/10/the-nigeria-mdg-information-system-nmis-takes-open-data-further/"
class="uri">http://csd.columbia.edu/2014/03/10/the-nigeria-mdg-information-system-nmis-takes-open-data-further/</a></p>
<p>Suggested citation: Nigeria NMIS facility database (2014), the Office
of the Senior Special Assistant to the President on the Millennium
Development Goals (OSSAP-MDGs) &amp; Columbia University</p>
</blockquote>
<p>For ease of use we’ve packaged this data set in the <code>pods</code>
library</p>
<div class="sourceCode" id="cb17"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb17-1"><a href="#cb17-1" aria-hidden="true" tabindex="-1"></a>data <span class="op">=</span> pods.datasets.nigeria_nmis()[<span class="st">&#39;Y&#39;</span>]</span>
<span id="cb17-2"><a href="#cb17-2" aria-hidden="true" tabindex="-1"></a>data.head()</span></code></pre></div>
<p>Alternatively, you can access the data directly with the following
commands.</p>
<div class="sourceCode" id="cb18"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb18-1"><a href="#cb18-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> urllib.request</span>
<span id="cb18-2"><a href="#cb18-2" aria-hidden="true" tabindex="-1"></a>urllib.request.urlretrieve(<span class="st">&#39;https://energydata.info/dataset/f85d1796-e7f2-4630-be84-79420174e3bd/resource/6e640a13-cab4-457b-b9e6-0336051bac27/download/healthmopupandbaselinenmisfacility.csv&#39;</span>, <span class="st">&#39;healthmopupandbaselinenmisfacility.csv&#39;</span>)</span>
<span id="cb18-3"><a href="#cb18-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-4"><a href="#cb18-4" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> pandas <span class="im">as</span> pd</span>
<span id="cb18-5"><a href="#cb18-5" aria-hidden="true" tabindex="-1"></a>data <span class="op">=</span> pd.read_csv(<span class="st">&#39;healthmopupandbaselinenmisfacility.csv&#39;</span>)</span></code></pre></div>
<p>Once it is loaded in the data can be summarized using the
<code>describe</code> method in pandas.</p>
<div class="sourceCode" id="cb19"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb19-1"><a href="#cb19-1" aria-hidden="true" tabindex="-1"></a>data.describe()</span></code></pre></div>
<p>We can also find out the dimensions of the dataset using the
<code>shape</code> property.</p>
<div class="sourceCode" id="cb20"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb20-1"><a href="#cb20-1" aria-hidden="true" tabindex="-1"></a>data.shape</span></code></pre></div>
<p>Dataframes have different functions that you can use to explore and
understand your data. In python and the Jupyter notebook it is possible
to see a list of all possible functions and attributes by typing the
name of the object followed by <code>.&lt;Tab&gt;</code> for example in
the above case if we type <code>data.&lt;Tab&gt;</code> it show the
columns available (these are attributes in pandas dataframes) such as
<code>num_nurses_fulltime</code>, and also functions, such as
<code>.describe()</code>.</p>
<p>For functions we can also see the documentation about the function by
following the name with a question mark. This will open a box with
documentation at the bottom which can be closed with the x button.</p>
<div class="sourceCode" id="cb21"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb21-1"><a href="#cb21-1" aria-hidden="true" tabindex="-1"></a>data.describe?</span></code></pre></div>
<div class="figure">
<div id="nigerian-health-facilities-figure" class="figure-frame">
<div class="centered" style="">
<img class="" src="https://mlatcl.github.io/deepnn/./slides/diagrams//ml/nigerian-health-facilities.png" width="60%" height="auto" align="center" style="background:none; border:none; box-shadow:none; display:block; margin-left:auto; margin-right:auto;vertical-align:middle">
</div>
</div>
<div id="nigerian-health-facilities-magnify" class="magnify"
onclick="magnifyFigure(&#39;nigerian-health-facilities&#39;)">
<img class="img-button" src="{{ '/assets/images/Magnify_Large.svg' | relative_url }}" style="width:1.5ex">
</div>
<div id="nigerian-health-facilities-caption" class="caption-frame">
<p>Figure: Location of the over thirty-four thousand health facilities
registered in the NMIS data across Nigeria. Each facility plotted
according to its latitude and longitude.</p>
</div>
</div>
<h2 id="multivariate-regression-on-nigeria-nmis-data">Multivariate
Regression on Nigeria NMIS Data</h2>
<div style="text-align:right">
<span class="editsection-bracket" style="">[</span><span
class="editsection"
style=""><a href="https://github.com/lawrennd/snippets/edit/main/_ml/includes/nigeria-nmis-linear-regression.md" target="_blank" onclick="ga('send', 'event', 'Edit Page', 'Edit', 'https://github.com/lawrennd/snippets/edit/main/_ml/includes/nigeria-nmis-linear-regression.md', 13);">edit</a></span><span class="editsection-bracket" style="">]</span>
</div>
<p>Now we will build a design matrix based on the numeric features that
include the number of nurses, and the number of midwives, the latitude
and longitude. These are our <em>covariates</em>. The response variable
will be the number of doctors. We build the design matrix as
follows:</p>
<p>Bias as an additional feature.</p>
<p>First we select the covariates and response variables and drop any
rows where there are missing values using the <code>pandas</code>
<code>dropna</code> method.</p>
<div class="sourceCode" id="cb22"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb22-1"><a href="#cb22-1" aria-hidden="true" tabindex="-1"></a>covariates <span class="op">=</span> [<span class="st">&#39;num_nurses_fulltime&#39;</span>, <span class="st">&#39;num_nursemidwives_fulltime&#39;</span>, <span class="st">&#39;latitude&#39;</span>, <span class="st">&#39;longitude&#39;</span>]</span>
<span id="cb22-2"><a href="#cb22-2" aria-hidden="true" tabindex="-1"></a>response <span class="op">=</span> [<span class="st">&#39;num_doctors_fulltime&#39;</span>]</span>
<span id="cb22-3"><a href="#cb22-3" aria-hidden="true" tabindex="-1"></a>data_without_missing <span class="op">=</span> data[covariates <span class="op">+</span> response].dropna()</span></code></pre></div>
<p>We can see how many rows we have dropped and have a quick sanity
check on our new data frame with <code>len</code>.</p>
<div class="sourceCode" id="cb23"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb23-1"><a href="#cb23-1" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="bu">len</span>(data) <span class="op">-</span> <span class="bu">len</span>(data_without_missing))</span></code></pre></div>
<p>We see that 2,735 of the entries had missing values in one of our
variables of interest.</p>
<p>You may also want to use <code>describe</code> or other functions to
explore the new data frame.</p>
<p>Now let’s perform a linear regression. But this time, we will create
a pandas data frame for the result so we can store it in a form that we
can visualize easily.</p>
<div class="sourceCode" id="cb24"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb24-1"><a href="#cb24-1" aria-hidden="true" tabindex="-1"></a>Phi <span class="op">=</span> data_without_missing[covariates]</span>
<span id="cb24-2"><a href="#cb24-2" aria-hidden="true" tabindex="-1"></a>Phi[<span class="st">&#39;Eins&#39;</span>] <span class="op">=</span> <span class="dv">1</span> <span class="co"># add a column for the offset</span></span>
<span id="cb24-3"><a href="#cb24-3" aria-hidden="true" tabindex="-1"></a>y <span class="op">=</span> data_without_missing[response]</span></code></pre></div>
<div class="sourceCode" id="cb25"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb25-1"><a href="#cb25-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> pandas <span class="im">as</span> pd</span></code></pre></div>
<div class="sourceCode" id="cb26"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb26-1"><a href="#cb26-1" aria-hidden="true" tabindex="-1"></a>w <span class="op">=</span> pd.DataFrame(data<span class="op">=</span>np.linalg.solve(Phi.T<span class="op">@</span>Phi, Phi.T<span class="op">@</span>y),  <span class="co"># solve linear regression here</span></span>
<span id="cb26-2"><a href="#cb26-2" aria-hidden="true" tabindex="-1"></a>                 index <span class="op">=</span> Phi.columns,  <span class="co"># columns of Phi become rows of w</span></span>
<span id="cb26-3"><a href="#cb26-3" aria-hidden="true" tabindex="-1"></a>                 columns<span class="op">=</span>[<span class="st">&#39;regression_coefficient&#39;</span>]) <span class="co"># the column of Phi is the value of regression coefficient</span></span></code></pre></div>
<p>We can check the residuals to see how good our estimates are. First
we create a pandas data frame containing the predictions and use it to
compute the residuals.</p>
<div class="sourceCode" id="cb27"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb27-1"><a href="#cb27-1" aria-hidden="true" tabindex="-1"></a>ypred <span class="op">=</span> pd.DataFrame(data<span class="op">=</span>(Phi<span class="op">@</span>w).values, columns<span class="op">=</span>[<span class="st">&#39;num_doctors_fulltime&#39;</span>])</span>
<span id="cb27-2"><a href="#cb27-2" aria-hidden="true" tabindex="-1"></a>resid <span class="op">=</span> y<span class="op">-</span>ypred</span></code></pre></div>
<p>Let’s look at the residuals. We can use <code>describe</code> to get
a summary.</p>
<div class="sourceCode" id="cb28"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb28-1"><a href="#cb28-1" aria-hidden="true" tabindex="-1"></a>resid.describe()</span></code></pre></div>
<p>We can see that while the standard deviation of our residuals is
around 3, (this is equivalent to a root mean square error). The smallest
and largest residual sow there are some significant outliers that our
regression isn’t picking up.</p>
<div class="figure">
<div id="nigeria-nmis-num-doctors-residuals-figure"
class="figure-frame">
<object class="svgplot " data="https://mlatcl.github.io/deepnn/./slides/diagrams//ml/nigeria-nmis-num-doctors-residuals.svg" width="80%" style=" ">
</object>
</div>
<div id="nigeria-nmis-num-doctors-residuals-magnify" class="magnify"
onclick="magnifyFigure(&#39;nigeria-nmis-num-doctors-residuals&#39;)">
<img class="img-button" src="{{ '/assets/images/Magnify_Large.svg' | relative_url }}" style="width:1.5ex">
</div>
<div id="nigeria-nmis-num-doctors-residuals-caption"
class="caption-frame">
<p>Figure: Residual values for the ratings from the prediction of the
movie rating given the data from the film.</p>
</div>
</div>
<p>Which shows our model <em>hasn’t</em> yet done a great job of
representation, because the spread of values is large. We can check what
the rating is dominated by in terms of regression coefficients.</p>
<div class="sourceCode" id="cb29"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb29-1"><a href="#cb29-1" aria-hidden="true" tabindex="-1"></a>w</span></code></pre></div>
<p>Checking our regression coefficients, we see that the number of
doctors is positively influenced by the number of nurses and the number
of midwives. The latitude and longitude have a smaller effect. The bias
term (‘eins’) is a small positive offset.</p>
<h1 id="aside">Aside</h1>
<p>Just for informational purposes, the actual approach used in software
for fitting a linear model <em>should</em> be a QR decomposition.</p>
<h2 id="solution-with-qr-decomposition">Solution with QR
Decomposition</h2>
<div style="text-align:right">
<span class="editsection-bracket" style="">[</span><span
class="editsection"
style=""><a href="https://github.com/lawrennd/snippets/edit/main/_ml/includes/qr-decomposition-regression.md" target="_blank" onclick="ga('send', 'event', 'Edit Page', 'Edit', 'https://github.com/lawrennd/snippets/edit/main/_ml/includes/qr-decomposition-regression.md', 13);">edit</a></span><span class="editsection-bracket" style="">]</span>
</div>
<p>Performing a solve instead of a matrix inverse is the more
numerically stable approach, but we can do even better. A <a
href="http://en.wikipedia.org/wiki/QR_decomposition">QR-decomposition</a>
of a matrix factorizes it into a matrix which is an orthogonal matrix
<span class="math inline">\(\mathbf{Q}\)</span>, so that <span
class="math inline">\(\mathbf{Q}^\top \mathbf{Q} = \mathbf{I}\)</span>.
And a matrix which is upper triangular, <span
class="math inline">\(\mathbf{R}\)</span>. <span class="math display">\[
\boldsymbol{ \Phi}^\top \boldsymbol{ \Phi}\boldsymbol{\beta} =
\boldsymbol{ \Phi}^\top \mathbf{ y}
\]</span> and we substitute <span class="math inline">\(\boldsymbol{
\Phi}= \mathbf{Q}{\mathbf{R}\)</span> so we have <span
class="math display">\[
(\mathbf{Q}\mathbf{R})^\top
(\mathbf{Q}\mathbf{R})\boldsymbol{\beta} = (\mathbf{Q}\mathbf{R})^\top
\mathbf{ y}
\]</span> <span class="math display">\[
\mathbf{R}^\top (\mathbf{Q}^\top \mathbf{Q}) \mathbf{R}
\boldsymbol{\beta} = \mathbf{R}^\top \mathbf{Q}^\top \mathbf{ y}
\]</span></p>
<p><span class="math display">\[
\mathbf{R}^\top \mathbf{R} \boldsymbol{\beta} = \mathbf{R}^\top
\mathbf{Q}^\top
\mathbf{ y}
\]</span> <span class="math display">\[
\mathbf{R} \boldsymbol{\beta} = \mathbf{Q}^\top \mathbf{ y}
\]</span> which leaves us with a lower triangular system to solve.</p>
<p>This is a more numerically stable solution because it removes the
need to compute <span class="math inline">\(\boldsymbol{
\Phi}^\top\boldsymbol{ \Phi}\)</span> as an intermediate. Computing
<span class="math inline">\(\boldsymbol{ \Phi}^\top\boldsymbol{
\Phi}\)</span> is a bad idea because it involves squaring all the
elements of <span class="math inline">\(\boldsymbol{ \Phi}\)</span> and
thereby potentially reducing the numerical precision with which we can
represent the solution. Operating on <span
class="math inline">\(\boldsymbol{ \Phi}\)</span> directly preserves the
numerical precision of the model.</p>
<p>This can be more particularly seen when we begin to work with
<em>basis functions</em> in the next session. Some systems that can be
resolved with the QR decomposition cannot be resolved by using solve
directly.</p>
<div class="sourceCode" id="cb30"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb30-1"><a href="#cb30-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> scipy <span class="im">as</span> sp</span></code></pre></div>
<div class="sourceCode" id="cb31"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb31-1"><a href="#cb31-1" aria-hidden="true" tabindex="-1"></a>Q, R <span class="op">=</span> np.linalg.qr(Phi)</span>
<span id="cb31-2"><a href="#cb31-2" aria-hidden="true" tabindex="-1"></a>w <span class="op">=</span> sp.linalg.solve_triangular(R, Q.T<span class="op">@</span>y) </span>
<span id="cb31-3"><a href="#cb31-3" aria-hidden="true" tabindex="-1"></a>w <span class="op">=</span> pd.DataFrame(w, index<span class="op">=</span>Phi.columns)</span>
<span id="cb31-4"><a href="#cb31-4" aria-hidden="true" tabindex="-1"></a>w</span></code></pre></div>
<h2 id="basis-function-models">Basis Function Models</h2>
<p>We are reviewing models that are <em>linear</em> in the parameters.
Very often we are interested in <em>non-linear</em> predictions. We can
make models that are linear in the parameters and given non-linear
predictions by introducing non-linear <em>basis functions</em>. A common
example is the polynomial basis.</p>
<h2 id="polynomial-basis">Polynomial Basis</h2>
<div style="text-align:right">
<span class="editsection-bracket" style="">[</span><span
class="editsection"
style=""><a href="https://github.com/lawrennd/snippets/edit/main/_ml/includes/polynomial-basis.md" target="_blank" onclick="ga('send', 'event', 'Edit Page', 'Edit', 'https://github.com/lawrennd/snippets/edit/main/_ml/includes/polynomial-basis.md', 13);">edit</a></span><span class="editsection-bracket" style="">]</span>
</div>
<p>The polynomial basis combines higher order polynomials together to
create the function. For example, the fourth order polynomial has five
components to its basis function. <span class="math display">\[
\phi_j(x) = x^j
\]</span></p>
<div class="sourceCode" id="cb32"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb32-1"><a href="#cb32-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span></code></pre></div>
<div class="sourceCode" id="cb33"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb33-1"><a href="#cb33-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> mlai</span></code></pre></div>
<div class="sourceCode" id="cb34"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb34-1"><a href="#cb34-1" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb34-2"><a href="#cb34-2" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> mlai <span class="im">import</span> polynomial</span></code></pre></div>
<div class="figure">
<div id="polynomial-basis-2-figure" class="figure-frame">
<object class="svgplot " data="https://mlatcl.github.io/deepnn/./slides/diagrams//ml/polynomial_basis004.svg" width="80%" style=" ">
</object>
</div>
<div id="polynomial-basis-2-magnify" class="magnify"
onclick="magnifyFigure(&#39;polynomial-basis-2&#39;)">
<img class="img-button" src="{{ '/assets/images/Magnify_Large.svg' | relative_url }}" style="width:1.5ex">
</div>
<div id="polynomial-basis-2-caption" class="caption-frame">
<p>Figure: The set of functions which are combined to form a
<em>polynomial</em> basis.</p>
</div>
</div>
<h2 id="functions-derived-from-polynomial-basis">Functions Derived from
Polynomial Basis</h2>
<p><span class="math display">\[
f(x) = {\color{red}{w_0}} + {\color{magenta}{w_1 x}} + {\color{blue}{w_2
x^2}} + {\color{green}{w_3 x^3}} + {\color{cyan}{w_4 x^4}}
\]</span></p>
<div class="figure">
<div id="polynomial-function-2-figure" class="figure-frame">
<object class="svgplot " data="https://mlatcl.github.io/deepnn/./slides/diagrams//ml/polynomial_function002.svg" width="80%" style=" ">
</object>
</div>
<div id="polynomial-function-2-magnify" class="magnify"
onclick="magnifyFigure(&#39;polynomial-function-2&#39;)">
<img class="img-button" src="{{ '/assets/images/Magnify_Large.svg' | relative_url }}" style="width:1.5ex">
</div>
<div id="polynomial-function-2-caption" class="caption-frame">
<p>Figure: A random combination of functions from the polynomial
basis.</p>
</div>
</div>
<p>The predictions from this model, <span class="math display">\[
f(x) = w_0 + w_1 x+ w_2 x^2 + w_3 x^3 + w_4 x^4
\]</span> are <em>linear</em> in the parameters, <span
class="math inline">\(\mathbf{ w}\)</span>, but <em>non-linear</em> in
the input <span class="math inline">\(x^3\)</span>. Here we are showing
a polynomial basis for a 1-dimensional input, <span
class="math inline">\(x\)</span>, but basis functions can also be
constructed for multidimensional inputs, <span
class="math inline">\(\mathbf{ x}\)</span>.}</p>
<p>In the neural network models, the “RELU function” is normally used as
a basis function, but for illustration we will continue with the
polynomial basis for these linear models.</p>
<h2 id="olympic-marathon-data-1">Olympic Marathon Data</h2>
<div style="text-align:right">
<span class="editsection-bracket" style="">[</span><span
class="editsection"
style=""><a href="https://github.com/lawrennd/snippets/edit/main/_datasets/includes/olympic-marathon-data.md" target="_blank" onclick="ga('send', 'event', 'Edit Page', 'Edit', 'https://github.com/lawrennd/snippets/edit/main/_datasets/includes/olympic-marathon-data.md', 13);">edit</a></span><span class="editsection-bracket" style="">]</span>
</div>
<table>
<tr>
<td width="70%">
<ul>
<li>Gold medal times for Olympic Marathon since 1896.</li>
<li>Marathons before 1924 didn’t have a standardized distance.</li>
<li>Present results using pace per km.</li>
<li>In 1904 Marathon was badly organized leading to very slow
times.</li>
</ul>
</td>
<td width="30%">
<div class="centered" style="">
<img class="" src="https://mlatcl.github.io/deepnn/./slides/diagrams//Stephen_Kiprotich.jpg" width="100%" height="auto" align="center" style="background:none; border:none; box-shadow:none; display:block; margin-left:auto; margin-right:auto;vertical-align:middle">
</div>
<small>Image from Wikimedia Commons <a href="http://bit.ly/16kMKHQ"
class="uri">http://bit.ly/16kMKHQ</a></small>
</td>
</tr>
</table>
<p>The first thing we will do is load a standard data set for regression
modelling. The data consists of the pace of Olympic Gold Medal Marathon
winners for the Olympics from 1896 to present. Let’s load in the data
and plot.</p>
<div class="sourceCode" id="cb35"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb35-1"><a href="#cb35-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb35-2"><a href="#cb35-2" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> pods</span></code></pre></div>
<div class="sourceCode" id="cb36"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb36-1"><a href="#cb36-1" aria-hidden="true" tabindex="-1"></a>data <span class="op">=</span> pods.datasets.olympic_marathon_men()</span>
<span id="cb36-2"><a href="#cb36-2" aria-hidden="true" tabindex="-1"></a>x <span class="op">=</span> data[<span class="st">&#39;X&#39;</span>]</span>
<span id="cb36-3"><a href="#cb36-3" aria-hidden="true" tabindex="-1"></a>y <span class="op">=</span> data[<span class="st">&#39;Y&#39;</span>]</span>
<span id="cb36-4"><a href="#cb36-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb36-5"><a href="#cb36-5" aria-hidden="true" tabindex="-1"></a>offset <span class="op">=</span> y.mean()</span>
<span id="cb36-6"><a href="#cb36-6" aria-hidden="true" tabindex="-1"></a>scale <span class="op">=</span> np.sqrt(y.var())</span>
<span id="cb36-7"><a href="#cb36-7" aria-hidden="true" tabindex="-1"></a>yhat <span class="op">=</span> (y <span class="op">-</span> offset)<span class="op">/</span>scale</span></code></pre></div>
<div class="figure">
<div id="olympic-marathon-figure" class="figure-frame">
<object class="svgplot " data="https://mlatcl.github.io/deepnn/./slides/diagrams//datasets/olympic-marathon.svg" width="80%" style=" ">
</object>
</div>
<div id="olympic-marathon-magnify" class="magnify"
onclick="magnifyFigure(&#39;olympic-marathon&#39;)">
<img class="img-button" src="{{ '/assets/images/Magnify_Large.svg' | relative_url }}" style="width:1.5ex">
</div>
<div id="olympic-marathon-caption" class="caption-frame">
<p>Figure: Olympic marathon pace times since 1896.</p>
</div>
</div>
<p>Things to notice about the data include the outlier in 1904, in that
year the Olympics was in St Louis, USA. Organizational problems and
challenges with dust kicked up by the cars following the race meant that
participants got lost, and only very few participants completed. More
recent years see more consistently quick marathons.</p>
<h2 id="polynomial-fits-to-olympic-marthon-data">Polynomial Fits to
Olympic Marthon Data</h2>
<div style="text-align:right">
<span class="editsection-bracket" style="">[</span><span
class="editsection"
style=""><a href="https://github.com/lawrennd/snippets/edit/main/_ml/includes/olympic-marathon-polynomial.md" target="_blank" onclick="ga('send', 'event', 'Edit Page', 'Edit', 'https://github.com/lawrennd/snippets/edit/main/_ml/includes/olympic-marathon-polynomial.md', 13);">edit</a></span><span class="editsection-bracket" style="">]</span>
</div>
<div class="sourceCode" id="cb37"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb37-1"><a href="#cb37-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span></code></pre></div>
<p>Define the polynomial basis function.</p>
<div class="sourceCode" id="cb38"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb38-1"><a href="#cb38-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> mlai</span></code></pre></div>
<div class="sourceCode" id="cb39"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb39-1"><a href="#cb39-1" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb39-2"><a href="#cb39-2" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> mlai <span class="im">import</span> polynomial</span></code></pre></div>
<div class="sourceCode" id="cb40"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb40-1"><a href="#cb40-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> polynomial(x, num_basis<span class="op">=</span><span class="dv">4</span>, data_limits<span class="op">=</span>[<span class="op">-</span><span class="fl">1.</span>, <span class="fl">1.</span>]):</span>
<span id="cb40-2"><a href="#cb40-2" aria-hidden="true" tabindex="-1"></a>    <span class="co">&quot;Polynomial basis&quot;</span></span>
<span id="cb40-3"><a href="#cb40-3" aria-hidden="true" tabindex="-1"></a>    centre <span class="op">=</span> data_limits[<span class="dv">0</span>]<span class="op">/</span><span class="fl">2.</span> <span class="op">+</span> data_limits[<span class="dv">1</span>]<span class="op">/</span><span class="fl">2.</span></span>
<span id="cb40-4"><a href="#cb40-4" aria-hidden="true" tabindex="-1"></a>    span <span class="op">=</span> data_limits[<span class="dv">1</span>] <span class="op">-</span> data_limits[<span class="dv">0</span>]</span>
<span id="cb40-5"><a href="#cb40-5" aria-hidden="true" tabindex="-1"></a>    z <span class="op">=</span> np.asarray(x, dtype<span class="op">=</span><span class="bu">float</span>) <span class="op">-</span> centre</span>
<span id="cb40-6"><a href="#cb40-6" aria-hidden="true" tabindex="-1"></a>    z <span class="op">=</span> <span class="dv">2</span><span class="op">*</span>z<span class="op">/</span>span   <span class="co"># scale the inputs to be within -1, 1 where polynomials are well behaved</span></span>
<span id="cb40-7"><a href="#cb40-7" aria-hidden="true" tabindex="-1"></a>    Phi <span class="op">=</span> np.zeros((x.shape[<span class="dv">0</span>], num_basis))</span>
<span id="cb40-8"><a href="#cb40-8" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(num_basis):</span>
<span id="cb40-9"><a href="#cb40-9" aria-hidden="true" tabindex="-1"></a>        Phi[:, i:i<span class="op">+</span><span class="dv">1</span>] <span class="op">=</span> z<span class="op">**</span>i</span>
<span id="cb40-10"><a href="#cb40-10" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> Phi</span></code></pre></div>
<p>Now we include the solution for the linear regression through
QR-decomposition.</p>
<div class="sourceCode" id="cb41"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb41-1"><a href="#cb41-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> basis_fit(Phi, y):</span>
<span id="cb41-2"><a href="#cb41-2" aria-hidden="true" tabindex="-1"></a>    <span class="co">&quot;Use QR decomposition to fit the basis.&quot;</span><span class="st">&quot;&quot;</span></span>
<span id="cb41-3"><a href="#cb41-3" aria-hidden="true" tabindex="-1"></a>    Q, R <span class="op">=</span> np.linalg.qr(Phi)</span>
<span id="cb41-4"><a href="#cb41-4" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> sp.linalg.solve_triangular(R, Q.T<span class="op">@</span>y) </span></code></pre></div>
<h2 id="linear-fit">Linear Fit</h2>
<div class="sourceCode" id="cb42"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb42-1"><a href="#cb42-1" aria-hidden="true" tabindex="-1"></a>poly_args <span class="op">=</span> {<span class="st">&#39;num_basis&#39;</span>:<span class="dv">2</span>, <span class="co"># two basis functions (1 and x)</span></span>
<span id="cb42-2"><a href="#cb42-2" aria-hidden="true" tabindex="-1"></a>             <span class="st">&#39;data_limits&#39;</span>:xlim}</span>
<span id="cb42-3"><a href="#cb42-3" aria-hidden="true" tabindex="-1"></a>Phi <span class="op">=</span> polynomial(x, <span class="op">**</span>poly_args)</span>
<span id="cb42-4"><a href="#cb42-4" aria-hidden="true" tabindex="-1"></a>w <span class="op">=</span> basis_fit(Phi, y)</span></code></pre></div>
<p>Now we make some predictions for the fit.</p>
<div class="sourceCode" id="cb43"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb43-1"><a href="#cb43-1" aria-hidden="true" tabindex="-1"></a>x_pred <span class="op">=</span> np.linspace(xlim[<span class="dv">0</span>], xlim[<span class="dv">1</span>], <span class="dv">400</span>)[:, np.newaxis]</span>
<span id="cb43-2"><a href="#cb43-2" aria-hidden="true" tabindex="-1"></a>Phi_pred <span class="op">=</span> polynomial(x_pred, <span class="op">**</span>poly_args)</span>
<span id="cb43-3"><a href="#cb43-3" aria-hidden="true" tabindex="-1"></a>f_pred <span class="op">=</span> Phi_pred<span class="op">@</span>w</span></code></pre></div>
<div class="figure">
<div id="olympic-marathon-polynomial-2-figure" class="figure-frame">
<object class="svgplot " data="https://mlatcl.github.io/deepnn/./slides/diagrams//ml/olympic-marathon-polynomial-2.svg" width="80%" style=" ">
</object>
</div>
<div id="olympic-marathon-polynomial-2-magnify" class="magnify"
onclick="magnifyFigure(&#39;olympic-marathon-polynomial-2&#39;)">
<img class="img-button" src="{{ '/assets/images/Magnify_Large.svg' | relative_url }}" style="width:1.5ex">
</div>
<div id="olympic-marathon-polynomial-2-caption" class="caption-frame">
<p>Figure: Fit of a 1-degree polynomial (a linear model) to the Olympic
marathon data.</p>
</div>
</div>
<h2 id="cubic-fit">Cubic Fit</h2>
<div class="sourceCode" id="cb44"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb44-1"><a href="#cb44-1" aria-hidden="true" tabindex="-1"></a>poly_args <span class="op">=</span> {<span class="st">&#39;num_basis&#39;</span>:<span class="dv">4</span>, <span class="co"># four basis: 1, x, x^2, x^3</span></span>
<span id="cb44-2"><a href="#cb44-2" aria-hidden="true" tabindex="-1"></a>             <span class="st">&#39;data_limits&#39;</span>:xlim}</span>
<span id="cb44-3"><a href="#cb44-3" aria-hidden="true" tabindex="-1"></a>Phi <span class="op">=</span> polynomial(x, <span class="op">**</span>poly_args)</span>
<span id="cb44-4"><a href="#cb44-4" aria-hidden="true" tabindex="-1"></a>w <span class="op">=</span> basis_fit(Phi, y)</span></code></pre></div>
<div class="sourceCode" id="cb45"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb45-1"><a href="#cb45-1" aria-hidden="true" tabindex="-1"></a>Phi_pred <span class="op">=</span> polynomial(x_pred, <span class="op">**</span>poly_args)</span>
<span id="cb45-2"><a href="#cb45-2" aria-hidden="true" tabindex="-1"></a>f_pred <span class="op">=</span> Phi_pred<span class="op">@</span>w</span></code></pre></div>
<div class="figure">
<div id="olympic-marathon-polynomial-4-figure" class="figure-frame">
<object class="svgplot " data="https://mlatcl.github.io/deepnn/./slides/diagrams//ml/olympic-marathon-polynomial-4.svg" width="80%" style=" ">
</object>
</div>
<div id="olympic-marathon-polynomial-4-magnify" class="magnify"
onclick="magnifyFigure(&#39;olympic-marathon-polynomial-4&#39;)">
<img class="img-button" src="{{ '/assets/images/Magnify_Large.svg' | relative_url }}" style="width:1.5ex">
</div>
<div id="olympic-marathon-polynomial-4-caption" class="caption-frame">
<p>Figure: Fit of a 3-degree polynomial (a cubic model) to the Olympic
marathon data.</p>
</div>
</div>
<h2 id="th-degree-polynomial-fit">9th Degree Polynomial Fit</h2>
<p>Now we’ll try a 9th degree polynomial fit to the data.</p>
<div class="sourceCode" id="cb46"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb46-1"><a href="#cb46-1" aria-hidden="true" tabindex="-1"></a>poly_args <span class="op">=</span> {<span class="st">&#39;num_basis&#39;</span>:<span class="dv">10</span>, <span class="co"># basis up to x^9</span></span>
<span id="cb46-2"><a href="#cb46-2" aria-hidden="true" tabindex="-1"></a>             <span class="st">&#39;data_limits&#39;</span>:xlim}</span>
<span id="cb46-3"><a href="#cb46-3" aria-hidden="true" tabindex="-1"></a>Phi <span class="op">=</span> polynomial(x, <span class="op">**</span>poly_args)</span>
<span id="cb46-4"><a href="#cb46-4" aria-hidden="true" tabindex="-1"></a>w <span class="op">=</span> basis_fit(Phi, y)</span></code></pre></div>
<div class="sourceCode" id="cb47"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb47-1"><a href="#cb47-1" aria-hidden="true" tabindex="-1"></a>Phi_pred <span class="op">=</span> polynomial(x_pred, <span class="op">**</span>poly_args)</span>
<span id="cb47-2"><a href="#cb47-2" aria-hidden="true" tabindex="-1"></a>f_pred <span class="op">=</span> Phi_pred<span class="op">@</span>w</span></code></pre></div>
<div class="figure">
<div id="olympic-marathon-polynomial-10-figure" class="figure-frame">
<object class="svgplot " data="https://mlatcl.github.io/deepnn/./slides/diagrams//ml/olympic-marathon-polynomial-10.svg" width="80%" style=" ">
</object>
</div>
<div id="olympic-marathon-polynomial-10-magnify" class="magnify"
onclick="magnifyFigure(&#39;olympic-marathon-polynomial-10&#39;)">
<img class="img-button" src="{{ '/assets/images/Magnify_Large.svg' | relative_url }}" style="width:1.5ex">
</div>
<div id="olympic-marathon-polynomial-10-caption" class="caption-frame">
<p>Figure: Fit of a 9-degree polynomial to the Olympic marathon
data.</p>
</div>
</div>
<h2 id="th-degree-polynomial-fit-1">16th Degree Polynomial Fit</h2>
<p>Now we’ll try a 16th degree polynomial fit to the data.</p>
<div class="sourceCode" id="cb48"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb48-1"><a href="#cb48-1" aria-hidden="true" tabindex="-1"></a>poly_args <span class="op">=</span> {<span class="st">&#39;num_basis&#39;</span>:<span class="dv">17</span>, <span class="co"># basis up to x^16</span></span>
<span id="cb48-2"><a href="#cb48-2" aria-hidden="true" tabindex="-1"></a>             <span class="st">&#39;data_limits&#39;</span>:xlim}</span>
<span id="cb48-3"><a href="#cb48-3" aria-hidden="true" tabindex="-1"></a>Phi <span class="op">=</span> polynomial(x, <span class="op">**</span>poly_args)</span>
<span id="cb48-4"><a href="#cb48-4" aria-hidden="true" tabindex="-1"></a>w <span class="op">=</span> basis_fit(Phi, y)</span></code></pre></div>
<div class="sourceCode" id="cb49"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb49-1"><a href="#cb49-1" aria-hidden="true" tabindex="-1"></a>Phi_pred <span class="op">=</span> polynomial(x_pred, <span class="op">**</span>poly_args)</span>
<span id="cb49-2"><a href="#cb49-2" aria-hidden="true" tabindex="-1"></a>f_pred <span class="op">=</span> Phi_pred<span class="op">@</span>w</span></code></pre></div>
<div class="figure">
<div id="olympic-marathon-polynomial-17-figure" class="figure-frame">
<object class="svgplot " data="https://mlatcl.github.io/deepnn/./slides/diagrams//ml/olympic-marathon-polynomial-17.svg" width="80%" style=" ">
</object>
</div>
<div id="olympic-marathon-polynomial-17-magnify" class="magnify"
onclick="magnifyFigure(&#39;olympic-marathon-polynomial-17&#39;)">
<img class="img-button" src="{{ '/assets/images/Magnify_Large.svg' | relative_url }}" style="width:1.5ex">
</div>
<div id="olympic-marathon-polynomial-17-caption" class="caption-frame">
<p>Figure: Fit of a 16-degree polynomial to the Olympic marathon
data.</p>
</div>
</div>
<h2 id="th-degree-polynomial-fit-2">26th Degree Polynomial Fit</h2>
<p>Now we’ll try a 26th degree polynomial fit to the data.</p>
<div class="sourceCode" id="cb50"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb50-1"><a href="#cb50-1" aria-hidden="true" tabindex="-1"></a>poly_args <span class="op">=</span> {<span class="st">&#39;num_basis&#39;</span>:<span class="dv">27</span>, <span class="co"># basis up to x^26</span></span>
<span id="cb50-2"><a href="#cb50-2" aria-hidden="true" tabindex="-1"></a>             <span class="st">&#39;data_limits&#39;</span>:xlim}</span>
<span id="cb50-3"><a href="#cb50-3" aria-hidden="true" tabindex="-1"></a>Phi <span class="op">=</span> polynomial(x, <span class="op">**</span>poly_args)</span>
<span id="cb50-4"><a href="#cb50-4" aria-hidden="true" tabindex="-1"></a>w <span class="op">=</span> basis_fit(Phi, y)</span></code></pre></div>
<div class="sourceCode" id="cb51"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb51-1"><a href="#cb51-1" aria-hidden="true" tabindex="-1"></a>Phi_pred <span class="op">=</span> polynomial(x_pred, <span class="op">**</span>poly_args)</span>
<span id="cb51-2"><a href="#cb51-2" aria-hidden="true" tabindex="-1"></a>f_pred <span class="op">=</span> Phi_pred<span class="op">@</span>w</span></code></pre></div>
<div class="figure">
<div id="olympic-marathon-polynomial-27-figure" class="figure-frame">
<object class="svgplot " data="https://mlatcl.github.io/deepnn/./slides/diagrams//ml/olympic-marathon-polynomial-27.svg" width="80%" style=" ">
</object>
</div>
<div id="olympic-marathon-polynomial-27-magnify" class="magnify"
onclick="magnifyFigure(&#39;olympic-marathon-polynomial-27&#39;)">
<img class="img-button" src="{{ '/assets/images/Magnify_Large.svg' | relative_url }}" style="width:1.5ex">
</div>
<div id="olympic-marathon-polynomial-27-caption" class="caption-frame">
<p>Figure: Fit of a 26-degree polynomial to the Olympic marathon
data.</p>
</div>
</div>
<h2 id="the-bootstrap">The Bootstrap</h2>
<div style="text-align:right">
<span class="editsection-bracket" style="">[</span><span
class="editsection"
style=""><a href="https://github.com/lawrennd/snippets/edit/main/_ml/includes/the-bootstrap.md" target="_blank" onclick="ga('send', 'event', 'Edit Page', 'Edit', 'https://github.com/lawrennd/snippets/edit/main/_ml/includes/the-bootstrap.md', 13);">edit</a></span><span class="editsection-bracket" style="">]</span>
</div>
<p>Bootstrap sampling <span class="citation"
data-cites="Efron:bootstrap79">(Efron, 1979)</span> is an approach to
assessing the sensitivity of the model to different variations on a data
set. In an ideal world, we’d like to be able to look at different
realisations from the original data generating distribution <span
class="math inline">\(\mathbb{P}(y, \mathbf{ x})\)</span>, but this is
not available to us.</p>
<p>In bootstrap sampling, we take the sample we have, <span
class="math display">\[
\mathbf{ y}, \mathbf{X}\sim \mathbb{P}(y, \mathbf{ x})
\]</span> and resample from that data, rather than from the true
distribution. So, we have a new data set, <span
class="math inline">\(\hat{\mathbf{ y}}\)</span>, <span
class="math inline">\(\hat{\mathbf{X}}\)</span> which is sampled from
the original <em>with</em> replacement.</p>
<div class="sourceCode" id="cb52"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb52-1"><a href="#cb52-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span></code></pre></div>
<div class="sourceCode" id="cb53"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb53-1"><a href="#cb53-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> bootstrap(X):</span>
<span id="cb53-2"><a href="#cb53-2" aria-hidden="true" tabindex="-1"></a>    <span class="co">&quot;Return a bootstrap sample from a data set.&quot;</span></span>
<span id="cb53-3"><a href="#cb53-3" aria-hidden="true" tabindex="-1"></a>    n <span class="op">=</span> X.shape[<span class="dv">0</span>]</span>
<span id="cb53-4"><a href="#cb53-4" aria-hidden="true" tabindex="-1"></a>    ind <span class="op">=</span> np.random.choice(n, n, replace<span class="op">=</span><span class="va">True</span>) <span class="co"># Sample randomly with replacement.</span></span>
<span id="cb53-5"><a href="#cb53-5" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> X[ind, :]  </span></code></pre></div>
<h2 id="bootstrap-and-olympic-marathon-data">Bootstrap and Olympic
Marathon Data</h2>
<div style="text-align:right">
<span class="editsection-bracket" style="">[</span><span
class="editsection"
style=""><a href="https://github.com/lawrennd/snippets/edit/main/_ml/includes/olympic-marathon-bootstrap-polynomial.md" target="_blank" onclick="ga('send', 'event', 'Edit Page', 'Edit', 'https://github.com/lawrennd/snippets/edit/main/_ml/includes/olympic-marathon-bootstrap-polynomial.md', 13);">edit</a></span><span class="editsection-bracket" style="">]</span>
</div>
<p>First we define a function to bootstrap resample our dataset.</p>
<div class="sourceCode" id="cb54"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb54-1"><a href="#cb54-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span></code></pre></div>
<div class="sourceCode" id="cb55"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb55-1"><a href="#cb55-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> bootstrap(X, y):</span>
<span id="cb55-2"><a href="#cb55-2" aria-hidden="true" tabindex="-1"></a>    <span class="co">&quot;Return a bootstrap sample from a data set.&quot;</span></span>
<span id="cb55-3"><a href="#cb55-3" aria-hidden="true" tabindex="-1"></a>    n <span class="op">=</span> X.shape[<span class="dv">0</span>]</span>
<span id="cb55-4"><a href="#cb55-4" aria-hidden="true" tabindex="-1"></a>    ind <span class="op">=</span> np.random.choice(n, n, replace<span class="op">=</span><span class="va">True</span>) <span class="co"># Sample randomly with replacement.</span></span>
<span id="cb55-5"><a href="#cb55-5" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> X[ind, :], y[ind, :]</span></code></pre></div>
<div class="sourceCode" id="cb56"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb56-1"><a href="#cb56-1" aria-hidden="true" tabindex="-1"></a>num_bootstraps <span class="op">=</span> <span class="dv">10</span></span></code></pre></div>
<div class="sourceCode" id="cb57"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb57-1"><a href="#cb57-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> bootstrap_fit(Phi, y, size):</span>
<span id="cb57-2"><a href="#cb57-2" aria-hidden="true" tabindex="-1"></a>    W <span class="op">=</span> np.zeros((Phi.shape[<span class="dv">1</span>], size))</span>
<span id="cb57-3"><a href="#cb57-3" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(size):</span>
<span id="cb57-4"><a href="#cb57-4" aria-hidden="true" tabindex="-1"></a>        Phi_hat, y_hat <span class="op">=</span> bootstrap(Phi, y)</span>
<span id="cb57-5"><a href="#cb57-5" aria-hidden="true" tabindex="-1"></a>        W[:, i:i<span class="op">+</span><span class="dv">1</span>] <span class="op">=</span> basis_fit(Phi_hat, y_hat)</span>
<span id="cb57-6"><a href="#cb57-6" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> W</span></code></pre></div>
<h2 id="linear-fit-1">Linear Fit</h2>
<div class="sourceCode" id="cb58"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb58-1"><a href="#cb58-1" aria-hidden="true" tabindex="-1"></a>poly_args <span class="op">=</span> {<span class="st">&#39;num_basis&#39;</span>:<span class="dv">2</span>, <span class="co"># two basis functions (1 and x)</span></span>
<span id="cb58-2"><a href="#cb58-2" aria-hidden="true" tabindex="-1"></a>             <span class="st">&#39;data_limits&#39;</span>:xlim}</span>
<span id="cb58-3"><a href="#cb58-3" aria-hidden="true" tabindex="-1"></a>Phi <span class="op">=</span> polynomial(x, <span class="op">**</span>poly_args)</span>
<span id="cb58-4"><a href="#cb58-4" aria-hidden="true" tabindex="-1"></a>W_hat <span class="op">=</span> bootstrap_fit(Phi, y, num_bootstraps)</span></code></pre></div>
<p>Now we make some predictions for the fit.</p>
<div class="sourceCode" id="cb59"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb59-1"><a href="#cb59-1" aria-hidden="true" tabindex="-1"></a>x_pred <span class="op">=</span> np.linspace(xlim[<span class="dv">0</span>], xlim[<span class="dv">1</span>], <span class="dv">400</span>)[:, np.newaxis]</span>
<span id="cb59-2"><a href="#cb59-2" aria-hidden="true" tabindex="-1"></a>Phi_pred <span class="op">=</span> polynomial(x_pred, <span class="op">**</span>poly_args)</span>
<span id="cb59-3"><a href="#cb59-3" aria-hidden="true" tabindex="-1"></a>f_pred <span class="op">=</span> Phi_pred<span class="op">@</span>W_hat</span></code></pre></div>
<div class="figure">
<div id="olympic-marathon-bootstrap-polynomial-2-figure"
class="figure-frame">
<object class="svgplot " data="https://mlatcl.github.io/deepnn/./slides/diagrams//ml/olympic-marathon-bootstrap-polynomial-2.svg" width="80%" style=" ">
</object>
</div>
<div id="olympic-marathon-bootstrap-polynomial-2-magnify"
class="magnify"
onclick="magnifyFigure(&#39;olympic-marathon-bootstrap-polynomial-2&#39;)">
<img class="img-button" src="{{ '/assets/images/Magnify_Large.svg' | relative_url }}" style="width:1.5ex">
</div>
<div id="olympic-marathon-bootstrap-polynomial-2-caption"
class="caption-frame">
<p>Figure: Fit of a 1 degree polynomial (a linear model) to the olympic
marathon data.</p>
</div>
</div>
<h2 id="cubic-fit-1">Cubic Fit</h2>
<div class="sourceCode" id="cb60"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb60-1"><a href="#cb60-1" aria-hidden="true" tabindex="-1"></a>poly_args <span class="op">=</span> {<span class="st">&#39;num_basis&#39;</span>:<span class="dv">4</span>, <span class="co"># four basis: 1, x, x^2, x^3</span></span>
<span id="cb60-2"><a href="#cb60-2" aria-hidden="true" tabindex="-1"></a>             <span class="st">&#39;data_limits&#39;</span>:xlim}</span>
<span id="cb60-3"><a href="#cb60-3" aria-hidden="true" tabindex="-1"></a>Phi <span class="op">=</span> polynomial(x, <span class="op">**</span>poly_args)</span>
<span id="cb60-4"><a href="#cb60-4" aria-hidden="true" tabindex="-1"></a>W_hat <span class="op">=</span> bootstrap_fit(Phi, y, num_bootstraps)</span></code></pre></div>
<div class="sourceCode" id="cb61"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb61-1"><a href="#cb61-1" aria-hidden="true" tabindex="-1"></a>Phi_pred <span class="op">=</span> polynomial(x_pred, <span class="op">**</span>poly_args)</span>
<span id="cb61-2"><a href="#cb61-2" aria-hidden="true" tabindex="-1"></a>f_pred <span class="op">=</span> Phi_pred<span class="op">@</span>W_hat</span></code></pre></div>
<div class="figure">
<div id="olympic-marathon-bootstrap-polynomial-4-figure"
class="figure-frame">
<object class="svgplot " data="https://mlatcl.github.io/deepnn/./slides/diagrams//ml/olympic-marathon-bootstrap-polynomial-4.svg" width="80%" style=" ">
</object>
</div>
<div id="olympic-marathon-bootstrap-polynomial-4-magnify"
class="magnify"
onclick="magnifyFigure(&#39;olympic-marathon-bootstrap-polynomial-4&#39;)">
<img class="img-button" src="{{ '/assets/images/Magnify_Large.svg' | relative_url }}" style="width:1.5ex">
</div>
<div id="olympic-marathon-bootstrap-polynomial-4-caption"
class="caption-frame">
<p>Figure: Fit of a 3 degree polynomial (a cubic model) to the olympic
marathon data.</p>
</div>
</div>
<h2 id="th-degree-polynomial-fit-3">9th Degree Polynomial Fit</h2>
<p>Now we’ll try a 9th degree polynomial fit to the data.</p>
<div class="sourceCode" id="cb62"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb62-1"><a href="#cb62-1" aria-hidden="true" tabindex="-1"></a>poly_args <span class="op">=</span> {<span class="st">&#39;num_basis&#39;</span>:<span class="dv">10</span>, <span class="co"># basis up to x^9</span></span>
<span id="cb62-2"><a href="#cb62-2" aria-hidden="true" tabindex="-1"></a>             <span class="st">&#39;data_limits&#39;</span>:xlim}</span>
<span id="cb62-3"><a href="#cb62-3" aria-hidden="true" tabindex="-1"></a>Phi <span class="op">=</span> polynomial(x, <span class="op">**</span>poly_args)</span>
<span id="cb62-4"><a href="#cb62-4" aria-hidden="true" tabindex="-1"></a>W_hat <span class="op">=</span> bootstrap_fit(Phi, y, num_bootstraps)</span></code></pre></div>
<div class="sourceCode" id="cb63"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb63-1"><a href="#cb63-1" aria-hidden="true" tabindex="-1"></a>Phi_pred <span class="op">=</span> polynomial(x_pred, <span class="op">**</span>poly_args)</span>
<span id="cb63-2"><a href="#cb63-2" aria-hidden="true" tabindex="-1"></a>f_pred <span class="op">=</span> Phi_pred<span class="op">@</span>W_hat</span></code></pre></div>
<div class="figure">
<div id="olympic-marathon-bootstrap-polynomial-10-figure"
class="figure-frame">
<object class="svgplot " data="https://mlatcl.github.io/deepnn/./slides/diagrams//ml/olympic-marathon-bootstrap-polynomial-10.svg" width="80%" style=" ">
</object>
</div>
<div id="olympic-marathon-bootstrap-polynomial-10-magnify"
class="magnify"
onclick="magnifyFigure(&#39;olympic-marathon-bootstrap-polynomial-10&#39;)">
<img class="img-button" src="{{ '/assets/images/Magnify_Large.svg' | relative_url }}" style="width:1.5ex">
</div>
<div id="olympic-marathon-bootstrap-polynomial-10-caption"
class="caption-frame">
<p>Figure: Fit of a 9 degree polynomial to the olympic marathon
data.</p>
</div>
</div>
<h2 id="th-degree-polynomial-fit-4">16th Degree Polynomial Fit</h2>
<p>Now we’ll try a 16th degree polynomial fit to the data.</p>
<div class="sourceCode" id="cb64"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb64-1"><a href="#cb64-1" aria-hidden="true" tabindex="-1"></a>poly_args <span class="op">=</span> {<span class="st">&#39;num_basis&#39;</span>:<span class="dv">17</span>, <span class="co"># basis up to x^16</span></span>
<span id="cb64-2"><a href="#cb64-2" aria-hidden="true" tabindex="-1"></a>             <span class="st">&#39;data_limits&#39;</span>:xlim}</span>
<span id="cb64-3"><a href="#cb64-3" aria-hidden="true" tabindex="-1"></a>Phi <span class="op">=</span> polynomial(x, <span class="op">**</span>poly_args)</span>
<span id="cb64-4"><a href="#cb64-4" aria-hidden="true" tabindex="-1"></a>W_hat <span class="op">=</span> bootstrap_fit(Phi, y, num_bootstraps)</span></code></pre></div>
<div class="sourceCode" id="cb65"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb65-1"><a href="#cb65-1" aria-hidden="true" tabindex="-1"></a>Phi_pred <span class="op">=</span> polynomial(x_pred, <span class="op">**</span>poly_args)</span>
<span id="cb65-2"><a href="#cb65-2" aria-hidden="true" tabindex="-1"></a>f_pred <span class="op">=</span> Phi_pred<span class="op">@</span>W_hat</span></code></pre></div>
<div class="figure">
<div id="olympic-marathon-bootstrap-polynomial-17-figure"
class="figure-frame">
<object class="svgplot " data="https://mlatcl.github.io/deepnn/./slides/diagrams//ml/olympic-marathon-bootstrap-polynomial-17.svg" width="80%" style=" ">
</object>
</div>
<div id="olympic-marathon-bootstrap-polynomial-17-magnify"
class="magnify"
onclick="magnifyFigure(&#39;olympic-marathon-bootstrap-polynomial-17&#39;)">
<img class="img-button" src="{{ '/assets/images/Magnify_Large.svg' | relative_url }}" style="width:1.5ex">
</div>
<div id="olympic-marathon-bootstrap-polynomial-17-caption"
class="caption-frame">
<p>Figure: Fit of a 16 degree polynomial to the olympic marathon
data.</p>
</div>
</div>
<h2 id="bias-variance-decomposition">Bias Variance Decomposition</h2>
<div style="text-align:right">
<span class="editsection-bracket" style="">[</span><span
class="editsection"
style=""><a href="https://github.com/lawrennd/snippets/edit/main/_ml/includes/bias-variance-dilemma.md" target="_blank" onclick="ga('send', 'event', 'Edit Page', 'Edit', 'https://github.com/lawrennd/snippets/edit/main/_ml/includes/bias-variance-dilemma.md', 13);">edit</a></span><span class="editsection-bracket" style="">]</span>
</div>
<p>One of Breiman’s ideas for improving predictive performance is known
as bagging <span class="citation"
data-cites="Breiman:bagging96">(<strong>Breiman:bagging96?</strong>)</span>.
The idea is to train a number of models on the data such that they
overfit (high variance). Then average the predictions of these models.
The models are trained on different bootstrap samples <span
class="citation" data-cites="Efron:bootstrap79">(Efron, 1979)</span> and
their predictions are aggregated giving us the acronym, Bagging. By
combining decision trees with bagging, we recover random forests <span
class="citation" data-cites="Breiman-forests01">(Breiman,
2001)</span>.</p>
<p>Bias and variance can also be estimated through Efron’s bootstrap
<span class="citation" data-cites="Efron:bootstrap79">(Efron,
1979)</span>, and the traditional view has been that there’s a form of
Goldilocks effect, where the best predictions are given by the model
that is ‘just right’ for the amount of data available. Not to simple,
not too complex. The idea is that bias decreases with increasing model
complexity and variance increases with increasing model complexity.
Typically plots begin with the Mummy bear on the left (too much bias)
end with the Daddy bear on the right (too much variance) and show a dip
in the middle where the Baby bear (just) right finds themselves.</p>
<p>The Daddy bear is typically positioned at the point where the model
can exactly interpolate the data. For a generalized linear model <span
class="citation" data-cites="McCullagh:gen_linear89">(McCullagh and
Nelder, 1989)</span>, this is the point at which the number of
parameters is equal to the number of data<a href="#fn1"
class="footnote-ref" id="fnref1"
role="doc-noteref"><sup>1</sup></a>.</p>
<p>The bias-variance decomposition <span class="citation"
data-cites="Geman:biasvariance92">(<strong>Geman:biasvariance92?</strong>)</span>
considers the expected test error for different variations of the
<em>training data</em> sampled from, <span
class="math inline">\(\mathbb{P}(\mathbf{ x}, y)\)</span> <span
class="math display">\[\begin{align*}
R(\mathbf{ w}) = &amp; \int \left(y- f^*(\mathbf{ x})\right)^2
\mathbb{P}(y, \mathbf{ x}) \text{d}y\text{d}\mathbf{ x}\\
&amp; \triangleq \mathbb{E}\left[ \left(y- f^*(\mathbf{ x})\right)^2
\right].
\end{align*}\]</span></p>
<p>This can be decomposed into two parts, <span class="math display">\[
\begin{align*}
\mathbb{E}\left[ \left(y- f(\mathbf{ x})\right)^2 \right] = &amp;
\text{bias}\left[f^*(\mathbf{ x})\right]^2  +
\text{variance}\left[f^*(\mathbf{ x})\right]  +\sigma^2,
\end{align*}
\]</span> where the bias is given by <span class="math display">\[
  \text{bias}\left[f^*(\mathbf{ x})\right] =
\mathbb{E}\left[f^*(\mathbf{ x})\right] - f(\mathbf{ x})
\]</span> and it summarizes error that arises from the model’s inability
to represent the underlying complexity of the data. For example, if we
were to model the marathon pace of the winning runner from the Olympics
by computing the average pace across time, then that model would exhibit
<em>bias</em> error because the reality of Olympic marathon pace is it
is changing (typically getting faster).</p>
<p>The variance term is given by <span class="math display">\[
  \text{variance}\left[f^*(\mathbf{ x})\right] =
\mathbb{E}\left[\left(f^*(\mathbf{ x}) - \mathbb{E}\left[f^*(\mathbf{
x})\right]\right)^2\right].
  \]</span> The variance term is often described as arising from a model
that is too complex, but we must be careful with this idea. Is the model
really too complex relative to the real world that generates the data?
The real world is a complex place, and it is rare that we are
constructing mathematical models that are more complex than the world
around us. Rather, the ‘too complex’ refers to ability to estimate the
parameters of the model given the data we have. Slight variations in the
training set cause changes in prediction.</p>
<p>Models that exhibit high variance are sometimes said to ‘overfit’ the
data whereas models that exhibit high bias are sometimes described as
‘underfitting’ the data.</p>
<p>Also related on generalization error is the so called ‘no free lunch
theorem’, which refers to our inability to decide what a better learning
algorithm is without making assumptions about the data <span
class="citation" data-cites="Wolpert:lack96">(Wolpert, 1996)</span> (see
also <span class="citation" data-cites="Wolpert-supervised02">Wolpert
(2002)</span>).</p>
<h2 id="regularization">Regularization</h2>
<div style="text-align:right">
<span class="editsection-bracket" style="">[</span><span
class="editsection"
style=""><a href="https://github.com/lawrennd/snippets/edit/main/_ml/includes/linear-regression-regularisation.md" target="_blank" onclick="ga('send', 'event', 'Edit Page', 'Edit', 'https://github.com/lawrennd/snippets/edit/main/_ml/includes/linear-regression-regularisation.md', 13);">edit</a></span><span class="editsection-bracket" style="">]</span>
</div>
<p>The solution to the linear system is given by solving, <span
class="math display">\[
\boldsymbol{ \Phi}^\top\boldsymbol{ \Phi}\mathbf{ w}= \boldsymbol{
\Phi}^\top\mathbf{ y}
\]</span> for <span class="math inline">\(\mathbf{ w}\)</span>.</p>
<p>But if <span class="math inline">\(\boldsymbol{
\Phi}^\top\boldsymbol{ \Phi}\)</span> is not full rank, this system
cannot be solved. This is reflective of an <em>underdetermined
system</em> of equations. There are <em>infinite</em> solutions. This
happens when there are more basis functions than data points, in effect
the number of data we have is not enough to determine the
parameters.</p>
<p>Thinking about this in terms of the Hessian, if <span
class="math inline">\(\boldsymbol{ \Phi}^\top\boldsymbol{ \Phi}\)</span>
is not full rank, then we are no longer at a minimum. We are in a
trough, because <em>not full rank</em> implies that there are fewer
eigenvectors than dimensions, in effect for those dimensions where there
is no eigenvector, the objective function is ‘flat’. It is ambivalent to
changes in parameters. This implies there are infinite valid
solutions.</p>
<p>One solution to this problem is to regularize the system.</p>
<h2 id="coefficient-shrinkage">Coefficient Shrinkage</h2>
<p>Coefficient shrinkage is a technique where the parameters of the of
the model are ‘encouraged’ to be small. In practice this is normally
done by augmenting the objective function with a term that keeps the
parameters low, typically by penalizing a norm.</p>
<h2 id="tikhonov-regularization">Tikhonov Regularization</h2>
<p>In neural network models this approach is sometimes called ‘weight
decay’. At every gradient step we reduce the value of the weight a
little. This idea comes from an approach called Tikhonov regularization
<span class="citation" data-cites="Tikhonov:book77">(Tikhonov and
Arsenin, 1977)</span>, where the objective function is augmented by the
L2 norm of the weights, <span class="math display">\[
L(\mathbf{ w}) = (\mathbf{ y}- \mathbf{ f})^\top(\mathbf{ y}- \mathbf{
f}) + \alpha\left\Vert \mathbf{W} \right\Vert_2^2
\]</span> with some weighting <span class="math inline">\(\alpha
&gt;0\)</span>. This has the effect of changing the Hessian at the
minimum to <span class="math display">\[
\boldsymbol{ \Phi}^\top\boldsymbol{ \Phi}+ \alpha \mathbf{I}
\]</span> Which is always full rank. The minimal eigenvalues are now
given by <span class="math inline">\(\alpha\)</span>.</p>
<h2 id="lasso">Lasso</h2>
<p>Other techniques for regularization based on a norm of the parameters
include the Lasso <span class="citation"
data-cites="Tibshirani-lasso96">(Tibshirani, 1996)</span>, which is an
L1 norm of the parameters</p>
<h2 id="splines-functions-hilbert-kernels">Splines, Functions, Hilbert
Kernels</h2>
<p>Regularization of the parameters has the desired effect of making the
solution viable, but it can sometimes be difficult to interpret,
particularly if the parameters don’t have any inherent meaning (like in
a neural network). An alternative approach is to regularize the
function, <span class="math inline">\(\mathbf{ f}\)</span>, directly,
(see e.g., <span class="citation"
data-cites="Kimeldorf:correspondence70">Kimeldorf and Wahba
(1970)</span> and <span class="citation" data-cites="Wahba:book90">Wahba
(1990)</span>). This is the approach taken by <em>spline models</em>
which use energy-based regularization for <span
class="math inline">\(f(\cdot)\)</span> and also <em>kernel methods</em>
such as the support vector machine <span class="citation"
data-cites="Scholkopf:learning01">(Schölkopf and Smola,
2001)</span>.</p>
<h2 id="training-with-noise">Training with Noise</h2>
<div style="text-align:right">
<span class="editsection-bracket" style="">[</span><span
class="editsection"
style=""><a href="https://github.com/lawrennd/snippets/edit/main/_ml/includes/training-with-noise-tikhonov-regularisation.md" target="_blank" onclick="ga('send', 'event', 'Edit Page', 'Edit', 'https://github.com/lawrennd/snippets/edit/main/_ml/includes/training-with-noise-tikhonov-regularisation.md', 13);">edit</a></span><span class="editsection-bracket" style="">]</span>
</div>
<p>In practice, and across the last two waves of neural networks, other
techniques for regularization have been used which can be seen as
perturbing the neural network in some way. For example, in dropout <span
class="citation" data-cites="Srivastava-dropout14">(Srivastava et al.,
2014)</span>, different basis functions are eliminated from the gradient
computation at each gradient update.</p>
<p>Many of these perturbations have some form of regularizing effect.
The exact nature of the effect is not always easy to characterize, but
in some cases, we can assess how these manipulations effect the model.
For example, <span class="citation" data-cites="Bishop:noise95">Bishop
(1995)</span> analyzed training with ‘noisy inputs’ and showed
conditions under which it’s equivalent to Tikhonov regularization.</p>
<p>But in general, these approaches can have different interpretations
and they’ve also been related to ensemble learning (e.g. related to
<em>bagging</em> or Bayesian approaches).</p>
<!--include{_ml/includes/bayesian-interpretation-of-regularisation.md}-->
<h2 id="shallow-and-deep-learning">Shallow and Deep Learning</h2>
<p>So far, we have been talking about <em>linear models</em> or
<em>shallow learning</em> as we might think of it. Let’s pause for a
moment and consider a <em>fully connected</em> deep neural network model
to relate the two ideas.</p>
<h2 id="deep-neural-network">Deep Neural Network</h2>
<div style="text-align:right">
<span class="editsection-bracket" style="">[</span><span
class="editsection"
style=""><a href="https://github.com/lawrennd/snippets/edit/main/_deepnn/includes/deep-neural-network.md" target="_blank" onclick="ga('send', 'event', 'Edit Page', 'Edit', 'https://github.com/lawrennd/snippets/edit/main/_deepnn/includes/deep-neural-network.md', 13);">edit</a></span><span class="editsection-bracket" style="">]</span>
</div>
<div class="sourceCode" id="cb66"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb66-1"><a href="#cb66-1" aria-hidden="true" tabindex="-1"></a><span class="op">%</span>pip install daft</span></code></pre></div>
<div class="figure">
<div id="deep-neural-network-figure" class="figure-frame">
<object class="svgplot " data="https://mlatcl.github.io/deepnn/./slides/diagrams//deepgp/deep-nn2.svg" width="70%" style=" ">
</object>
</div>
<div id="deep-neural-network-magnify" class="magnify"
onclick="magnifyFigure(&#39;deep-neural-network&#39;)">
<img class="img-button" src="{{ '/assets/images/Magnify_Large.svg' | relative_url }}" style="width:1.5ex">
</div>
<div id="deep-neural-network-caption" class="caption-frame">
<p>Figure: A deep neural network. Input nodes are shown at the bottom.
Each hidden layer is the result of applying an affine transformation to
the previous layer and placing through an activation function.</p>
</div>
</div>
<p>Mathematically, each layer of a neural network is given through
computing the activation function, <span
class="math inline">\(\phi(\cdot)\)</span>, contingent on the previous
layer, or the inputs. In this way the activation functions, are composed
to generate more complex interactions than would be possible with any
single layer. <span class="math display">\[
\begin{align*}
    \mathbf{ h}_{1} &amp;= \phi\left(\mathbf{W}_1 \mathbf{ x}\right)\\
    \mathbf{ h}_{2} &amp;=  \phi\left(\mathbf{W}_2\mathbf{
h}_{1}\right)\\
    \mathbf{ h}_{3} &amp;= \phi\left(\mathbf{W}_3 \mathbf{
h}_{2}\right)\\
    f&amp;= \mathbf{ w}_4 ^\top\mathbf{ h}_{3}
\end{align*}
\]</span></p>
<p>Under our basis function perspective, we can see that our deep neural
network is mathematical composition of basis function models. Each layer
contains a separate basis function set, so <span class="math display">\[
f(\mathbf{ x}; \mathbf{W})  =  \mathbf{ w}_4 ^\top\phi\left(\mathbf{W}_3
\phi\left(\mathbf{W}_2\phi\left(\mathbf{W}_1 \mathbf{
x}\right)\right)\right).
\]</span></p>
<p>In this course there are two reasons for looking at the shallow
model. Firstly, it is easier to introduce the concepts of regularisation
in the linear model regime. Secondly, the matrix forms we see, e.g.,
expressions like <span class="math inline">\(\boldsymbol{ \Phi}^\top
\boldsymbol{ \Phi}\)</span>, appear in both models.</p>
<p>For deep learning, we can no longer optimize the parameters of the
model through solving a linear system<a href="#fn2" class="footnote-ref"
id="fnref2" role="doc-noteref"><sup>2</sup></a>. Instead, we need to
turn to non-linear optimization algorithms. For deep learning, that’s
typically stochastic gradient descent.</p>
<p>While it’s possible to compute the Hessian in a neural network, <span
class="citation" data-cites="Bishop-exact92">Bishop (1992)</span>, we
also find that it varies across the parameter space and will not
normally be positive definite. In practice, the number of parameters is
normally so large that storing the Hessian is impossible (it has
quadratic cost in the number of weights/parameters) due to memory
constraints.</p>
<p>This means that while the theory of minima in optimization is well
understood, empirical experiments with large neural networks are hard
and the lessons of small models do not all translate to the very large
systems.</p>
<p>We can stay within the framework of linear models but take a step
closer to neural network models by introducing functions that are
non-linear in the inputs, <span class="math inline">\(\mathbf{
x}\)</span>, known as <em>basis functions</em>.</p>
<h2 id="overparameterised-systems">Overparameterised Systems</h2>
<p>If we could examine the Hessian of a neural network at its minimum,
we can speculate about what we would find. In particular, we would find
that it would have very many low (or negative) eigenvalues in many
directions. This is indicative of the parameters being <em>badly
determined</em> because of the neural network model being heavily
<em>overparameterized</em>. So how does it generalize?</p>
<p>Simply put, there is not enough regularization encoded in the
objective function of the neural network models we are using to explain
the generalization performance. There must be something in the
algorithms we are using that causes these highly overparameterized
models to generalise well.</p>
<h2 id="double-descent">Double Descent</h2>
<div style="text-align:right">
<span class="editsection-bracket" style="">[</span><span
class="editsection"
style=""><a href="https://github.com/lawrennd/snippets/edit/main/_deepnn/includes/double-descent.md" target="_blank" onclick="ga('send', 'event', 'Edit Page', 'Edit', 'https://github.com/lawrennd/snippets/edit/main/_deepnn/includes/double-descent.md', 13);">edit</a></span><span class="editsection-bracket" style="">]</span>
</div>
<div class="figure">
<div id="double-descent-figure" class="figure-frame">
<div class="centered" style="">
<img class="" src="https://mlatcl.github.io/deepnn/./slides/diagrams//ml/double-descent.png" width="100%" height="auto" align="center" style="background:none; border:none; box-shadow:none; display:block; margin-left:auto; margin-right:auto;vertical-align:middle">
</div>
</div>
<div id="double-descent-magnify" class="magnify"
onclick="magnifyFigure(&#39;double-descent&#39;)">
<img class="img-button" src="{{ '/assets/images/Magnify_Large.svg' | relative_url }}" style="width:1.5ex">
</div>
<div id="double-descent-caption" class="caption-frame">
<p>Figure: <em>Left</em> traditional perspective on generalization.
There is a sweet spot of operation where the training error is still
non-zero. Overfitting occurs when the variance increases. <em>Right</em>
The double descent phenomenon, the modern models operate in an
interpolation regime where they reconstruct the training data fully but
are well regularized in their interpolations for test data. Figure from
<span class="citation" data-cites="Belkin:reconciling19">Belkin et al.
(2019)</span>.</p>
</div>
</div>
<p>But the modern empirical finding is that when we move beyond Daddy
bear, into the dark forest of the massively overparameterized model we
can achieve good generalization. Indeed, recent work is showing that
large language models are even <em>memorizing</em> data <span
class="citation" data-cites="Carlini-extracting20">(Carlini et al.,
2020)</span> like non-parametric models do.</p>
<p>As <span class="citation" data-cites="Zhang:understanding17">Zhang et
al. (2017)</span> starkly illustrated with their random labels
experiment, within the dark forest there are some terrible places, big
bad wolves of overfitting that will gobble up your model. But as
empirical evidence shows there is also a safe and hospitable Grandma’s
house where these highly overparameterized models are safely consumed.
Fundamentally, it must be about the route you take through the forest,
and the precautions you take to ensure the wolf doesn’t see where you’re
going and beat you to the door.</p>
<p>There are two implications of this empirical result. Firstly, that
there is a great deal of new theory that needs to be developed.
Secondly, that theory is now obliged to conflate two aspects to
modelling that we generally like to keep separate: the model and the
algorithm.</p>
<p>Classical statistical theory around predictive generalization
focusses specifically on the class of models that is being used for data
fitting. Historically, whether that theory follows a Fisher-aligned
estimation approach (see e.g., <span class="citation"
data-cites="Vapnik:book98">Vapnik (1998)</span>) or model-based Bayesian
approach (see e.g., <span class="citation"
data-cites="Ghahramani:probabilistic15">Ghahramani (2015)</span>),
neither is fully equipped to deal with these new circumstances because,
to continue our rather tortured analogy, these theories provide us with
a characterization of the <em>destination</em> of the algorithm, and
seek to ensure that we reach that destination. Modern machine learning
requires theories of the <em>journey</em> and what our route through the
forest should be.</p>
<p>Crucially, the destination is always associated with 100% accuracy on
the training set. An objective that is always achievable for the
overparameterized model.</p>
<p>Intuitively, it seems that a highly overparameterized model places
Grandma’s house on the edge of the dark forest. Making it easily and
quickly accessible to the algorithm. The larger the model, the more
exposed Grandma’s house becomes. Perhaps this is due to some form of
blessing of dimensionality brings Grandma’s house closer to the edge of
the forest in a high dimensional setting. Really, we should think of
Grandma’s house as a low dimensional manifold of destinations that are
safe. A path through the forest where the wolf of overfitting doesn’t
venture. In the GLM case, we know already that when the number of
parameters matches the number of data there is precisely one location in
parameter space where accuracy on the training data is 100%. Our
previous misunderstanding of generalization stemmed from the fact that
(seemingly) it is highly unlikely that this single point is a good place
to be from the perspective of generalization. Additionally, it is often
difficult to find. Finding the precise polynomial coefficients in a
least squares regression to exactly fit the basis to a small data set
such as the Olympic marathon data requires careful consideration of the
numerical properties and an orthogonalization of the design matrix <span
class="citation" data-cites="Lawson:least95">(Lawson and Hanson,
1995)</span>.</p>
<p>It seems that with a highly overparameterized model, these locations
become easier to find and they provide good generalization properties.
In machine learning this is known as the “double descent phenomenon”
(see e.g., <span class="citation"
data-cites="Belkin:reconciling19">Belkin et al. (2019)</span>).</p>
<p>See also this talk by Misha Belkin: <a
href="http://www.ipam.ucla.edu/abstract/?tid=15552&amp;pcode=GLWS4"
class="uri">http://www.ipam.ucla.edu/abstract/?tid=15552&amp;pcode=GLWS4</a>
and these related papers <a
href="https://www.pnas.org/content/116/32/15849.short"
class="uri">https://www.pnas.org/content/116/32/15849.short</a>, <a
href="https://www.pnas.org/content/117/20/10625"
class="uri">https://www.pnas.org/content/117/20/10625</a></p>
<h2 id="neural-tangent-kernel">Neural Tangent Kernel</h2>
<div style="text-align:right">
<span class="editsection-bracket" style="">[</span><span
class="editsection"
style=""><a href="https://github.com/lawrennd/snippets/edit/main/_deepnn/includes/neural-tangent-kernel.md" target="_blank" onclick="ga('send', 'event', 'Edit Page', 'Edit', 'https://github.com/lawrennd/snippets/edit/main/_deepnn/includes/neural-tangent-kernel.md', 13);">edit</a></span><span class="editsection-bracket" style="">]</span>
</div>
<p>Another approach to analysis exploits the fact that optimization is
occurring in a very high dimensional parameter space. By considering
initializations that involve small random weights (known as the NTK
initialization) and noting that small updates in the learning mean that
the model doesn’t move far from this initialization <span
class="citation" data-cites="Jacot-ntk18">(Jacot et al.,
2018)</span>.</p>
<p>For very wide neural networks, when these conditions are fulfilled,
the network can be approximately represented by a <em>kernel</em> known
as the neural tangent kernel. A kernel is a regularizer that operates in
<em>function space</em> rather than <em>feature space</em>.</p>
<h2 id="regularization-in-optimization">Regularization in
Optimization</h2>
<div style="text-align:right">
<span class="editsection-bracket" style="">[</span><span
class="editsection"
style=""><a href="https://github.com/lawrennd/snippets/edit/main/_deepnn/includes/regularisation-in-optimisation.md" target="_blank" onclick="ga('send', 'event', 'Edit Page', 'Edit', 'https://github.com/lawrennd/snippets/edit/main/_deepnn/includes/regularisation-in-optimisation.md', 13);">edit</a></span><span class="editsection-bracket" style="">]</span>
</div>
<p>Another interesting theoretical direction is to study the path that
neural network algorithms take when finding the optima. For certain
simple linear systems, you can analytically study the ‘gradient
flow’.</p>
<p>Neural networks are normally trained by (stochastic) gradient
descent. This is a discrete optimization algorithm where at each point,
a step in the direction of the (approximate) gradient is taken.</p>
<p>Gradient flow replaces this discrete update with a differential
equation, where the step at any point is an exact gradient update. As a
result, the path of the optimization can be studied as a
<em>differential equation</em>.</p>
<p>By making assumptions about the initialization, the optimum that
gradient flow will find can be characterised. For a highly
overparameterized linear model, <span class="citation"
data-cites="Gunasekar-implicit2017">Gunasekar et al. (2017)</span> show
in matrix factorization, that for particular initializations, the
optimum will be a <em>global</em> optimum of the objective that
minimizes the L2-norm.</p>
<p>By reparameterizing the linear model so that each <span
class="math inline">\(w_i = u_i^2 - v_i^2\)</span> and optimising in the
space defined by <span class="math inline">\(\mathbf{u}\)</span> and
<span class="math inline">\(\mathbf{v}\)</span> <span class="citation"
data-cites="Woodworth-kernel20">Woodworth et al. (2020)</span> show that
the L1 norm is found.</p>
<p>Other papers have looked at <em>deep linear models</em> <span
class="citation" data-cites="Arora-convergence19">(Arora et al.,
2019)</span> where <span class="math display">\[
f(\mathbf{ x}; \mathbf{W}) = \mathbf{W}_4 \mathbf{W}_3 \mathbf{W}_2
\mathbf{W}_1 \mathbf{ x}.
\]</span> In these models, a gradient flow analysis shows that the model
finds solutions where the linear mapping, <span class="math display">\[
\mathbf{W}= \mathbf{W}_4 \mathbf{W}_3 \mathbf{W}_2 \mathbf{W}_1
\]</span> is very low rank. This is highly suggestive of another type of
regularization that could be occurring in deep neural networks. Low rank
parameter matrices mean that the effective capacity of the neural
network is reduced. Indeed, empirical observations of the rank of deep
nets trained on data suggest that they may be finding such
solutions.</p>
<h2 id="thanks">Thanks!</h2>
<p>For more information on these subjects and more you might want to
check the following resources.</p>
<ul>
<li>twitter: <a href="https://twitter.com/lawrennd">@lawrennd</a></li>
<li>podcast: <a href="http://thetalkingmachines.com">The Talking
Machines</a></li>
<li>newspaper: <a
href="http://www.theguardian.com/profile/neil-lawrence">Guardian Profile
Page</a></li>
<li>blog: <a
href="http://inverseprobability.com/blog.html">http://inverseprobability.com</a></li>
</ul>
<h1 id="references">References</h1>
<p>bootstrap</p>
<p>David Hogg’s lecture <a
href="https://speakerdeck.com/dwhgg/linear-regression-with-huge-numbers-of-parameters"
class="uri">https://speakerdeck.com/dwhgg/linear-regression-with-huge-numbers-of-parameters</a></p>
<p>The Deep Bootstrap <a
href="https://twitter.com/PreetumNakkiran/status/1318007088321335297?s=20"
class="uri">https://twitter.com/PreetumNakkiran/status/1318007088321335297?s=20</a></p>
<p>Aki Vehtari on Leave One Out Uncertainty: <a
href="https://arxiv.org/abs/2008.10296"
class="uri">https://arxiv.org/abs/2008.10296</a> (check for his
references).</p>
<div id="refs" class="references csl-bib-body hanging-indent"
role="list">
<div id="ref-Arora-convergence19" class="csl-entry" role="listitem">
Arora, S., Cohen, N., Golowich, N., Hu, W., 2019. <a
href="https://openreview.net/forum?id=SkMQg3C5K7">A convergence analysis
of gradient descent for deep linear neural networks</a>, in:
International Conference on Learning Representations.
</div>
<div id="ref-Belkin:reconciling19" class="csl-entry" role="listitem">
Belkin, M., Hsu, D., Ma, S., Soumik Mandal, and, 2019. Reconciling
modern machine-learning practice and the classical bias-variance
trade-off. Proc. Natl. Acad. Sci. USA 116, 15849–15854.
</div>
<div id="ref-Bishop:noise95" class="csl-entry" role="listitem">
Bishop, C.M., 1995. Training with noise is equivalent to
<span>T</span>ikhonov regularization. Neural Computation 7, 108–116. <a
href="https://doi.org/10.1162/neco.1995.7.1.108">https://doi.org/10.1162/neco.1995.7.1.108</a>
</div>
<div id="ref-Bishop-exact92" class="csl-entry" role="listitem">
Bishop, C.M., 1992. Exact calculation of the hessian matrix for the
multilayer perceptron. Neural Computation 4, 494–501. <a
href="https://doi.org/10.1162/neco.1992.4.4.494">https://doi.org/10.1162/neco.1992.4.4.494</a>
</div>
<div id="ref-Breiman-forests01" class="csl-entry" role="listitem">
Breiman, L., 2001. Random forests. Mach. Learn. 45, 5–32. <a
href="https://doi.org/10.1023/A:1010933404324">https://doi.org/10.1023/A:1010933404324</a>
</div>
<div id="ref-Carlini-extracting20" class="csl-entry" role="listitem">
Carlini, N., Tramèr, F., Wallace, E., Jagielski, M., Herbert-Voss, A.,
Lee, K., Roberts, A., Brown, T., Song, D., Erlingsson, U., Oprea, A.,
Raffel, C., 2020. <a href="https://arxiv.org/abs/2012.07805">Extracting
training data from large language models</a>.
</div>
<div id="ref-Efron:bootstrap79" class="csl-entry" role="listitem">
Efron, B., 1979. Bootstrap methods: Another look at the jackkife. Annals
of Statistics 7, 1–26.
</div>
<div id="ref-Ghahramani:probabilistic15" class="csl-entry"
role="listitem">
Ghahramani, Z., 2015. Probabilistic machine learning and artificial
intelligence. Nature 452–459.
</div>
<div id="ref-Gunasekar-implicit2017" class="csl-entry" role="listitem">
Gunasekar, S., Woodworth, B., Bhojanapalli, S., Neyshabur, B., Srebro,
N., 2017. <a href="https://arxiv.org/abs/1705.09280">Implicit
regularization in matrix factorization</a>.
</div>
<div id="ref-Jacot-ntk18" class="csl-entry" role="listitem">
Jacot, A., Gabriel, F., Hongler, C., 2018. <a
href="https://papers.nips.cc/paper/2018/hash/5a4be1fa34e62bb8a6ec6b91d2462f5a-Abstract.html">Neural
tangent kernel: Convergence and generalization in neural networks</a>,
in: Bengio, S., Wallach, H., Larochelle, H., Grauman, K., Cesa-Bianchi,
N., Garnett, R. (Eds.), Advances in Neural Information Processing
Systems. Curran Associates, Inc., pp. 8571–8580.
</div>
<div id="ref-Kimeldorf:correspondence70" class="csl-entry"
role="listitem">
Kimeldorf, G.S., Wahba, G., 1970. <a
href="https://www.jstor.org/stable/2239347">A correspondence between
<span>B</span>ayesian estimation of stochastic processes and smoothing
by splines</a>. Annals of Mathematical Statistics 41, 495–502.
</div>
<div id="ref-Lawson:least95" class="csl-entry" role="listitem">
Lawson, C.L., Hanson, R.J., 1995. Solving least squares problems. SIAM.
<a
href="https://doi.org/10.1137/1.9781611971217">https://doi.org/10.1137/1.9781611971217</a>
</div>
<div id="ref-McCullagh:gen_linear89" class="csl-entry" role="listitem">
McCullagh, P., Nelder, J.A., 1989. Generalized linear models, 2nd ed.
Chapman; Hall.
</div>
<div id="ref-Scholkopf:learning01" class="csl-entry" role="listitem">
Schölkopf, B., Smola, A.J., 2001. Learning with kernels. mit, Cambridge,
MA.
</div>
<div id="ref-Srivastava-dropout14" class="csl-entry" role="listitem">
Srivastava, N., Hinton, G., Krizhevsky, A., Sutskever, I.,
Salakhutdinov, R., 2014. <a
href="http://jmlr.org/papers/v15/srivastava14a.html">Dropout: A simple
way to prevent neural networks from overfitting</a>. Journal of Machine
Learning Research 15, 1929–1958.
</div>
<div id="ref-Nigeria-nmis14" class="csl-entry" role="listitem">
The Office of the Senior Special Assistant to the President on the
Millennium Development Goals (OSSAP-MDGs), Columbia University, 2014.
Nigeria <span>NMIS</span> facility database.
</div>
<div id="ref-Tibshirani-lasso96" class="csl-entry" role="listitem">
Tibshirani, R., 1996. <a
href="http://www.jstor.org/stable/2346178">Regression shrinkage and
selection via the lasso</a>. Journal of the Royal Statistical Society.
Series B (Methodological) 58, 267–288.
</div>
<div id="ref-Tikhonov:book77" class="csl-entry" role="listitem">
Tikhonov, A.N., Arsenin, V.Y., 1977. Solutions of ill-posed problems. V.
H. Winston, Washington, DC.
</div>
<div id="ref-Vapnik:book98" class="csl-entry" role="listitem">
Vapnik, V.N., 1998. Statistical learning theory. wiley, New York.
</div>
<div id="ref-Wahba:book90" class="csl-entry" role="listitem">
Wahba, G., 1990. Spline models for observational data, First. ed. SIAM.
<a
href="https://doi.org/10.1137/1.9781611970128">https://doi.org/10.1137/1.9781611970128</a>
</div>
<div id="ref-Wolpert-supervised02" class="csl-entry" role="listitem">
Wolpert, D.H., 2002. The supervised learning no-free-lunch theorems, in:
Roy, R., Köppen, M., Ovaska, S., Furuhashi, T., Hoffmann, F. (Eds.),
Soft Computing and Industry. Springer, London, pp. 25–. <a
href="https://doi.org/10.1007/978-1-4471-0123-9_3">https://doi.org/10.1007/978-1-4471-0123-9_3</a>
</div>
<div id="ref-Wolpert:lack96" class="csl-entry" role="listitem">
Wolpert, D.H., 1996. The lack of a priori distinctions between learning
algorithms. Neural Computation 8. <a
href="https://doi.org/10.1162/neco.1996.8.7.1341">https://doi.org/10.1162/neco.1996.8.7.1341</a>
</div>
<div id="ref-Woodworth-kernel20" class="csl-entry" role="listitem">
Woodworth, B., Gunasekar, S., Lee, J.D., Moroshko, E., Savarese, P.,
Golan, I., Soudry, D., Srebro, N., 2020. <a
href="https://arxiv.org/abs/2002.09277">Kernel and rich regimes in
overparametrized models</a>.
</div>
<div id="ref-Zhang:understanding17" class="csl-entry" role="listitem">
Zhang, C., Bengio, S., Hardt, M., Recht, B., Vinyals, O., 2017.
Understanding deep learning requires rethinking generalization, in:
https://openreview.net/forum?id=Sy8gdB9xx (Ed.), International
Conference on Learning Representations.
</div>
</div>
<aside id="footnotes" class="footnotes footnotes-end-of-document"
role="doc-endnotes">
<hr />
<ol>
<li id="fn1"><p>Assuming we are ignoring parameters in the link function
and the distribution function.<a href="#fnref1" class="footnote-back"
role="doc-backlink">↩︎</a></p></li>
<li id="fn2"><p>Apart from the last layer of parmeters in models with
quadratic loss functions.<a href="#fnref2" class="footnote-back"
role="doc-backlink">↩︎</a></p></li>
</ol>
</aside>

