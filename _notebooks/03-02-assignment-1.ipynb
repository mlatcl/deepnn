{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GWCH9PwSq5Ah"
   },
   "source": [
    "# Assignment 1\n",
    "\n",
    "### Ferenc Huszar, Nic Lane and Neil Lawrence\n",
    "\n",
    "### 9th February 2021\n",
    "\n",
    "Welcome to the first assignment for the Deep Neural Networks module. This assignment will test the principles that we have looked at in the first five lectures. \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uUUZp6_0EgYb"
   },
   "source": [
    "## Double Descent\n",
    "\n",
    "This assignment has six questions for a total of 100 marks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wD1jV2ZNrTdl"
   },
   "source": [
    "### Background\n",
    "\n",
    "In this assignment we are going to start by exploring a recent paper on the 'double descent phenomenon'. \n",
    "\n",
    "In [Belkin et al](https://www.pnas.org/content/116/32/15849.short) the authors explore double descent, firstly using *random Fourier features* as a basis function. We quote from their paper below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6LxMzbUfQtkS"
   },
   "source": [
    "> We first consider a popular class of nonlinear parametric models  called random Fourier features (RFF) (13), which can be viewed as a class of 2-layer neural networks with fixed weights in the first layer. The RFF model family $\\mathcal{H}_N$ with $N$ (complex-valued) parameters consists of functions\n",
    "$h : \\mathbb{R}^d \\rightarrow \\mathbb{C}$ of the form\n",
    "$$\n",
    "h(x) =\\sum_{k=1}^N a_k \\phi(x ; v_k ) \\quad \\text{where} \\quad \\phi(x ; v_k) := e^{\\sqrt{-1}\\langle v_k, x\\rangle},\n",
    "$$ \n",
    "and the vectors $v_1, \\dots , v_N$ are sampled independently from the\n",
    "standard normal distribution in $\\mathbb{R}^d$.\n",
    "\n",
    "From Belkin et al. pg 2 of paper, section titled **Random Fourier Features**. For more on random Fourier features see [Rahimi and Recht](https://people.eecs.berkeley.edu/~brecht/papers/07.rah.rec.nips.pdf).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "P0nwCs9MIlgS"
   },
   "source": [
    "**Note** that the notation that Belkin et al use, $\\langle v_k, x \\rangle$ is just the notation for an inner product between the vector $v_k$ and the vector $x$. We'll use the notation $v_k^\\top x$ to denote this below.\n",
    "\n",
    "Following the papers description, our first task is to create a function for computing the basis. The paper proposes using a basis which is\n",
    "$$\n",
    "\\exp(i z) = \\cos(z) + i \\sin(z)\n",
    "$$\n",
    "to form the *random Fourier features*. This leads to complex valued weights, $\\{a_k\\}_{i=1}^N$. So rather than doing that, we set our design matrix up in the following way:\n",
    "$$\n",
    "\\boldsymbol{\\Phi} = \\begin{bmatrix} \\cos(v_1^\\top x_1) & \\sin(v_1^\\top x_1) & cos(v_2^\\top x_1) & \\sin(v_2^\\top x_1) & \\cdots & \\cos(v_N^\\top x_1) & \\sin(v_N^\\top x_1) \\\\\n",
    "\\cos(v_1^\\top x_2) & \\sin(v_1^\\top x_2) & cos(v_2^\\top x_2) & \\sin(v_2^\\top x_2) & \\cdots & \\cos(v_N^\\top x_2) & \\sin(v_N^\\top x_2) \\\\\n",
    "\\vdots & \\vdots & \\vdots & \\vdots & \\ddots & \\vdots & \\vdots \\\\\n",
    "\\cos(v_1^\\top x_n) & \\sin(v_1^\\top x_n) & cos(v_2^\\top x_n) & \\sin(v_2^\\top x_n) & \\cdots & \\cos(v_N^\\top x_n) & \\sin(v_N^\\top x_n)\\end{bmatrix}\n",
    "$$\n",
    "Giving us a matrix $\\boldsymbol{\\Phi} \\in \\mathbb{R}^{n \\times 2N}$ where we are following Belkin et al's notation with $n$ number of data points and $N$ number of random Fourier features. Note that because each feature (in our representation) contains a sine and a cosine, the total number of basis functions is $2N$. \n",
    "\n",
    "The frequencies of the features, as Belkin et al explain, should be sampled from a normal density. In our notation below we collect these features in a matrix `V`, represented here as a two dimensional `numpy` array.  \n",
    "\n",
    "If the data is stored in a That leads to the following code for implementing the random Fourier features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Wn-lsxRj3gkd"
   },
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QAFfuMNjSX4g"
   },
   "outputs": [],
   "source": [
    "def random_fourier_basis(X, V):\n",
    "  arg = X@V\n",
    "  Phi = np.zeros((X.shape[0], 2*V.shape[1]))\n",
    "  Phi[:, 0::2] = np.cos(arg) \n",
    "  Phi[:, 1::2] = np.sin(arg)\n",
    "  return Phi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bUW21S6gIjdP"
   },
   "source": [
    "Given our design matrix, $\\boldsymbol{\\Phi}$ we can compute the output of the functions, which Belkin et al denote $h(x)$ for each of the points using matrix multiplication. \n",
    "$$\n",
    "\\mathbf{h} = \\boldsymbol{\\Phi} \\mathbf{a}\n",
    "$$\n",
    "where $\\mathbf{a} \\in \\mathbb{R}^N$ is a vector containing the elements $a_k$.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YZ2tQLsXg_Ll"
   },
   "outputs": [],
   "source": [
    "def predict(X_test, a, V):\n",
    "  Phi_test = random_fourier_basis(X_test, V)\n",
    "  return Phi_test@a"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "o3xhWVrJSbIC"
   },
   "source": [
    "> Our learning procedure using $\\mathcal{H}_N$ is as follows. Given data $(x_1, y_1), \\dots , (x_n , y_n )$ from $\\mathbb{R}^{d} \\times \\mathbb{R}$, we find the predictor $h_{n,N} \\in \\mathcal{H}_N$ via ERM with squared loss. That is, we minimize the empirical risk objective $\\frac{1}{n} \\sum_{i=1}^n (h(x_i) − y_i)^2$\n",
    "over all functions $h \\in \\mathcal{H}_N$. When the minimizer is not unique (as is always the case when $N > n$), we choose the minimizer whose coefficients $(a_1, \\dots , a_N )$ have the minimum $\\ell_2$ norm. This choice of norm is intended as an approximation to the RKHS norm $\\|h\\|_{\\mathcal{H}_\\infty}$, which is generally difficult to compute for arbitrary functions in $\\mathcal{H}_N$ . For problems with multiple outputs (e.g., multiclass classification), we use functions with vector-valued outputs and the sum of the squared losses for each output.\n",
    "\n",
    "We already reviewed empirical risk minimization in the second lecture of week one. Here, Belkin et al are using the *squared loss* even for a classification task. This is probably not something you'd do in practice, but for our analysis below it will serve as it makes gradients easier to compute, and the optimum easy to solve analytically. \n",
    "\n",
    "We can implement the average squared error they described as follows. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Bv5R13g_zl8j"
   },
   "outputs": [],
   "source": [
    "def error(X, y, a, V):\n",
    "  y_pred = predict(X, a, V)\n",
    "  return np.mean((y_pred - y)**2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pAIrDpN1RyGT"
   },
   "source": [
    "And now we can implement the fixed point update equation we reviewed in [Lecture 2 of Week 1](https://mlatcl.github.io/deepnn/lectures/01-02-generalisation-and-neural-networks.html) for fitting the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-CTCGHlpX3r-"
   },
   "outputs": [],
   "source": [
    "def fit(X_train, y_train, N, lambd=0.01):\n",
    "  d = X_train.shape[1]\n",
    "  V = np.random.normal(0.0, 1, size=(d, N))\n",
    "  Phi_train = random_fourier_basis(X_train, V)\n",
    "  if Phi_train.shape[1]>X_train.shape[0]:\n",
    "    # N > n, use L2 regularizer\n",
    "    a = np.linalg.solve(Phi_train.T@Phi_train + lambd*np.eye(N*2), Phi_train.T@y_train)\n",
    "  else:\n",
    "    a = np.linalg.solve(Phi_train.T@Phi_train, Phi_train.T@y_train)\n",
    "  return a, V"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nsIuObJ8SCgY"
   },
   "source": [
    "Here we have included `lambd` to represent a regularisation parameter, so that we can minimize the $\\ell_2$ norm as Belkin et al suggest when the number of parameters in the model (which is $2N$ for this random Fourier basis) is greater than the number of data, $n$. This is called [Tikhonov regularization](https://en.wikipedia.org/wiki/Tikhonov_regularization)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5tQ-ou3rSX1U"
   },
   "source": [
    "### Loading a Data Set\n",
    "\n",
    "For their paper, Belkin et al use the MNIST digits data set. To make things run a little quicker for us, we'll use a digits data set from scikit learn. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "w9hwekPaUrF4"
   },
   "outputs": [],
   "source": [
    "import sklearn\n",
    "from sklearn import datasets\n",
    "\n",
    "digits = datasets.load_digits()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UdX_VXH0SsjH"
   },
   "source": [
    "The original data has 10 digits. We'll modify the task form classifyign individual digits to separating curvy digits from those with angles. Our task will be to separate `0`, `3`, `6`, `8`, `9` from `1`, `2`, `4`, `5`, `7`. This leads to an approximate balance between positive and negative examples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vNQuPFV2Uv4A"
   },
   "outputs": [],
   "source": [
    "X = digits['data']\n",
    "y = ((digits['target'] == 0)\n",
    "     | (digits['target'] == 3)\n",
    "     | (digits['target'] == 6)\n",
    "     | (digits['target'] == 8)\n",
    "     | (digits['target'] == 9))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2ZjPSU9vTbOi"
   },
   "source": [
    "We will normalise the input data. Normalization is often a key part of getting machine learning algorithms working well. In many cases, you would normalize the columns of your data independently. But here, because it's an image, my instinct is to apply a single scaling. Because the pixels live naturally on the same scale originally. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hc3onmgqTMpk"
   },
   "outputs": [],
   "source": [
    "X /= X.std()\n",
    "X /= np.sqrt(64)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eUZ_ZT4dU--k"
   },
   "source": [
    "I've subsequently also scaled by the square root of the dimensionality. This ensures that on average, the length of the vector $x$ is 1/64, or in other words, $1/d$ where $d$ is the dimensionality of the input image (which is $8\\times 8$)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FYH30fqITNdQ"
   },
   "source": [
    "Similarly, it's quite common to use labels of $-1$ or $1$ for targets when using a squared error. This works well because it's symmetric about zero. But an encoding of $0$ vs $1$ would also likely work. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "knw9kyfZTPh6"
   },
   "outputs": [],
   "source": [
    "y = y*2 - 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Takd5nVXWMYP"
   },
   "source": [
    "We can use scikit learn's built in facility for forming a training and test set from the model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GAF05ymaZv5B"
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.5, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "L35wXzhTWSKj"
   },
   "source": [
    "And now we can use our code to fit our model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rrR20K2kg5II"
   },
   "outputs": [],
   "source": [
    "a, V = fit(X_train, y_train, 200)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mUsjh7lxWV0u"
   },
   "source": [
    "And check the error. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9H8Xs9MLg48i"
   },
   "outputs": [],
   "source": [
    "e = error(X_test, y_test, a, V)\n",
    "print('Error: {}'.format(e))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "x_XccddYWffA"
   },
   "source": [
    "### Question 1\n",
    "\n",
    "Fit models using different values for $N$ between 1 and 2000. \n",
    "\n",
    "Produce a plot that has an $x$-axis given by the number of parameters in the model (remember that's $2N$) divided by the number of *training* data. On the $y$-axis plot the loss computed on the *test* data. This serves as an estimate of the true risk.\n",
    "\n",
    "Choose appropriate intervals for plotting so that the full form of the curve is seen. Beware that the fit will be slow as $N\\rightarrow 2000$. \n",
    "\n",
    "You can use `matplotlib` or your preferred plotting library to create the plot.\n",
    "\n",
    "Describe what you see in the plot with reference to both *classical* generalisation theory and the generalisation performance of modern neural network models.\n",
    "\n",
    "*15 marks*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JrrsEKs20GRK"
   },
   "outputs": [],
   "source": [
    "## Place your code for your answer to question 1 in this box. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TkR4NjA0Xs8e"
   },
   "source": [
    "#### Answer 1 text\n",
    "\n",
    "Place your description of the plot in this box.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "W0GMsqKJX7Rl"
   },
   "source": [
    "## Gradient descent in pytorch\n",
    "\n",
    " Having seen the model above written in `numpy` we are now going to explore the same model in `pytorch`. First we rewrite our basis functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5OhBZqd5ZYFu"
   },
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "aKhcFylyj3jQ"
   },
   "outputs": [],
   "source": [
    "def random_fourier_basis(X, V):\n",
    "  arg = X@V\n",
    "  Phi = torch.zeros((X.shape[0], 2*V.shape[1]))\n",
    "  Phi[:, 0::2] = torch.cos(arg) \n",
    "  Phi[:, 1::2] = torch.sin(arg)\n",
    "  return Phi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wA1cxs61ZlYM"
   },
   "outputs": [],
   "source": [
    "def predict(X_test, a, V):\n",
    "  Phi_test = random_fourier_basis(X_test, V)\n",
    "  return Phi_test@a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rr5GIMqd2nCB"
   },
   "outputs": [],
   "source": [
    "def error(X, y, a, V):\n",
    "  y_pred = predict(X, a, V)\n",
    "  return torch.mean((y_pred - y)**2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "IVPFccr-Zqn9"
   },
   "outputs": [],
   "source": [
    "# For completeness we also include the function for 'direct fit' in pytorch. You \n",
    "# don't need it for the answers below but you can use it to check convergence\n",
    "# if you like.\n",
    "\n",
    "def fit(X_train, y_train, N, lambd=0.01):\n",
    "  d = X_train.shape[1]\n",
    "  V = torch.normal(0.0, 1, size=(d, N))\n",
    "  Phi_train = fourier_basis(X_train, V)\n",
    "  if Phi_train.shape[1]>X_train.shape[0]:\n",
    "    # N > n, use L2 regularizer\n",
    "    a, LU = torch.solve(Phi_train.T@y_train, Phi_train.T@Phi_train + lambd*eye(N*2))\n",
    "  else:\n",
    "    a, LU = torch.solve(Phi_train.T@y_train, Phi_train.T@Phi_train)\n",
    "  return a, V"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "amYeiZxcOAWs"
   },
   "source": [
    "Because we will fit these models with gradient descent now, we have to randomly initialize the parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zlOOGlLma5qe"
   },
   "outputs": [],
   "source": [
    "a = torch.normal(0, 0.001, size=(2*N, 1))\n",
    "V = torch.normal(0.0, 1, size=(X.shape[1], N))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gvZp8dHlOX0a"
   },
   "source": [
    "We also have to tell `torch` that `a` is a variable that we would like to later calculate gradients with respect to, using the `.requires_grad` attribute. The result of this is that wherever we use the variable `a` in subsequent Maths operations, a computational graph with all those operations is going to be built in the background. This computational graph is what allows the `autograd` module of `torch` to later calculate gradients using reverse-mode automatic differentiation, a.k.a. backpropagation.\n",
    "\n",
    "As in this example we assume `V` to be fixed to randomly selected values, and not learned, so we don't need to do the same for that variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "D9os1n3IOYs3"
   },
   "outputs": [],
   "source": [
    "a.requires_grad=True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oFcB1MWwd1Yh"
   },
   "source": [
    "*We* need to convert our training and test data from `numpy` arrays into `pytorch` tensors. Note that by default pytoch works with single precision, so we also convert these data arrays (which are `double` in `numpy`) to `float`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "a50kghjw3oR3"
   },
   "outputs": [],
   "source": [
    "Xt_train = torch.from_numpy(X_train).float()\n",
    "Xt_test = torch.from_numpy(X_test).float()\n",
    "yt_train = torch.from_numpy(y_train[:, np.newaxis]).float()\n",
    "yt_test = torch.from_numpy(y_test[:, np.newaxis]).float()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "inhVICILeJZA"
   },
   "source": [
    "The magic of automatic differentiation allows us to compute the gradients in `pytorch` by requesting `.backward()`. This function can be called on any `pytorch` tensor that has a computational graph attached. Backpropagation will walk bacjwards on this computational graph and save the relevant gradients in the `.grad` attribute of each `Tensor` whose `requires_grad` attribute is set to `True.`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xIc4_5bQbQMc"
   },
   "outputs": [],
   "source": [
    "error(Xt_train, yt_train, a, V).backward()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tGP-jxJLeVEw"
   },
   "source": [
    "And now we can simply look at the gradient from pytorch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "X05WunK0b6LV"
   },
   "outputs": [],
   "source": [
    "a.grad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NEEQkbcNebxG"
   },
   "source": [
    "### Question 2\n",
    "\n",
    "Analytically derive the gradient for the objective function with respect to parameters $\\mathbf{a}$. Give your answer in the box below.\n",
    "\n",
    "Then implement the answer in code and compare the gradient to the values given by `autograd` in `pytorch` with a scatter plot.\n",
    "\n",
    "*10 marks*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Am0Ux-HCR89c"
   },
   "source": [
    "#### Answer 2\n",
    "\n",
    "Please add your answer by completing the equation below.\n",
    "\n",
    "The gradient of the loss with respect to $\\mathbf{a}$ is:\n",
    "$$\n",
    "\\frac{\\text{d}}{\\text{d} \\mathbf{a}} \\frac{1}{n} (\\boldsymbol{\\Phi} \\mathbf{a} − \\mathbf{y})^\\top (\\boldsymbol{\\Phi} \\mathbf{a} − \\mathbf{y}) = ?\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FEWa_gBpcz_b"
   },
   "outputs": [],
   "source": [
    "## Place your code for your answer to question 2 in this box.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "j-nVfHtTrhcv"
   },
   "source": [
    "### A simple SGD training loop\n",
    "\n",
    "Optimisaton of neural networks is typically done by some variant of gradient descent. `pytorch` provides some convenient omptimizaton routines, like `torch.optim.SGD` that we can use to fit the stochastic gradient descent algorithm. \n",
    "\n",
    "We include an example of a simple stochastic gradient descent training loop below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "riZlz_lwh3Nl"
   },
   "outputs": [],
   "source": [
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "def sgd_fit(\n",
    "    X_train, y_train,\n",
    "    N, a_std = 0.001,\n",
    "    lr=0.001, epochs=100, batchsize=90):\n",
    "  ''' Fits a generalised linear model with N random fourier features\n",
    "  to data using stochastic gradient descent.\n",
    "\n",
    "  Parameters\n",
    "  ----------\n",
    "  X_train, y_train : torch.Tensor\n",
    "    Training input and output tensors\n",
    "  N : int\n",
    "    Number of random Fourier features to use\n",
    "  a_std : float\n",
    "    Standard deviation of parameters a at initialization\n",
    "  lr : float\n",
    "    Learning rate\n",
    "  epochs : int\n",
    "    Number of training epochs\n",
    "  batchsize : int\n",
    "    Number of datapoints in each minibatch\n",
    "\n",
    "  Returns\n",
    "  -------\n",
    "  a, V\n",
    "    torch.Tensors contatining parameters of the trained model\n",
    "  '''\n",
    "  dataloader = DataLoader(\n",
    "      TensorDataset(X_train, y_train),\n",
    "      batch_size=batchsize,\n",
    "      shuffle=True\n",
    "  )\n",
    "  a = torch.normal(0, a_std, size=(2*N, 1))\n",
    "  a.requires_grad=True\n",
    "  V = torch.normal(0.0, 1, size=(X.shape[1], N))\n",
    "  optimizer = torch.optim.SGD([a], lr=lr)\n",
    "  for t in range(epochs):\n",
    "    for X_minibatch, y_minibatch in dataloader:\n",
    "\t    optimizer.zero_grad()\n",
    "\t    error(X_minibatch, y_minibatch, a, V).backward()\n",
    "\t    optimizer.step()\n",
    "  return a, V"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ft4qMdNkXcLV"
   },
   "source": [
    "Note: In practice, one would not write a training loop from scratch like this. There are well-tested libraries with extensive functionality like `ignite` in pytorch and `keras` in TensorFlow. We included this simple code so as to illustrate what is going on in a SGD training loop."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BrKdnGrKpt9K"
   },
   "source": [
    "### Question 3\n",
    "\n",
    "Now we have used `pytorch` to compute the gradients, we'll optimize with stochastic gradient descent. \n",
    "\n",
    "Firstly, you'll look at the convergence of stochastic gradient descent for four different sizes of model, $N=5$, $N=100$, $N=400$ and $N=1000$.\n",
    "\n",
    "Modify the `sgd_fit` code to monitor convergence of both the test error and the training error as training progresses.\n",
    "\n",
    "Produce plots of the changing training and test error as the iterations procede for each model size $N$ above.\n",
    "\n",
    "For each model size ($N$) vary the the learning rate, number of iterations, batchsize, to improve the rate of convergence.\n",
    "\n",
    "You may want to use `torch.manual_seed` to set the random seed. Try different random seeds. What do you find?\n",
    "\n",
    "*15 marks*\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "sSn0DC3QtDxa"
   },
   "outputs": [],
   "source": [
    "## You can modify the function below to create your answers\n",
    "\n",
    "def sgd_fit_plot(\n",
    "    X_train, y_train, X_test, y_test,\n",
    "    N, a_std = 0.001,\n",
    "    lr=0.001, epochs=100, batchsize=30):\n",
    "  dataloader = DataLoader(\n",
    "      TensorDataset(X_train, y_train),\n",
    "      batch_size=batchsize,\n",
    "      shuffle=True\n",
    "  )\n",
    "  a = torch.normal(0, a_std, size=(2*N, 1))\n",
    "  a.requires_grad=True\n",
    "  V = torch.normal(0.0, 1, size=(X.shape[1], N))\n",
    "  optimizer = torch.optim.SGD([a], lr=0.001)\n",
    "  train_error = []\n",
    "  test_error = []\n",
    "  for t in range(epochs):\n",
    "    for X_minibatch, y_minibatch in dataloader:\n",
    "\t    optimizer.zero_grad()\n",
    "\t    error(X_minibatch, y_minibatch, a, V).backward()\n",
    "\t    optimizer.step()\n",
    "    train_error.append(error(X_train, y_train, a, V))\n",
    "    test_error.append(error(X_test, y_test, a, V))\n",
    "  return a, V, train_error, test_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-I1HhR-kiiob"
   },
   "outputs": [],
   "source": [
    "## Place your code for your answer to question 3 for N=5 in this box.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wfxeDs7ImlFj"
   },
   "outputs": [],
   "source": [
    "## Place your code for your answer to question 3 for N=100 in this box.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4E_eeuhQvEIx"
   },
   "outputs": [],
   "source": [
    "## Place your code for your answer to question 3 for N=400 in this box.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9q95rzlbvUsZ"
   },
   "outputs": [],
   "source": [
    "## Place your code for your answer to question 3 for N=1000 in this box.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qccbV66OzKve"
   },
   "source": [
    "#### Answer 3\n",
    "\n",
    "Explain what you find see in the different plots in this text box."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9WYbNfXOstNv"
   },
   "source": [
    "### Question 4\n",
    "\n",
    "Use what you have learned about learning rates and number of training iterations to recreate the plot of test error against $2N/n$  but using stochastic gradient descent to plot the new model. \n",
    "\n",
    "Justify your choice of learning rate and model dimensionality.\n",
    "\n",
    "Compare this plot with the direct fit you did in numpy above. How do they differ?\n",
    "\n",
    "*20 marks*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "p3LMWTHMm7-8"
   },
   "outputs": [],
   "source": [
    "## Place your code for your answer to question 4 in this box.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cc4khEpTv06Q"
   },
   "source": [
    "#### Answer 4\n",
    "\n",
    "Place your text for your answer to question 4 in this box.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IW63ZYBKKU9c"
   },
   "source": [
    "## Sawtooth network\n",
    "\n",
    "This part of the assignment looks at the *sawtooth* network Ferenc showed in the third lecture to demonstrate that deep ReLU networks can represent exponentially complex piecewise linear functions. The function is defined by the following recursion:\n",
    "\n",
    "\\begin{align}\n",
    "f_l(x) &= 2\\cdot \\vert f_{l-1}(x)\\vert - 2\\\\\n",
    "f_0(x) &= x\n",
    "\\end{align}\n",
    "\n",
    "This function can be implemented as a deep, narrow neural network with just two ReLU units at each layer. Below, I provide a simple `pytorch` implementation. The network has three different layers in it: the input layer, the middle layers, which are repeated, and the output layer. ReLU nonlinearities are sandwiched between them.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FITD8dt5KUdR"
   },
   "outputs": [],
   "source": [
    "from torch.nn import Identity, Sequential, Linear, ReLU\n",
    "\n",
    "def sawtooth_input_layer():\n",
    "  '''Input layer of a Sawtooth network. A Linear layer with fixed weights.'''\n",
    "  layer = Linear(1, 2, bias=False)\n",
    "  with torch.no_grad():\n",
    "    layer.weight.data = torch.Tensor([[1], [-1]])\n",
    "  return layer \n",
    "\n",
    "def sawtooth_middle_layer():\n",
    "  '''Middle layer of a Sawtooth network. A Linear layer with fixed weights.'''\n",
    "  layer = Linear(2, 2)\n",
    "  with torch.no_grad():\n",
    "    layer.weight.data = torch.Tensor([[2, 2], [-2, -2]])\n",
    "    layer.bias.data = torch.Tensor([-2, 2])\n",
    "  return layer\n",
    "\n",
    "def sawtooth_output_layer():\n",
    "  '''Output layer of a Sawtooth network. A Linear layer with fixed weights.'''\n",
    "  layer = Linear(2, 1)\n",
    "  with torch.no_grad():\n",
    "    layer.weight.data = torch.Tensor([[2, 2]])\n",
    "    layer.bias.data = torch.Tensor([-2])\n",
    "\n",
    "  return layer\n",
    "\n",
    "def get_sawtooth_network(num_hidden_layers = 5, middle_layer = sawtooth_middle_layer):\n",
    "  '''Returns a nn.Sequential model with ReLU activations and weights fixed to\n",
    "  implement the sawtooth function.\n",
    "  \n",
    "  Parameters\n",
    "  ----------\n",
    "  num_hidden_layers : int\n",
    "    Specifies the number of hidden layers. When 0, the Identity is returned.\n",
    "  middle_layer: function\n",
    "    Function to use to construct the layers in the middle. Default is to use\n",
    "    `sawtooth_middle_layer`.\n",
    "  '''\n",
    "  blocks = []\n",
    "  if num_hidden_layers < 0:\n",
    "    raise ValueError('Number of hidden layers must be non-negative')  \n",
    "  elif num_hidden_layers == 0:\n",
    "    blocks = [Identity]\n",
    "  else:\n",
    "    for l in range(num_hidden_layers):\n",
    "      if l==0:\n",
    "        blocks.append(sawtooth_input_layer())\n",
    "      else:\n",
    "        blocks.append(middle_layer())\n",
    "      blocks.append(ReLU())\n",
    "    blocks.append(sawtooth_output_layer())\n",
    "  return Sequential(*blocks)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "u9_uA_9LU0yl"
   },
   "source": [
    "In pytorch, you can use models the same way as if they were python functions. Below I plot a 6-layer network's output. (note that we have to call `.detach` on the output of the model which detaches the data from the computational graph that was built, which would be used for automatic differentiation)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "AvAu2pqyMozX"
   },
   "outputs": [],
   "source": [
    "x = torch.linspace(-2,2,1000)[:, None]\n",
    "\n",
    "model = get_sawtooth_network(6)\n",
    "\n",
    "plt.plot(x, model(x).detach());"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EqRNQxfcXFKe"
   },
   "source": [
    "The sawtooth network was deliberately constructed to show that exponentially complex networks can be represented by even very simple deep network architectures like this. But are they typical? How easy is it to break this exponential complexity property of the sawtooth networks? The assignment question below explores this a bit."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8EJ-HqNtVlw3"
   },
   "source": [
    "### Question 5\n",
    "\n",
    "I implemented `get_sawtooth_network` so that it is possible to pass a custom functions for constructing the *middle* layers of the network. Create two new versions of this middle layer constructor in the code blocks below:\n",
    "\n",
    "1. `noisy_sawtooth_middle_layer` should initialize the weights with a bit of random perturbation around the original fixed weights of the sawtooth network. The magnitude of the perturbation is controlled by an argument `noise_level`\n",
    "2. `random_init_middle_layer` should initialize the weights completely randomly.\n",
    "\n",
    "Create new models using these randomized layers and plot the model's output like above. I have included code blocks for plotting for convenience. Describe what you find. Is the number of linear segments still exponential?\n",
    "\n",
    "Finally try reinitializing the whole model weights using standard initialization scheme. *Tip:* Use the `reset_parameters` function on each `Linear` layer in the model.\n",
    "\n",
    "Please add a summmary to the text cell below.\n",
    "\n",
    "*20 marks*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Ih1tBSZiWR1b"
   },
   "outputs": [],
   "source": [
    "# Modify the function below so it returns a layer with randomly perturbed\n",
    "# weights and biases\n",
    "\n",
    "def noisy_sawtooth_middle_layer(noise_level = 0.1):\n",
    "  '''A noisy middle layer of a Sawtooth network. A 2x2 linear layer whose weights\n",
    "  are randomly perturbed around the fixed weights of the sawtooth network.\n",
    "\n",
    "  Parameters\n",
    "  ----------\n",
    "  noise_level : float\n",
    "    Controls the amount of noise added.\n",
    "  '''\n",
    "  layer = Linear(2, 2)\n",
    "  with torch.no_grad():\n",
    "    layer.weight.data = torch.Tensor([[2, 2], [-2, -2]])\n",
    "    layer.bias.data = torch.Tensor([-2, 2])\n",
    "  return layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lyu4pX37azSX"
   },
   "outputs": [],
   "source": [
    "# Modify the function below so it returns a layer with randomly initialized\n",
    "# weights and biases. \n",
    "\n",
    "def random_init_middle_layer(noise_level = 0.1):\n",
    "  '''A randomly initialized 2x2 linear layer.'''\n",
    "  layer = Linear(2, 2)\n",
    "  with torch.no_grad():\n",
    "    layer.weight.data = torch.Tensor([[2, 2], [-2, -2]])\n",
    "    layer.bias.data = torch.Tensor([-2, 2])\n",
    "  return layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Q96XkT40XiKo"
   },
   "outputs": [],
   "source": [
    "# We added this plotting cell for convenience, you may build on these to\n",
    "# explore what the functions you implemented do\n",
    "from functools import partial\n",
    "\n",
    "model = get_sawtooth_network(\n",
    "    num_hidden_layers = 6,\n",
    "    middle_layer = partial(noisy_sawtooth_middle_layer, noise_level=0.7)\n",
    "  )\n",
    "\n",
    "plt.plot(x, model(x).detach());"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HlTPnuHATwD8"
   },
   "source": [
    "#### Answer 5 text\n",
    "\n",
    "Please describe what you found in this text cell."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "V__UvCg2tgGZ"
   },
   "source": [
    "## Data Movement\n",
    "\n",
    "During the hardware lecture last Thursday, Nic discussed the importance of data movement and overhead (such as increased latency) that can occur when data is forced to 'spill over' into slow parts of the memory/storage hierarchy during inference or training. \n",
    "\n",
    "To assist with this challenge a range of model architectures have been developed that have small memory footprints. One very popular architecture of this variety is  [MobileNetv2](https://arxiv.org/abs/1801.04381). It was designed to run on smartphones and similar devices through an architecture that reduces memory and compute requirements, while still maintaining accuracy for common vision tasks.\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lbnhpflDBa88"
   },
   "source": [
    "### Question 6\n",
    "\n",
    "__Part 1:__ The working set of each layer in the model (defined during the Thursday lecture) is one way of defining the memory requirement of a neural network. Estimate the working set of each layer of MobileNetv2 and provide it as a table assuming only inference is performed. In the MobileNetv2 paper linked above, you will find Table 2 provides a detailed description of each layer of the model, please base your answer on this specification. You can compute this by hand, or you could specify the architecture in pytorch and use a function like [modelsummary](https://pypi.org/project/pytorch-model-summary/) to help you. Include a brief explaination as to how you calculated the values for your table. *Please remember the working set memory of each layer will be larger than just the weight parameters alone.*\n",
    "\n",
    "__Part 2:__ Assume that you perform inference using MobileNetv2 on a low-end mobile processor that has 128Kb of on-chip memory available for the model to use. Your input is 56x56 in size, how many times when performing one inference will you need to use off-chip memory?  \n",
    "\n",
    "*If you find it necessary to make certain assumptions when providing your answers, please briefly state them as part of your answer.*\n",
    "\n",
    "*20 marks*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Answer 6 Part 1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Answer 6 Part 2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "03-02-assignment-1.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
